---
title: "Deliverable 3"
author: "Júlia Gasull i Claudia Sánchez"
date: \today
output:
  html_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: Numeric and Binary targets Forecasting Models
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# First setups
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```

## Load Required Packages for this deliverable
We load the necessary packages and set working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
#setwd("~/Github Repositories/FIB-ADEI-LAB/deliverable3")
#filepath<-"~/Github Repositories/FIB-ADEI-LAB/deliverable3"
setwd("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2")
filepath<-"C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2"

# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car","lmtest","ggplot2","moments","factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr")

missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

## Load processed data from last deliverable
```{r}
#load(paste0(filepath,"/Taxi5000_del2.RData"))
load("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable3/Taxi5000_del2.RData")
summary(df)
```

# Refactor
```{r}
names(df)

names(df)[names(df) == "VendorID"] <- "f.vendor_id"
names(df)[names(df) == "RateCodeID"] <- "f.code_rate_id"
names(df)[names(df) == "Pickup_longitude"] <- "q.pickup_longitude"
names(df)[names(df) == "Pickup_latitude"] <- "q.pickup_latitude"
names(df)[names(df) == "Dropoff_longitude"] <- "q.dropoff_longitude"
names(df)[names(df) == "Dropoff_latitude"] <- "q.dropoff_latitude"
names(df)[names(df) == "Passenger_count"] <- "q.passenger_count"
names(df)[names(df) == "Trip_distance"] <- "q.trip_distance"
names(df)[names(df) == "Fare_amount"] <- "q.fare_amount"
names(df)[names(df) == "Extra"] <- "q.extra"
names(df)[names(df) == "MTA_tax"] <- "f.mta_tax"
names(df)[names(df) == "Tip_amount"] <- "q.tip_amount"
names(df)[names(df) == "Tolls_amount"] <- "q.tolls_amount"
names(df)[names(df) == "improvement_surcharge"] <- "f.improvement_surcharge"
names(df)[names(df) == "Total_amount"] <- "target.total_amount"
names(df)[names(df) == "Payment_type"] <- "f.payment_type"
names(df)[names(df) == "Trip_type"] <- "f.trip_type"
names(df)[names(df) == "hour"] <- "q.hour"
names(df)[names(df) == "period"] <- "f.period"
names(df)[names(df) == "tlenkm"] <- "q.tlenkm"
names(df)[names(df) == "traveltime"] <- "q.traveltime"
names(df)[names(df) == "espeed"] <- "q.espeed"
names(df)[names(df) == "pickup"] <- "qual.pickup"
names(df)[names(df) == "dropoff"] <- "qual.dropoff"
names(df)[names(df) == "Trip_distance_range"] <- "f.trip_distance_range"
names(df)[names(df) == "paidTolls"] <- "f.paid_tolls"
names(df)[names(df) == "TipIsGiven"] <- "target.tip_is_given"
names(df)[names(df) == "passenger_groups"] <- "f.passenger_groups"
#names(df)[names(df) == "f.cost"] <- ""
#names(df)[names(df) == "f.tt"] <- ""

df$hcpck <- NULL
df$claKM <- NULL
df$hcpckMCA <- NULL
df$hcpckMCA_hcpck <- NULL
df$hcpckMCA_claKM <- NULL

names(df)
```

Remove total amount equal to 0
```{r}
df<-df[!(df$target.total_amount=="0"),]
```


# Create factors needed for this deliverable (according to teacher's video recording)
We must create: f.cost, f.dist, f.tt and f.hour.
We already have f.cost and f.tt, so we will only have to create f.dist and f.hour:

## f.dist
```{r}
df$f.dist[df$q.trip_distance<=1.6] = "(0, 1.6]"
df$f.dist[(df$q.trip_distance>1.6) & (df$q.trip_distance<=3)] = "(1.6, 3]"
df$f.dist[(df$q.trip_distance>3) & (df$q.trip_distance<=5.5)] = "(3, 5.5]"
df$f.dist[(df$q.trip_distance>5.5) & (df$q.trip_distance<=30)] = "(5.5, 30]"
df$f.dist<-factor(df$f.dist)
```

## f.hour
```{r}
df$f.hour[(df$q.hour>=17) & (df$q.hour<18)] = "17"
df$f.hour[(df$q.hour>=18) & (df$q.hour<19)] = "18"
df$f.hour[(df$q.hour>=19) & (df$q.hour<20)] = "19"
df$f.hour[(df$q.hour>=20) & (df$q.hour<21)] = "20"
df$f.hour[(df$q.hour>=21) & (df$q.hour<22)] = "21"
df$f.hour[(df$q.hour>=22) & (df$q.hour<23)] = "22"
df$f.hour[(df$q.hour<17)] = "other"
df$f.hour[(df$q.hour>=23)] = "other"
df$f.hour<-factor(df$f.hour)
```

# Listing out variables
```{r}
vars_con<-names(df)[c(3:10,12:13,15,18,20:22)];
vars_dis<-names(df)[c(1:2,16,19,27:32)]; 
vars_res<-names(df)[c(15,27)];
vars_cexp<-vars_con[c(5:10,12:15)];
```

# Useful information
## Y (Numeric Target).

This variable will be the target for linear model building (connected to blocks Statistical Modeling I and II).

# Explanatory variables numeric only

Before we begin to see correlations with our target, we should consider the normality of this.

## Normality
```{r}
hist(df$target.total_amount,50,freq=F,col="darkslateblue",border = "darkslateblue")
mm<-mean(df$target.total_amount);ss<-sd(df$target.total_amount)
curve(dnorm(x,mean=mm,sd=ss),col="red",lwd=2,lty=3, add=T)

shapiro.test(df$target.total_amount)
```

We see that the target total_amount is not normally distributed for the following reasons:

* graph: there is no symmetry in the plot
* shapiro: we see that the p-value is too large to accept the assumption that target.total_amount is normally distributed

### Symmetry
```{r}
skewness(df$target.total_amount)
```

Normal data should have 0 skewness: we see that our data is right skewed (3.18).

### Kurtosis
```{r}
kurtosis(df$target.total_amount)
```

Normal data should be 3. We have 21.1, so, in this case, our data is not normal.

## Method 1: take the most correlated variables 
```{r}
# we use spearman method since out target is not normally distributed
round(cor(df[,c("target.total_amount",vars_cexp)], method="spearman"),dig=2)
```

We see that the diagonal is full of '1', since this command gives us the correlation between the same variable. Apart from this diagonal, however, there are more high correlations. Let's see which ones are correlated with our target:

* q.fare_amount: 0.97
* q.trip_distance: 0.93
* q.tlenkm: 0.91 (like trip_distance)
* q.traveltime: 0.90
* q.tip_amount: 0.41 (not much, but must be taken into account)
* q.espeed: 0.29 (not much, but must be taken into account)
* q.tolls_amount: 0.15  (not much, but must be taken into account)
* we can see that some of them are not correlated:
  + q.extra (0.03)
  + q.passenger_count (0.01)
  + q.hour (-0.01)
  
After seeing the correlation, to make an initial model, we should select the ones that are most correlated, which are:

* q.fare_amount
* q.trip_distance (we are not taking tlenkm because of redundance)
* q.traveltime
* q.tip_amount
* q.espeed
* q.tolls_amount

## Method 2:take the entire dataset with a condes
```{r}
res.con <- condes(df,num.var=which(names(df)=="target.total_amount"))
```

```{r}
res.con$quanti
```

Com hem pogut veure abans, les variables més correlacionades són:

* q.fare_amount: 0.94
  + it is normal for the rate to go up when the price goes up
* q.trip_distance: 0.90
  + the more distance, the more time, and therefore the more price
* q.tlenkm: 0.88 
  + just like the previous one
* q.traveltime: 0.76
  + the longer, the more price
* q.tip_amount: 0.57
  + not so much related, but we can keep in mind that people tend to give a percentage of the total price
* q.espeed: 0.40
* q.tolls_amount: 0.26

```{r}
res.con$quali
```

To talk about factor variables, we need to visualize res.con$quali. So let's see:

* f.trip_distance_range
  + we see that they are totally related, just as we see with que.trip_distance, since the longer distance, the longer time, and therefore the more price
* f.cost
  + is equivalent to our target
* f.tt
  + he longer time, the more price
* f.dist
  + just like with f.trip_distance_range
* f.paid_tolls
  + if you pay more, it means that the trip has lasted longer, and therefore has been longer, and is more likely to have gone through more tolls
* target.tip_is_given
  + just like before, but we can keep in mind that people tend to give a percentage of the total price

## Method 3: if few explanatory variables are available -> take all of them
```{r}
vars_cexp
cor(df$q.trip_distance,df$q.tlenkm)
```

To give an example, we see that the two distances we have, trip_distance and tlenkm, are closely related, since they represent the same.

### Model 1
```{r}
model_1 <- lm(
  target.total_amount~., 
  data=df[,c("target.total_amount",vars_cexp)]
)
summary(model_1)
```

Model_1 explains 93.38% of the variability of the target. We also see, according to the F-statistic, that it should be rejected.

We cannot use variables that are so correlated at the same time to act as explanatory variables. Therefore, we need to make a model in which we do not have these correlations.

But first, let's see which of them are that correlated:
```{r}
vif(model_1)  # Check association between explanatory vars
```

When the variance inflation factor is greater than 5, we need to consider whether or not we keep a variable.

* q.trip_distance: 115.12
* q.tlenkm: 97.05
* q.fare_amount: 10.38
* q.traveltime: 5.26

In this case we have to choose how far we stay. Since we work better with km than with miles (or inches, or whatever it is), we could choose the variable q.tlenkm.

### Model 1 with BIC
```{r}
model_1_bic <- step( model_1, k=log(nrow(df)) )
```

The BIC has been eliminating the variables it has considered, without worsening the AIC. However, since it does not take into account either correlations or concepts, it is probably not optimal.

Let's see how it turned out:

```{r}
vif(model_1_bic)
```

Note that trip_distance still has a vif greater than 5 (11.047912), and so does fare_amount (9.218707).

```{r}
summary(model_1_bic)
```

However, we see that it continues to explain much of the variability of our target (93.38%).

Therefore, we will try to make a model manually based on what model_1_bic has shown us and our knowledge of the data:

### Model 2

```{r}
model_2 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.fare_amount +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.tlenkm +
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
) 
summary(model_2)
```

We see that the explainability is now 93.34%.

```{r}
vif(model_2)  # Check association between explanatory vars
```

Even so, owning one is still beyond the reach of the average person.

We try to make a new model without the distance:

### Model 3

```{r}
model_3 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.fare_amount +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
) 
summary(model_3)
```

We see that the explainability is now 92.99%.

```{r}
vif(model_3)  # Check association between explanatory vars
```

The live ones are fine now. Still, we’ve pulled the distance, which conceptually we can’t afford. Therefore, we will try to remove another variable with a high vif (q.fare_amount), instead of q.tlenkm:


### Model 4

```{r}
model_4 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.tlenkm +
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
)
summary(model_4)
```

We see that the explainability is now 85.71%.

```{r}
vif(model_4)  # Check association between explanatory vars
```

Despite having high vifs, we still have high explicability of the variability of our target and, given that the variable we have taken out we can remove with time and distance from the trip, we do not need it.

So we continue to stay with this variable and make new models. We apply BIC to help us a little:
```{r}
model_4_bic <- step( model_4, k=log(nrow(df)) )
```

Following BIC, we have to eliminate variables until the vif's are less than 5. Therefore, the model that meets this is:

### Model 5

```{r}
model_5 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
) 
summary(model_5)
```

We see that the explainability is now 85.65%

```{r}
vif(model_5)  # Check association between explanatory vars
```

There is no vif that exceeds 5.

Let's now discriminate the variables independently:
```{r}
marginalModelPlots(model_5)
```

We see that there is not much mismatch of the marginal variables. If there were any, we would have to transform our explanatory variables.

## Diagnostics

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

Looking at the results, we can say that:

* There is no normality
* And, in terms of the Residual vs Leverage graph, our variables are within the R model, but it's not very reliable, so it doesn't help us much.

All this is due to the fact that our target variable was no longer normally distributed. To solve this, we apply the logarithm:

```{r}
model_6 <- lm(
  log(target.total_amount)~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
) 
summary(model_6)
```

We see that when doing the logarithm, the coefficient of determination is getting lower and lower, now it is 79.57%. We have seen that it has gotten worse than the previous model. Therefore, we discard it. We will work with model_5.

However, let's remember the last three models we used:

* Model 4
  + Coefficient of determination = 85.71%
  + VIFs: 
    - q.passenger_count: 1.004131
    - q.extra: 1.065525
    - q.tip_amount: 1.226836
    - q.tolls_amount: 1.063271
    - q.hour: 1.072210
    - q.tlenkm: 6.329422
    - q.traveltime: 4.244321
    - q.espeed: 2.768123
* Model 5
  + Coefficient of determination = 85.65%
  + VIFs: 
    - q.passenger_count: 1.003693
    - q.extra: 1.006079
    - q.tip_amount: 1.225445
    - q.tolls_amount: 1.063204
    - q.tlenkm: 2.452236
    - q.traveltime: 2.270921
* Model 6
  + Coefficient of determination = 79.57%
  + VIFs: 
    - q.passenger_count: 1.003693
    - q.extra: 1.006079
    - q.tip_amount: 1.225445
    - q.tolls_amount: 1.063204
    - q.tlenkm: 2.452236
    - q.traveltime: 2.270921

According to the coefficient of explicability, the ranking is: model_4 >> model_5 >> model_6. As for the VIFs, however, the ranking is: model_6 >> model_5 >> model_4. Since VIFs are acceptable on both model_5 and model_6, and not acceptable on model_4, the smartest option is to choose model_5.

So, let's look at the effects of this model:

```{r}
Anova(model_5)
```

We see that now the net effects are significant.

```{r}
library(effects)
plot(allEffects(model_5)) 
```

We see that our model defines the following:

* q.passenger_count does not depend on target.total_amount
* q.extra grows if target.total_amount grows
* q.tip_amount grows if target.total_amount grows
* q.tolls_amount grows if target.total_amount grows
* q.tlenkm grows if target.total_amount grows
* q.traveltime grows if target.total_amount grows

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

We see that the residues are not completely optimal.

--------------------------------------------------------------------------------


```{r}

##### ?? no trobo on ho explica
# summary(resid(model_5))
# sel1<-Boxplot(rstudent(model_5));sel1 # sel1 already contains row numbers
# ll1<-which(row.names(df) %in% names(rstudent(model_5)[sel1]));ll1
# 
# sel2<-which(hatvalues(model_5)>6*length(model_5$coefficients)/nrow(df));sel2;length(sel2) # sel2 contains row names
# ll2<-which(row.names(df) %in% names(hatvalues(model_5)[sel2]));ll2
# 
# sel3<-which(abs(cooks.distance(model_5))>4/(nrow(df)-length(model_5$coefficients)));sel3;length(sel3)
# ll3<-which(row.names(df) %in% names(cooks.distance(model_5)[sel3]));ll3
# 
# sel4<-Boxplot(cooks.distance(model_5));sel4  # sel4 already contains row numbers
# 
# sel3<-which((cooks.distance(model_5))>0.1);sel3;length(sel3)# sel3 contains row names
# ll3<-which(row.names(df) %in% names(cooks.distance(model_5)[sel3]));ll3
# 
# influencePlot(model_5,id=list(method="noteworthy", n=5, cex=0.5))
# with(df,tapply(Total_amount,RateCodeID,summary))

library(MASS)
boxcox(
    target.total_amount~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
)
```

We see the lambda parameter estimation method in the boxcox method. This gives us an idea of the power to which we need to raise the target variable in order to improve the properties of the linear model.

It is worth trying a new model with a square root in the target variable:

```{r}
model_7 <- lm(
  sqrt(target.total_amount)~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
) 
summary(model_7)
```

We see that the coefficient has improved, from 85.65% (modul_5) to 86.23% (model_7). But ... is it worth it from a residual point of view?

```{r}
par(mfrow=c(2,2))
plot( model_7, id.n=0 )
par(mfrow=c(1,1))
```

We see we haven’t won too much. So we stick to model_5.


##### 2020.12.04 min 00:41:00

----------------------------------------------------------------------------------------

## Using factors as explanatory variables
### Try to change numerical each regressor by its discretized factor

```{r}

model_8<-lm(log(target.total_amount) ~  q.extra + q.tip_amount + q.tolls_amount + f.improvement_surcharge + q.espeed  + log(q.tlenkm),data=df)
Anova(model_8)
vif(model_8)
residualPlots(model_8)

vars_enum<-c("q.extra","q.tip_amount","q.tolls_amount","f.improvement_surcharge","tlenkm")
vars_edis<-c("VendorID","RateCodeID","Payment_type","period")

df$f.extra <- factor(df$Extra)

m31<-lm(log(Total_amount) ~ f.extra + Tip_amount + Tolls_amount + improvement_surcharge + espeed + log(tlenkm)  ,data=df)

anova(m30, m31)  # NOT POSSIBLE
BIC(m30,m31)

m32<-lm(log(Total_amount) ~ f.extra + Tip_amount + f.tolls+ improvement_surcharge + espeed + log(tlenkm)  ,data=df)
BIC(m30,m31,m32)

m33<-lm(log(Total_amount) ~ f.extra + Tip_amount + f.tolls+ f.scharge + espeed + log(tlenkm)  ,data=df)
BIC(m30,m31,m32,m33)

m34<-lm(log(Total_amount) ~ f.extra + Tip_amount + f.tolls+ f.scharge + f.speed + log(tlenkm)  ,data=df)
BIC(m30,m31,m32,m33,m34)

m35 <- .....

Anova(m35)
summary(m35)
model.matrix(m35)[1:12,]

par(mfrow=c(2,2))
plot( m35, id.n=0 )
par(mfrow=c(1,1))

influencePlot( m35, id=c(list="noteworthy",n=5))
residualPlots( m35 )
marginalModelPlots( m35 )

ll1<-Boxplot(rstudent(m35));ll1
# ll1<-which(row.names(df) %in% names(rstudent(m25)[sel1]));ll1
df[ll1,]
ll4 <- Boxplot( cooks.distance( m35 ));ll4
ll4<-c(649, 4088)
dfred<-df[-ll4,]

# Outliers dels residus - Verticals - Cal suprimir-los: el model no pot explicar-los

m36<-lm(log(Total_amount) ~ f.extra + Tip_amount + f.tolls+ f.scharge + f.speed + log(tlenkm)  ,data=dfred)
summary(m36)
Anova(m36)
vif(m36)

```

### Adding factors: main effects

```{r}
names(df)
m40<-lm(log(Total_amount) ~ Tip_amount + log(tlenkm)+ f.tolls+ f.scharge + f.speed + f.extra + RateCodeID + VendorID + Payment_type+period ,data=df)

summary(m40)
Anova( m40 )

m41<-lm(log(Total_amount) ~ Tip_amount + log(tlenkm)+ f.tolls + f.speed + f.extra + RateCodeID + Payment_type+period ,data=df)

anova(m41, m40)
```


```{r}
# Interactions
# 
m50<-lm(log(Total_amount) ~ (Tip_amount + log(tlenkm))*(f.tolls + f.speed + f.extra + RateCodeID + Payment_type+period) ,data=df)

m51<-step( m50, k=log(nrow(df)))
Anova(m51)
summary( m51 )


m55<- ....

ll1<-Boxplot(rstudent(m55));ll1
sel2<-which(hatvalues(m55)>5*length(m55$coefficients)/nrow(df));sel2;length(sel2)
ll2<-which(row.names(df) %in% names(hatvalues(m25)[sel2]));ll2
sel3<-which(cooks.distance(m55)> 0.5 );sel3;length(sel3)
ll3<-which(row.names(df) %in% names(cooks.distance(m55)[sel3]));ll3


```
















## Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:

* At least two numerical variables have to be considered as explicative variables for initial steps in model building, called covariates. Non-linear models have to be checked for consistency.
* Select the most significant factors found in Multivariant Data Analysis as initial model factors.  Put some reasonable limits to initial model complexity.
* **You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.**
* Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable)

## Outcome/Target : A binary response variable (Binary Target) will be the response variable for Binary Regression Models included in Statistical Modeling Part III.

* Explicative Variables for modeling purposes are those available in dataset, exceptions will be indicated, if any.

* Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:
  + Split the sample in work and test samples (consisting on a 80-20 split). Working data frame has to be used for model building purposes.
  + At least two numerical variables have to be considered as explicative variables for initial steps in model building.
  + Select the most significant factors according to feature selection as initial model factors.  Put some reasonable limits to initial model complexity.
  + **You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.**
  + Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable).
  + You have to predict Y (Binary Target)  in the Working Data Frame  vs the rest according to the best validated model that you can find and make a confusion matrix.
  + Make a confusion matrix in the Testing Data Frame for **Y (Binary Target)** according to the best validated model found.
 

## Confusion Matrix:
When referring to the performance of a classification model, we are interested in the model’s ability to correctly predict or separate the classes. When looking at the errors made by a classification model, the confusion matrix gives the full picture. Consider e.g. a three class problem with the classes A, and B. The confusion matrix shows how the predictions are made by the model. The rows correspond to the known class of the data, i.e. the labels in the data. The columns correspond to the predictions made by the model. The value of each of element in the matrix is the number of predictions made with the class corresponding to the column for examples with the correct value as represented by the row. Thus, the diagonal elements show the number of correct classifications made for each class, and the off-diagonal elements show the errors made.

