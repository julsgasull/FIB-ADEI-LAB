---
title: "Deliverable 3"
author: "Júlia Gasull i Claudia Sánchez"
date: \today
output:
  html_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: Numeric and Binary targets Forecasting Models
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# First setups
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```

## Load Required Packages for this deliverable
We load the necessary packages and set working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
setwd("~/Documents/uni/FIB-ADEI-LAB/deliverable3")
filepath<-"~/Documents/uni/FIB-ADEI-LAB/deliverable3"
#setwd("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2")
#filepath<-"C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2"

# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car","lmtest","ggplot2","moments","factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr")

missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

## Load processed data from last deliverable
```{r}
load(paste0(filepath,"/Taxi5000_del2.RData"))
#load("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable3/Taxi5000_del2.RData")
summary(df)
```

# Refactor
```{r}
names(df)

names(df)[names(df) == "VendorID"] <- "f.vendor_id"
names(df)[names(df) == "RateCodeID"] <- "f.code_rate_id"
names(df)[names(df) == "Pickup_longitude"] <- "q.pickup_longitude"
names(df)[names(df) == "Pickup_latitude"] <- "q.pickup_latitude"
names(df)[names(df) == "Dropoff_longitude"] <- "q.dropoff_longitude"
names(df)[names(df) == "Dropoff_latitude"] <- "q.dropoff_latitude"
names(df)[names(df) == "Passenger_count"] <- "q.passenger_count"
names(df)[names(df) == "Trip_distance"] <- "q.trip_distance"
names(df)[names(df) == "Fare_amount"] <- "q.fare_amount"
names(df)[names(df) == "Extra"] <- "q.extra"
names(df)[names(df) == "MTA_tax"] <- "f.mta_tax"
names(df)[names(df) == "Tip_amount"] <- "q.tip_amount"
names(df)[names(df) == "Tolls_amount"] <- "q.tolls_amount"
names(df)[names(df) == "improvement_surcharge"] <- "f.improvement_surcharge"
names(df)[names(df) == "Total_amount"] <- "target.total_amount"
names(df)[names(df) == "Payment_type"] <- "f.payment_type"
names(df)[names(df) == "Trip_type"] <- "f.trip_type"
names(df)[names(df) == "hour"] <- "q.hour"
names(df)[names(df) == "period"] <- "f.period"
names(df)[names(df) == "tlenkm"] <- "q.tlenkm"
names(df)[names(df) == "traveltime"] <- "q.traveltime"
names(df)[names(df) == "espeed"] <- "q.espeed"
names(df)[names(df) == "pickup"] <- "qual.pickup"
names(df)[names(df) == "dropoff"] <- "qual.dropoff"
names(df)[names(df) == "Trip_distance_range"] <- "f.trip_distance_range"
names(df)[names(df) == "paidTolls"] <- "f.paid_tolls"
names(df)[names(df) == "TipIsGiven"] <- "target.tip_is_given"
names(df)[names(df) == "passenger_groups"] <- "f.passenger_groups"
#names(df)[names(df) == "f.cost"] <- ""
#names(df)[names(df) == "f.tt"] <- ""

df$hcpck <- NULL
df$claKM <- NULL
df$hcpckMCA <- NULL
df$hcpckMCA_hcpck <- NULL
df$hcpckMCA_claKM <- NULL

names(df)
```

Remove total amount equal to 0
```{r}
df<-df[!(df$target.total_amount=="0"),]
```


# Create factors needed for this deliverable (according to teacher's video recording)
We must create: f.cost, f.dist, f.tt and f.hour.
We already have f.cost and f.tt, so we will only have to create f.dist and f.hour:

## f.dist
```{r}
df$f.dist[df$q.trip_distance<=1.6] = "(0, 1.6]"
df$f.dist[(df$q.trip_distance>1.6) & (df$q.trip_distance<=3)] = "(1.6, 3]"
df$f.dist[(df$q.trip_distance>3) & (df$q.trip_distance<=5.5)] = "(3, 5.5]"
df$f.dist[(df$q.trip_distance>5.5) & (df$q.trip_distance<=30)] = "(5.5, 30]"
df$f.dist<-factor(df$f.dist)
```

## f.hour
```{r}
df$f.hour[(df$q.hour>=17) & (df$q.hour<18)] = "17"
df$f.hour[(df$q.hour>=18) & (df$q.hour<19)] = "18"
df$f.hour[(df$q.hour>=19) & (df$q.hour<20)] = "19"
df$f.hour[(df$q.hour>=20) & (df$q.hour<21)] = "20"
df$f.hour[(df$q.hour>=21) & (df$q.hour<22)] = "21"
df$f.hour[(df$q.hour>=22) & (df$q.hour<23)] = "22"
df$f.hour[(df$q.hour<17)] = "other"
df$f.hour[(df$q.hour>=23)] = "other"
df$f.hour<-factor(df$f.hour)
```

## f.espeed
```{r}
df$f.espeed[(df$q.espeed>=3) & (df$q.espeed<10)]  = "[03,10)"
df$f.espeed[(df$q.espeed>=10) & (df$q.espeed<20)] = "[10,20)"
df$f.espeed[(df$q.espeed>=20) & (df$q.espeed<30)] = "[20,30)"
df$f.espeed[(df$q.espeed>=30) & (df$q.espeed<40)] = "[30,40)"
df$f.espeed[(df$q.espeed>=40) & (df$q.espeed<50)] = "[40,50)"
df$f.espeed[(df$q.espeed>=50) & (df$q.espeed<=55)] = "[50,55]"
df$f.espeed<-factor(df$f.espeed)
```

# Listing out variables
```{r}
vars_con<-names(df)[c(3:10,12:13,15,18,20:22)];
vars_dis<-names(df)[c(1:2,16,19,27:32)]; 
vars_res<-names(df)[c(15,27)];
vars_cexp<-vars_con[c(5:10,12:15)];
```

# Useful information
## Y (Numeric Target).

This variable will be the target for linear model building (connected to blocks Statistical Modeling I and II).

# Explanatory variables numeric only

Before we begin to see correlations with our target, we should consider the normality of this.

## Normality
```{r}
hist(df$target.total_amount,50,freq=F,col="darkslateblue",border = "darkslateblue")
mm<-mean(df$target.total_amount);ss<-sd(df$target.total_amount)
curve(dnorm(x,mean=mm,sd=ss),col="red",lwd=2,lty=3, add=T)

shapiro.test(df$target.total_amount)
```

We see that the target total_amount is not normally distributed for the following reasons:

* graph: there is no symmetry in the plot
* shapiro: we see that the p-value is too large to accept the assumption that target.total_amount is normally distributed

### Symmetry
```{r}
skewness(df$target.total_amount)
```

Normal data should have 0 skewness: we see that our data is right skewed (3.18).

### Kurtosis
```{r}
kurtosis(df$target.total_amount)
```

Normal data should be 3. We have 21.1, so, in this case, our data is not normal.

## Method 1: take the most correlated variables 
```{r}
# we use spearman method since out target is not normally distributed
round(cor(df[,c("target.total_amount",vars_cexp)], method="spearman"),dig=2)
```

We see that the diagonal is full of '1', since this command gives us the correlation between the same variable. Apart from this diagonal, however, there are more high correlations. Let's see which ones are correlated with our target:

* q.fare_amount: 0.97
* q.trip_distance: 0.93
* q.tlenkm: 0.91 (like trip_distance)
* q.traveltime: 0.90
* q.tip_amount: 0.41 (not much, but must be taken into account)
* q.espeed: 0.29 (not much, but must be taken into account)
* q.tolls_amount: 0.15  (not much, but must be taken into account)
* we can see that some of them are not correlated:
  + q.extra (0.03)
  + q.passenger_count (0.01)
  + q.hour (-0.01)
  
After seeing the correlation, to make an initial model, we should select the ones that are most correlated, which are:

* q.fare_amount
* q.trip_distance (we are not taking tlenkm because of redundance)
* q.traveltime
* q.tip_amount
* q.espeed
* q.tolls_amount

## Method 2: take the entire dataset with a condes
```{r}
res.con <- condes(df,num.var=which(names(df)=="target.total_amount"))
```

```{r}
res.con$quanti
```

Com hem pogut veure abans, les variables més correlacionades són:

* q.fare_amount: 0.94
  + it is normal for the rate to go up when the price goes up
* q.trip_distance: 0.90
  + the more distance, the more time, and therefore the more price
* q.tlenkm: 0.88 
  + just like the previous one
* q.traveltime: 0.76
  + the longer, the more price
* q.tip_amount: 0.57
  + not so much related, but we can keep in mind that people tend to give a percentage of the total price
* q.espeed: 0.40
* q.tolls_amount: 0.26

```{r}
res.con$quali
```

To talk about factor variables, we need to visualize res.con$quali. So let's see:

* f.trip_distance_range
  + we see that they are totally related, just as we see with que.trip_distance, since the longer distance, the longer time, and therefore the more price
* f.cost
  + is equivalent to our target
* f.tt
  + he longer time, the more price
* f.dist
  + just like with f.trip_distance_range
* f.paid_tolls
  + if you pay more, it means that the trip has lasted longer, and therefore has been longer, and is more likely to have gone through more tolls
* target.tip_is_given
  + just like before, but we can keep in mind that people tend to give a percentage of the total price

## Method 3: if few explanatory variables are available -> take all of them
```{r}
vars_cexp
cor(df$q.trip_distance,df$q.tlenkm)
```

To give an example, we see that the two distances we have, trip_distance and tlenkm, are closely related, since they represent the same.

### Model 1
```{r}
model_1 <- lm(
  target.total_amount~., 
  data=df[,c("target.total_amount",vars_cexp)]
)
summary(model_1)
```

Model_1 explains 93.4% of the variability of the target. We also see, according to the F-statistic, that it should be rejected.

We cannot use variables that are so correlated at the same time to act as explanatory variables. Therefore, we need to make a model in which we do not have these correlations.

But first, let's see which of them are that correlated:
```{r}
vif(model_1)  # Check association between explanatory vars
```

When the variance inflation factor is greater than 5, we need to consider whether or not we keep a variable.

* q.trip_distance: 137.215426
* q.tlenkm: 116.473412
* q.fare_amount: 10.203484
* q.traveltime: 5.069225

In this case we have to choose how far we stay. Since we work better with km than with miles (or inches, or whatever it is), we could choose the variable q.tlenkm.

### Model 1 with BIC
```{r}
model_1_bic <- step( model_1, k=log(nrow(df)) )
```

The BIC has been eliminating the variables it has considered, without worsening the AIC. However, since it does not take into account either correlations or concepts, it is probably not optimal.

Let's see how it turned out:

```{r}
vif(model_1_bic)
```

Note that tlenkm still has a vif greater than 5 (9.377307), and so does fare_amount (7.898396).

```{r}
summary(model_1_bic)
```

However, we see that it continues to explain much of the variability of our target (93.39%).

Therefore, we will try to make a model manually based on what model_1_bic has shown us and our knowledge of the data:

### Model 2

```{r}
model_2 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.fare_amount +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.tlenkm +
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
) 
summary(model_2)
```

We see that the explainability is now 93.39%.

```{r}
vif(model_2)  # Check association between explanatory vars
```

Even so, owning one is still beyond the reach of the average person.

We try to make a new model without the distance:

### Model 3

```{r}
model_3 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.fare_amount +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
) 
summary(model_3)
```

We see that the explainability is now 92.99%.

```{r}
vif(model_3)  # Check association between explanatory vars
```

The live ones are fine now. Still, we’ve pulled the distance, which conceptually we can’t afford. Therefore, we will try to remove another variable with a high vif (q.fare_amount), instead of q.tlenkm:


### Model 4

```{r}
model_4 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.tlenkm +
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
)
summary(model_4)
```

We see that the explainability is now 86.17%.

```{r}
vif(model_4)  # Check association between explanatory vars
```

Despite having high vifs, we still have high explicability of the variability of our target and, given that the variable we have taken out we can remove with time and distance from the trip, we do not need it.

So we continue to stay with this variable and make new models. We apply BIC to help us a little:
```{r}
model_4_bic <- step( model_4, k=log(nrow(df)) )
```

Following BIC, we have to eliminate variables until the vif's are less than 5. Therefore, the model that meets this is:

### Model 5

```{r}
model_5 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
) 
summary(model_5)
```

We see that the explainability is now 86.09%

```{r}
vif(model_5)  # Check association between explanatory vars
```

There is no vif that exceeds 5.

Let's now discriminate the variables independently:
```{r}
marginalModelPlots(model_5)
```

We see that there is not much mismatch of the marginal variables. If there were any, we would have to transform our explanatory variables.

## Diagnostics

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

Looking at the results, we can say that:

* There is no normality
* And, in terms of the Residual vs Leverage graph, our variables are within the R model, but it's not very reliable, so it doesn't help us much.

All this is due to the fact that our target variable was no longer normally distributed. To solve this, we apply the logarithm:

```{r}
model_6 <- lm(
  log(target.total_amount)~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
) 
summary(model_6)
```

We see that when doing the logarithm, the coefficient of determination is getting lower and lower, now it is 79.51%. We have seen that it has gotten worse than the previous model. Therefore, we discard it. We will work with model_5.

However, let's remember the last three models we used:

* Model 4
  + Coefficient of determination = 86,17%
  + VIFs: 
    - q.passenger_count: 1.004128
    - q.extra: 1.065604
    - q.tip_amount: 1.227688
    - q.tolls_amount: 1.063359
    - q.hour: 1.072115
    - q.tlenkm: 6.195063
    - q.traveltime: 4.147204
    - q.espeed: 2.731942
* Model 5
  + Coefficient of determination = 86.09%
  + VIFs: 
    - q.passenger_count: 1.003687
    - q.extra: 1.006299
    - q.tip_amount: 1.226347
    - q.tolls_amount: 1.063286
    - q.tlenkm: 2.431645
    - q.traveltime: .249571
* Model 6
  + Coefficient of determination = 79.51%
  + VIFs: 
    - q.passenger_count: 1.003687
    - q.extra: 1.006299
    - q.tip_amount: 1.226347
    - q.tolls_amount: 1.063286
    - q.tlenkm: 2.431645
    - q.traveltime: 2.249571

According to the coefficient of explicability, the ranking is: model_4 >> model_5 >> model_6. As for the VIFs, however, the ranking is: model_6 >> model_5 >> model_4. Since VIFs are acceptable on both model_5 and model_6, and not acceptable on model_4, the smartest option is to choose model_5.

So, let's look at the effects of this model:

```{r}
Anova(model_5)
```

We see that now the net effects are significant.

```{r}
library(effects)
plot(allEffects(model_5)) 
```

We see that our model defines the following:

* q.passenger_count does not depend on target.total_amount
* q.extra grows if target.total_amount grows
* q.tip_amount grows if target.total_amount grows
* q.tolls_amount grows if target.total_amount grows
* q.tlenkm grows if target.total_amount grows
* q.traveltime grows if target.total_amount grows

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

We see that the residues are not completely optimal.

--------------------------------------------------------------------------------


```{r}
library(MASS)
boxcox(
    target.total_amount~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
)
```

We see the lambda parameter estimation method in the boxcox method. This gives us an idea of the power to which we need to raise the target variable in order to improve the properties of the linear model.

It is worth trying a new model with a square root in the target variable:

```{r}
model_7 <- lm(
  sqrt(target.total_amount)~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
) 
summary(model_7)
```

We see that the coefficient has improved, from 85.09% (model_5) to 86.41% (model_7). But ... is it worth it from a residual point of view?

```{r}
par(mfrow=c(2,2))
plot( model_7, id.n=0 )
par(mfrow=c(1,1))
```

We see we haven’t won too much. So we stick to model_5.

----------------------------------------------------------------------------------------

## Using factors as explanatory variables
### Try to change numerical each regressor by its discretized factor

```{r}
model_8<-lm(log(target.total_amount)~ q.extra + q.tip_amount +  q.tolls_amount + f.improvement_surcharge + q.espeed  + log(q.tlenkm), data=df)
summary(model_8)
``` 

We see that the explainability is now 87.77%. The more influent effects in this models are the length in km of the trip and the tip amount given.

```{r}
Anova(model_8)
vif(model_8)
residualPlots(model_8)
```

```{r}
# vars_enum<-c("q.extra","q.tip_amount","q.tolls_amount","f.improvement_surcharge","tlenkm")
# vars_edis<-c("VendorID","RateCodeID","Payment_type","period")
# 
df$f.extra <- factor(df$q.extra)

model_9<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    q.tolls_amount + 
    f.improvement_surcharge + 
    q.espeed + 
    log(q.tlenkm)  
  ,data=df
)
BIC(model_8,model_9)
```

We can see from the BIC that the model_9 is better than the model_8, so it is correct to consider extra as factor. Next, we will do the same with the tolls_amount and use the factor we had already created (paid_tolls).

```{r}
model_10<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    q.espeed + 
    log(q.tlenkm)  
  ,data=df
)
BIC(model_8,model_9,model_10)
```

We see can see that it is correct to use the paid_tolls factor to improve our model. We will try it now with the effective speed.

```{r}
model_11<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    f.espeed + 
    log(q.tlenkm)  
  ,data=df
)
BIC(model_8,model_9,model_10,model_11)
``` 
We can see that the best approach is the model_10, so we are going to stick to it for now.

```{r}
model_12 <- model_10

Anova(model_12)
summary(model_12)
```
We can see from the Anova test that f.extra has 2 freedom degrees and globally it does have a significant net effect once the other variables are in the model.

We are going to take a look at the residues.
```{r}
par(mfrow=c(2,2))
plot( model_12, id.n=0 )
par(mfrow=c(1,1))
```
Looking at the results, we can say that:

* There is no normality
* And, in terms of the Residual vs Leverage graph, our variables are within the R model, but it's not very reliable, so it doesn't help us much.


We proceed to take a look at the influence plot to check our influent residuals for model_12.
```{r}
influencePlot( model_12, id=c(list="noteworthy",n=5))
```

We see this model as a disaster. That is, we have a student waste of the order of 35. We can confirm that this is too much. We have to compare student waste with a normal standard. Therefore, we would say that the model we have so far is a model that has a serious waste problem.

In order for this not to happen to us, we need to work on the variable q.tlenkm.

So let's create a new model that does not give so many problems:

```{r}
model_13<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    q.espeed + 
    q.tlenkm
  ,data=df
)

BIC(model_12,model_13)

summary(model_13)
vif(model_13)

influencePlot( model_13, id=c(list="noteworthy",n=5))
```

After doing certain tests, taking into account the influences, the coefficients of explicability and the vifs, we decided that the best we can get is a model where q.tlenkm does not apply any operation.

So let's analyze it:

```{r}
residualPlots(model_13)
```

In the residualPlots, what we find is, for each factor, a boxplot of its categories and, for each quantitative variable, a pearson graph.

The observations at the bottom right of the q.tlenkm variable are giving us problems, as they have too low a student residue.

Let's use another tool to fully understand our model:

```{r}
marginalModelPlots(model_13)
```

In relation to the variable q.tip_amount, we see that there is a bit of mismatch, but not much, since tips given in cash are always declared as 0. Therefore, the data are not entirely real.

As for the variable q.tlenkm, we see that some observations do not follow the required pattern, and we have to modify them in some way.

How do we do that?

```{r}
ll1<-Boxplot(rstudent(model_13));ll1
ll1<-c(1908, 1564, 1057)
df[ll1,]
```

Let's see the strangest:

* 1908
  + target.total_amount: 108.41
  + q.tip_amount: 17
  + q.espeed: 55
  + q.tlenkm: 48.28
* 1564
  + target.total_amount: 128.76
  + q.tip_amount: 17
  + q.espeed: 55
  + q.tlenkm: 48.28
* 1057
  + target.total_amount: 3.8
  + q.tip_amount: 0
  + q.espeed: 33.4844
  + q.tlenkm: 21.56521
  
We see that the first two have exactly the same distance and speed, but no price. And, as for the last one, we see that they cover almost 22km and only pay $ 4.

We do the same for the cook distance:

```{r}
ll4 <- Boxplot(cooks.distance(model_13));ll4
ll4<-c(1908, 1564, 674)
df[ll4,]
```

* 1908
  + target.total_amount: 108.41
  + q.tip_amount: 17
  + q.espeed: 55
  + q.tlenkm: 48.28
* 1564
  + target.total_amount: 128.76
  + q.tip_amount: 17
  + q.espeed: 55
  + q.tlenkm: 48.28
* 674
  + target.total_amount: 86.15
  + q.tip_amount: 14.35 
  + q.espeed: 55
  + q.tlenkm: 43.45229

We see that they are all very similar.

It is necessary to eliminate these observations that do not have the same tendency as our model:

```{r}
dfred<-df[-ll4,]

model_14<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    q.espeed + 
    q.tlenkm
  ,data=dfred
)

summary(model_14)
Anova(model_14)
vif(model_14)
```

We see that the coefficient of determination has increased a bit and it seems that we have no collinearity problems.

### Adding factors: main effects

```{r}
names(df)
model_15<-lm(
  log(target.total_amount) ~ 
    q.tip_amount + 
    q.tlenkm+ 
    f.paid_tolls+ 
    f.improvement_surcharge + 
    f.espeed + 
    f.extra + 
    f.code_rate_id + 
    f.vendor_id + 
    f.payment_type+
    f.period 
  ,data=df
)
summary(model_15)
Anova(model_15)
```

Veiem que, de totes les variables explicatives introduides noves, les que podem salvar són: 

* f.espeed: 19.85
* f.payment_type: 12.67

Creem un nou model amb aquestes:

```{r}
model_16<-lm(
  log(target.total_amount) ~ 
    q.tip_amount + 
    q.tlenkm+ 
    f.paid_tolls+ 
    f.espeed + 
    f.extra + 
    f.code_rate_id + 
    f.payment_type+
    f.period 
  ,data=df
)

anova(model_15, model_16)
```

Veiem que no ens hem perdut res.

#### Interactions

```{r}
model_17<-lm(
  log(target.total_amount) ~ 
    (q.tip_amount + q.tlenkm)*(f.paid_tolls + f.espeed + f.extra + f.code_rate_id + f.payment_type + f.period)
  ,data=df
)

model_17<-step( model_17, k=log(nrow(df)))
```

Aquest mètode ens diu que:

* log(target.total_amount) depends on:
  + q.tip_amount 
  + q.tlenkm 
  + f.paid_tolls
  + f.espeed 
  + f.extra 
  + f.code_rate_id 
  + f.payment_type 
  + f.period
* and there are interactionsa between:
  + q.tip_amount:f.paid_tolls 
  + q.tip_amount:f.espeed 
  + q.tip_amount:f.extra 
  + q.tip_amount:f.code_rate_id 
  + q.tip_amount:f.period 
  + q.tlenkm:f.paid_tolls 
  + q.tlenkm:f.espeed 
  + q.tlenkm:f.extra 
  + q.tlenkm:f.code_rate_id 
  + q.tlenkm:f.payment_type 
  + q.tlenkm:f.period

```{r}
Anova(model_17)
summary(model_17)
```


!!!! FALTA DIAGNOSI EXHAUSTIVA !!!
````{r}
ll1<-Boxplot(rstudent(model_17));ll1
sel2<-which(hatvalues(model_17)>5*length(model_17$coefficients)/nrow(df));sel2;length(sel2)
ll2<-which(row.names(model_17) %in% names(hatvalues(model_17)[sel2]));ll2
sel3<-which(cooks.distance(model_17)> 0.5 );sel3;length(sel3)
ll3<-which(row.names(df) %in% names(cooks.distance(model_17)[sel3]));ll3
```


!!!!!!!!!!!!!!!!! Until 2020.12.04 - ALL VIDEO




## Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:

* At least two numerical variables have to be considered as explicative variables for initial steps in model building, called covariates. Non-linear models have to be checked for consistency.
* Select the most significant factors found in Multivariant Data Analysis as initial model factors.  Put some reasonable limits to initial model complexity.
* **You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.**
* Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable)

## Outcome/Target : A binary response variable (Binary Target) will be the response variable for Binary Regression Models included in Statistical Modeling Part III.

* Explicative Variables for modeling purposes are those available in dataset, exceptions will be indicated, if any.

* Multivariant Analysis conducted in previous deliverables has to be used to select the initial model. Students have some degrees in freedom in model building, but the following conditions are requested:
  + Split the sample in work and test samples (consisting on a 80-20 split). Working data frame has to be used for model building purposes.
  + At least two numerical variables have to be considered as explicative variables for initial steps in model building.
  + Select the most significant factors according to feature selection as initial model factors.  Put some reasonable limits to initial model complexity.
  + **You have to consider at least one interaction between a couple of factors and one interaction between factor and covariate.**
  + Diagnostics of the final model have to be undertaken. Lack of fit observations and influence data have to be selected and discussed (connections to multidimensional outliers in Multivariant Data Analysis is highly valuable).
  + You have to predict Y (Binary Target)  in the Working Data Frame  vs the rest according to the best validated model that you can find and make a confusion matrix.
  + Make a confusion matrix in the Testing Data Frame for **Y (Binary Target)** according to the best validated model found.
 

## Confusion Matrix:
When referring to the performance of a classification model, we are interested in the model’s ability to correctly predict or separate the classes. When looking at the errors made by a classification model, the confusion matrix gives the full picture. Consider e.g. a three class problem with the classes A, and B. The confusion matrix shows how the predictions are made by the model. The rows correspond to the known class of the data, i.e. the labels in the data. The columns correspond to the predictions made by the model. The value of each of element in the matrix is the number of predictions made with the class corresponding to the column for examples with the correct value as represented by the row. Thus, the diagonal elements show the number of correct classifications made for each class, and the off-diagonal elements show the errors made.

