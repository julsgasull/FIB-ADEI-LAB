---
title: "Deliverable 3"
author: "Júlia Gasull i Claudia Sánchez"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    toc: no
    toc_depth: '4'
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: Numeric and Binary targets Forecasting Models
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# First setups
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```

## Load Required Packages for this deliverable
We load the necessary packages and set working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
#setwd("~/Documents/uni/FIB-ADEI-LAB/deliverable3")
#filepath<-"~/Documents/uni/FIB-ADEI-LAB/deliverable3"
setwd("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2")
filepath<-"C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2"

# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car","lmtest","ggplot2","moments","factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr")

missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

## Load processed data from last deliverable
```{r}
#load(paste0(filepath,"/Taxi5000_del2.RData"))
load("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable3/Taxi5000_del2.RData")
summary(df)
```

# Refactor
```{r}
names(df)

names(df)[names(df) == "VendorID"] <- "f.vendor_id"
names(df)[names(df) == "RateCodeID"] <- "f.code_rate_id"
names(df)[names(df) == "Pickup_longitude"] <- "q.pickup_longitude"
names(df)[names(df) == "Pickup_latitude"] <- "q.pickup_latitude"
names(df)[names(df) == "Dropoff_longitude"] <- "q.dropoff_longitude"
names(df)[names(df) == "Dropoff_latitude"] <- "q.dropoff_latitude"
names(df)[names(df) == "Passenger_count"] <- "q.passenger_count"
names(df)[names(df) == "Trip_distance"] <- "q.trip_distance"
names(df)[names(df) == "Fare_amount"] <- "q.fare_amount"
names(df)[names(df) == "Extra"] <- "q.extra"
names(df)[names(df) == "MTA_tax"] <- "f.mta_tax"
names(df)[names(df) == "Tip_amount"] <- "q.tip_amount"
names(df)[names(df) == "Tolls_amount"] <- "q.tolls_amount"
names(df)[names(df) == "improvement_surcharge"] <- "f.improvement_surcharge"
names(df)[names(df) == "Total_amount"] <- "target.total_amount"
names(df)[names(df) == "Payment_type"] <- "f.payment_type"
names(df)[names(df) == "Trip_type"] <- "f.trip_type"
names(df)[names(df) == "hour"] <- "q.hour"
names(df)[names(df) == "period"] <- "f.period"
names(df)[names(df) == "tlenkm"] <- "q.tlenkm"
names(df)[names(df) == "traveltime"] <- "q.traveltime"
names(df)[names(df) == "espeed"] <- "q.espeed"
names(df)[names(df) == "pickup"] <- "qual.pickup"
names(df)[names(df) == "dropoff"] <- "qual.dropoff"
names(df)[names(df) == "Trip_distance_range"] <- "f.trip_distance_range"
names(df)[names(df) == "paidTolls"] <- "f.paid_tolls"
names(df)[names(df) == "TipIsGiven"] <- "target.tip_is_given"
names(df)[names(df) == "passenger_groups"] <- "f.passenger_groups"
#names(df)[names(df) == "f.cost"] <- ""
#names(df)[names(df) == "f.tt"] <- ""

df$hcpck <- NULL
df$claKM <- NULL
df$hcpckMCA <- NULL
df$hcpckMCA_hcpck <- NULL
df$hcpckMCA_claKM <- NULL

names(df)
```

Remove total amount equal to 0
```{r}
df<-df[!(df$target.total_amount=="0"),]
```


# Create factors needed for this deliverable (according to teacher's video recording)
We must create: f.cost, f.dist, f.tt and f.hour.
We already have f.cost and f.tt, so we will only have to create f.dist and f.hour:

## f.dist
```{r}
df$f.dist[df$q.trip_distance<=1.6] = "(0, 1.6]"
df$f.dist[(df$q.trip_distance>1.6) & (df$q.trip_distance<=3)] = "(1.6, 3]"
df$f.dist[(df$q.trip_distance>3) & (df$q.trip_distance<=5.5)] = "(3, 5.5]"
df$f.dist[(df$q.trip_distance>5.5) & (df$q.trip_distance<=30)] = "(5.5, 30]"
df$f.dist<-factor(df$f.dist)
```

## f.hour
```{r}
df$f.hour[(df$q.hour>=17) & (df$q.hour<18)] = "17"
df$f.hour[(df$q.hour>=18) & (df$q.hour<19)] = "18"
df$f.hour[(df$q.hour>=19) & (df$q.hour<20)] = "19"
df$f.hour[(df$q.hour>=20) & (df$q.hour<21)] = "20"
df$f.hour[(df$q.hour>=21) & (df$q.hour<22)] = "21"
df$f.hour[(df$q.hour>=22) & (df$q.hour<23)] = "22"
df$f.hour[(df$q.hour<17)] = "other"
df$f.hour[(df$q.hour>=23)] = "other"
df$f.hour<-factor(df$f.hour)
```

## f.espeed
```{r}
df$f.espeed[(df$q.espeed>=3) & (df$q.espeed<10)]  = "[03,10)"
df$f.espeed[(df$q.espeed>=10) & (df$q.espeed<20)] = "[10,20)"
df$f.espeed[(df$q.espeed>=20) & (df$q.espeed<30)] = "[20,30)"
df$f.espeed[(df$q.espeed>=30) & (df$q.espeed<40)] = "[30,40)"
df$f.espeed[(df$q.espeed>=40) & (df$q.espeed<50)] = "[40,50)"
df$f.espeed[(df$q.espeed>=50) & (df$q.espeed<=55)] = "[50,55]"
df$f.espeed<-factor(df$f.espeed)
```

# Listing out variables
```{r}
vars_con<-names(df)[c(3:10,12:13,15,18,20:22)];
vars_dis<-names(df)[c(1:2,16,19,27:32)]; 
vars_res<-names(df)[c(15,27)];
vars_cexp<-vars_con[c(5:10,12:15)];
```

# Useful information
## Y (Numeric Target).

This variable will be the target for linear model building (connected to blocks Statistical Modeling I and II).


--------------------------------------------------------------------------------


# Quantitative Logistics Regression (explanatory variables numeric only)

Steps to follow:

1. Enter all relevant numerical variables in the model
2. See if you need to replace a number with its equivalent factor
3. Add to the best model of step 2, the main effects of the factors and retain the significant net effects.
4. Add interactions: between factor-factor and between factor-numeric (doubles).
5. Diagnosis of waste and observations. Lack of adjustment and / or influential.


Before we begin to see correlations with our target, we should consider the normality of this.

## Normality
```{r}
hist(df$target.total_amount,50,freq=F,col="darkslateblue",border = "darkslateblue")
mm<-mean(df$target.total_amount);ss<-sd(df$target.total_amount)
curve(dnorm(x,mean=mm,sd=ss),col="red",lwd=2,lty=3, add=T)

shapiro.test(df$target.total_amount)
```

We see that the target total_amount is not normally distributed for the following reasons:

* graph: there is no symmetry in the plot
* shapiro: we see that the p-value is too large to accept the assumption that target.total_amount is normally distributed

### Symmetry
```{r}
skewness(df$target.total_amount)
```

Normal data should have 0 skewness: we see that our data is right skewed (3.18).

### Kurtosis
```{r}
kurtosis(df$target.total_amount)
```

Normal data should be 3. We have 21.1, so, in this case, our data is not normal.

## Method 1: take the most correlated variables 
```{r}
# we use spearman method since out target is not normally distributed
round(cor(df[,c("target.total_amount",vars_cexp)], method="spearman"),dig=2)
```

We see that the diagonal is full of '1', since this command gives us the correlation between the same variable. Apart from this diagonal, however, there are more high correlations. Let's see which ones are correlated with our target:

* q.fare_amount: 0.97
* q.trip_distance: 0.93
* q.tlenkm: 0.91 (like trip_distance)
* q.traveltime: 0.90
* q.tip_amount: 0.41 (not much, but must be taken into account)
* q.espeed: 0.29 (not much, but must be taken into account)
* q.tolls_amount: 0.15  (not much, but must be taken into account)
* we can see that some of them are not correlated:
  + q.extra (0.03)
  + q.passenger_count (0.01)
  + q.hour (-0.01)
  
After seeing the correlation, to make an initial model, we should select the ones that are most correlated, which are:

* q.fare_amount
* q.trip_distance (we are not taking tlenkm because of redundance)
* q.traveltime
* q.tip_amount
* q.espeed
* q.tolls_amount

## Method 2: take the entire dataset with a condes
```{r}
res.con <- condes(df,num.var=which(names(df)=="target.total_amount"))
```

```{r}
res.con$quanti
```

Com hem pogut veure abans, les variables més correlacionades són:

* q.fare_amount: 0.94
  + it is normal for the rate to go up when the price goes up
* q.trip_distance: 0.90
  + the more distance, the more time, and therefore the more price
* q.tlenkm: 0.88 
  + just like the previous one
* q.traveltime: 0.76
  + the longer, the more price
* q.tip_amount: 0.57
  + not so much related, but we can keep in mind that people tend to give a percentage of the total price
* q.espeed: 0.40
* q.tolls_amount: 0.26

```{r}
res.con$quali
```

To talk about factor variables, we need to visualize res.con$quali. So let's see:

* f.trip_distance_range
  + we see that they are totally related, just as we see with que.trip_distance, since the longer distance, the longer time, and therefore the more price
* f.cost
  + is equivalent to our target
* f.tt
  + he longer time, the more price
* f.dist
  + just like with f.trip_distance_range
* f.paid_tolls
  + if you pay more, it means that the trip has lasted longer, and therefore has been longer, and is more likely to have gone through more tolls
* target.tip_is_given
  + just like before, but we can keep in mind that people tend to give a percentage of the total price

## Method 3: if few explanatory variables are available -> take all of them
```{r}
vars_cexp
cor(df$q.trip_distance,df$q.tlenkm)
```

To give an example, we see that the two distances we have, trip_distance and tlenkm, are closely related, since they represent the same.

### Model 1
```{r}
model_1 <- lm(
  target.total_amount~.
  ,data=df[,c("target.total_amount",vars_cexp)]
);summary(model_1)
```

Model_1 explains 93.4% of the variability of the target. We also see, according to the F-statistic, that it should be rejected.

We cannot use variables that are so correlated at the same time to act as explanatory variables. Therefore, we need to make a model in which we do not have these correlations.

But first, let's see which of them are that correlated:
```{r}
vif(model_1)  # Check association between explanatory vars
```

When the variance inflation factor is greater than 5, we need to consider whether or not we keep a variable.

* q.trip_distance: 137.215426
* q.tlenkm: 116.473412
* q.fare_amount: 10.203484
* q.traveltime: 5.069225

In this case we have to choose how far we stay. Since we work better with km than with miles (or inches, or whatever it is), we could choose the variable q.tlenkm.

### Model 1 with BIC
```{r}
model_1_bic <- step( model_1, k=log(nrow(df)) )
```

The BIC has been eliminating the variables it has considered, without worsening the AIC. However, since it does not take into account either correlations or concepts, it is probably not optimal.

Let's see how it turned out:

```{r}
vif(model_1_bic)
```

Note that tlenkm still has a vif greater than 5 (9.377307), and so does fare_amount (7.898396).

```{r}
summary(model_1_bic)
```

However, we see that it continues to explain much of the variability of our target (93.39%).

Therefore, we will try to make a model manually based on what model_1_bic has shown us and our knowledge of the data:

### Model 2

```{r}
model_2 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.fare_amount +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.tlenkm +
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
);summary(model_2)
```

We see that the explainability is now 93.39%.

```{r}
vif(model_2)  # Check association between explanatory vars
```

Even so, owning one is still beyond the reach of the average person.

We try to make a new model without the distance:

### Model 3

```{r}
model_3 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.fare_amount +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
);summary(model_3)
```

We see that the explainability is now 92.99%.

```{r}
vif(model_3)  # Check association between explanatory vars
```

The live ones are fine now. Still, we’ve pulled the distance, which conceptually we can’t afford. Therefore, we will try to remove another variable with a high vif (q.fare_amount), instead of q.tlenkm:


### Model 4

```{r}
model_4 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.extra + 
    q.tip_amount +
    q.tolls_amount +
    q.hour + 
    q.tlenkm +
    q.traveltime +
    q.espeed 
  ,
  data=df[,c("target.total_amount",vars_cexp)]
);summary(model_4)
```

We see that the explainability is now 86.17%.

```{r}
vif(model_4)  # Check association between explanatory vars
```

Despite having high vifs, we still have high explicability of the variability of our target and, given that the variable we have taken out we can remove with time and distance from the trip, we do not need it.

So we continue to stay with this variable and make new models. We apply BIC to help us a little:
```{r}
model_4_bic <- step( model_4, k=log(nrow(df)) )
```

Following BIC, we have to eliminate variables until the vif's are less than 5. Therefore, the model that meets this is:

### Model 5

```{r}
model_5 <- lm(
  target.total_amount~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
);summary(model_5)
```

We see that the explainability is now 86.09%

```{r}
vif(model_5)  # Check association between explanatory vars
```

There is no vif that exceeds 5.

Let's now discriminate the variables independently:
```{r}
marginalModelPlots(model_5)
```

We see that there is not much mismatch of the marginal variables. If there were any, we would have to transform our explanatory variables.

## Diagnostics

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

Looking at the results, we can say that:

* There is no normality
* And, in terms of the Residual vs Leverage graph, our variables are within the R model, but it's not very reliable, so it doesn't help us much.

All this is due to the fact that our target variable was no longer normally distributed. To solve this, we apply the logarithm:

```{r}
model_6 <- lm(
  log(target.total_amount)~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
);summary(model_6)
```

We see that when doing the logarithm, the coefficient of determination is getting lower and lower, now it is 79.51%. We have seen that it has gotten worse than the previous model. Therefore, we discard it. We will work with model_5.

However, let's remember the last three models we used:

* Model 4
  + Coefficient of determination = 86,17%
  + VIFs: 
    - q.passenger_count: 1.004128
    - q.extra: 1.065604
    - q.tip_amount: 1.227688
    - q.tolls_amount: 1.063359
    - q.hour: 1.072115
    - q.tlenkm: 6.195063
    - q.traveltime: 4.147204
    - q.espeed: 2.731942
* Model 5
  + Coefficient of determination = 86.09%
  + VIFs: 
    - q.passenger_count: 1.003687
    - q.extra: 1.006299
    - q.tip_amount: 1.226347
    - q.tolls_amount: 1.063286
    - q.tlenkm: 2.431645
    - q.traveltime: .249571
* Model 6
  + Coefficient of determination = 79.51%
  + VIFs: 
    - q.passenger_count: 1.003687
    - q.extra: 1.006299
    - q.tip_amount: 1.226347
    - q.tolls_amount: 1.063286
    - q.tlenkm: 2.431645
    - q.traveltime: 2.249571

According to the coefficient of explicability, the ranking is: model_4 >> model_5 >> model_6. As for the VIFs, however, the ranking is: model_6 >> model_5 >> model_4. Since VIFs are acceptable on both model_5 and model_6, and not acceptable on model_4, the smartest option is to choose model_5.

So, let's look at the effects of this model:

```{r}
Anova(model_5)
```

We see that now the net effects are significant.

```{r}
library(effects)
plot(allEffects(model_5)) 
```

We see that our model defines the following:

* q.passenger_count does not depend on target.total_amount
* q.extra grows if target.total_amount grows
* q.tip_amount grows if target.total_amount grows
* q.tolls_amount grows if target.total_amount grows
* q.tlenkm grows if target.total_amount grows
* q.traveltime grows if target.total_amount grows

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

We see that the residues are not completely optimal.


```{r}
library(MASS)
boxcox(
    target.total_amount~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
)
```

We see the lambda parameter estimation method in the boxcox method. This gives us an idea of the power to which we need to raise the target variable in order to improve the properties of the linear model.

It is worth trying a new model with a square root in the target variable:

```{r}
model_7 <- lm(
  sqrt(target.total_amount)~ 
    q.passenger_count +
    q.extra +
    q.tip_amount +
    q.tolls_amount +
    q.tlenkm +
    q.traveltime
  ,
  data=df
);summary(model_7)
```

We see that the coefficient has improved, from 85.09% (model_5) to 86.41% (model_7). But ... is it worth it from a residual point of view?

```{r}
par(mfrow=c(2,2))
plot( model_7, id.n=0 )
par(mfrow=c(1,1))
```

We see we haven’t won too much. So we stick to model_5.

## Using factors as explanatory variables
### Try to change numerical each regressor by its discretized factor

```{r}
model_8<-lm(log(target.total_amount)~ q.extra + q.tip_amount +  q.tolls_amount + f.improvement_surcharge + q.espeed  + log(q.tlenkm), data=df)
summary(model_8)
``` 

We see that the explainability is now 87.77%. The more influent effects in this models are the length in km of the trip and the tip amount given.

```{r}
Anova(model_8)
vif(model_8)
residualPlots(model_8)
```

```{r}
# vars_enum<-c("q.extra","q.tip_amount","q.tolls_amount","f.improvement_surcharge","tlenkm")
# vars_edis<-c("VendorID","RateCodeID","Payment_type","period")
# 
df$f.extra <- factor(df$q.extra)

model_9<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    q.tolls_amount + 
    f.improvement_surcharge + 
    q.espeed + 
    log(q.tlenkm)  
  ,data=df
)
BIC(model_8,model_9)
```

We can see from the BIC that the model_9 is better than the model_8, so it is correct to consider extra as factor. Next, we will do the same with the tolls_amount and use the factor we had already created (paid_tolls).

```{r}
model_10<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    q.espeed + 
    log(q.tlenkm)  
  ,data=df
)
BIC(model_8,model_9,model_10)
```

We see can see that it is correct to use the paid_tolls factor to improve our model. We will try it now with the effective speed.

```{r}
model_11<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    f.espeed + 
    log(q.tlenkm)  
  ,data=df
)
BIC(model_8,model_9,model_10,model_11)
``` 
We can see that the best approach is the model_10, so we are going to stick to it for now.

```{r}
model_12 <- model_10

Anova(model_12)
summary(model_12)
```
We can see from the Anova test that f.extra has 2 freedom degrees and globally it does have a significant net effect once the other variables are in the model.

We are going to take a look at the residues.
```{r}
par(mfrow=c(2,2))
plot( model_12, id.n=0 )
par(mfrow=c(1,1))
```
Looking at the results, we can say that:

* There is no normality
* And, in terms of the Residual vs Leverage graph, our variables are within the R model, but it's not very reliable, so it doesn't help us much.


We proceed to take a look at the influence plot to check our influent residuals for model_12.
```{r}
influencePlot( model_12, id=c(list="noteworthy",n=5))
```

We see this model as a disaster. That is, we have a student waste of the order of 35. We can confirm that this is too much. We have to compare student waste with a normal standard. Therefore, we would say that the model we have so far is a model that has a serious waste problem.

Intento treure outliers multivariants de total_amount i tlenkm:

```{r}
library(mvoutlier)
library(chemometrics)
multivariant_outliers <- Moutlier(df[, c(15,20)], quantile = 0.995)
multivariant_outliers$cutoff
par(mfrow=c(1,1))
plot(multivariant_outliers$md, multivariant_outliers$rd, type="n")
text(multivariant_outliers$md, multivariant_outliers$rd, labels=rownames(df[, c(15,20)]), cex=0.5) 

ll_mvoutliers<-c('1237379', '1208612', '1171898', '488540', '211894', '638666', '329000', '1175981', '604912')

df <- df[!(row.names(df) %in% ll_mvoutliers),]

multivariant_outliers <- Moutlier(df[, c(15,20)], quantile = 0.995)
multivariant_outliers$cutoff
par(mfrow=c(1,1))
plot(multivariant_outliers$md, multivariant_outliers$rd, type="n")
text(multivariant_outliers$md, multivariant_outliers$rd, labels=rownames(df[, c(15,20)]), cex=0.75) 

```



In order for this not to happen to us, we need to work on the variable q.tlenkm.

So let's create a new model that does not give so many problems:

```{r}
model_13<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    q.espeed + 
    log(q.tlenkm)
  ,data=df
)

summary(model_13)
vif(model_13)

influencePlot( model_13, id=c(list="noteworthy",n=5))
```

After doing certain tests, taking into account the influences, the coefficients of explicability and the vifs, we decided that the best we can get is a model where q.tlenkm does not apply any operation.

So let's analyze it:

```{r}
residualPlots(model_13)
```

In the residualPlots, what we find is, for each factor, a boxplot of its categories and, for each quantitative variable, a pearson graph.

Let's use another tool to fully understand our model:

```{r}
marginalModelPlots(model_13)
```

In relation to the variable q.tip_amount, we see that there is a bit of mismatch, but not much, since tips given in cash are always declared as 0. Therefore, the data are not entirely real.

As for the variable q.tlenkm, we see that some observations do not follow the required pattern, and we have to modify them in some way.

How do we do that?

```{r}
ll1<-Boxplot(rstudent(model_13));ll1
ll1<-c(4269, 80, 2621)
df[ll1,]
```

Let's see the strangest:

* 4269
  + target.total_amount: 5.0
  + q.tip_amount: 0
  + q.espeed: 11.06889
  + q.tlenkm: 16.769364
* 80
  + target.total_amount: 3.8 
  + q.tip_amount: 0
  + q.espeed: 23.16672 
  + q.tlenkm: 9.012326
* 2621
  + target.total_amount: 3.8
  + q.tip_amount: 0
  + q.espeed: 23.05353
  + q.tlenkm: 8.851392

Veiem que són observacionsa vastant normals. Tot i això, per exemple, podem destacar que la observació 4269, a la qual ja se li aplica una tarifa de 5$, per molts km és que hagi fet, el preu no ha pujat.

We do the same for the cook distance:

```{r}
ll4 <- Boxplot(cooks.distance(model_13));ll4
ll4<-c(4269, 2005, 2434)
df[ll4,]
```

* 4269
  + target.total_amount: 5.0
  + q.tip_amount: 0
  + q.espeed: 11.06889
  + q.tlenkm: 16.769364
* 2005
  + target.total_amount: 50.00 
  + q.tip_amount: 0
  + q.espeed: 27.33968 
  + q.tlenkm: 1.00000
* 2434
  + target.total_amount: 49.99 
  + q.tip_amount: 0
  + q.espeed: 23.79045 
  + q.tlenkm: 1.00000

We see that, apart from the first, explained above, the other two observations have a trip length of 1km, but instead has been paid about $ 50. We see that this is not possible.

It is necessary to eliminate these observations that do not have the same tendency as our model:

```{r}
dfred<-df[-ll4,]

model_14<-lm(
  log(target.total_amount)~ 
    f.extra + 
    q.tip_amount + 
    f.paid_tolls + 
    f.improvement_surcharge + 
    q.espeed + 
    log(q.tlenkm)
  ,data=dfred
)

summary(model_14)
Anova(model_14)
vif(model_14)
```

We see that the coefficient of determination has increased a bit and it seems that we have no collinearity problems.

### Adding factors: main effects

```{r}
names(df)
model_15<-lm(
  log(target.total_amount) ~ 
    q.tip_amount + 
    log(q.tlenkm)+ 
    f.paid_tolls+ 
    f.improvement_surcharge + 
    f.espeed + 
    f.extra + 
    f.code_rate_id + 
    f.vendor_id + 
    f.payment_type+
    f.period 
  ,data=df
)
summary(model_15)
Anova(model_15)
```

We see that, of all the new explanatory variables introduced, the ones we can save are:

* f.espeed: 22.49
* f.code_rate_id: 11.06
* f.payment_type: 4.19

We create a new model with them:

```{r}
model_16<-lm(
  log(target.total_amount) ~ 
    q.tip_amount + 
    log(q.tlenkm)+ 
    f.paid_tolls+ 
    f.espeed + 
    f.extra + 
    f.code_rate_id + 
    f.payment_type+
    f.period 
  ,data=df
)

anova(model_15, model_16)
```

We see that we haven't lost anything.

#### Interactions

```{r}
model_17<-lm(
  log(target.total_amount) ~ 
    (q.tip_amount + log(q.tlenkm))*(f.paid_tolls + f.espeed + f.extra + f.code_rate_id + f.payment_type + f.period)
  ,data=df
)

model_17<-step( model_17, k=log(nrow(df)))
```

This method tells us that:

* log(target.total_amount) depends on:
  + q.tip_amount 
  + log(q.tlenkm) 
  + f.paid_tolls 
  + f.espeed 
  + f.extra 
  + f.code_rate_id 
  + f.payment_type 
* and there are interactionsa between:
  + q.tip_amount:f.espeed 
  + q.tip_amount:f.code_rate_id 
  + log(q.tlenkm):f.espeed 
  + log(q.tlenkm):f.extra 
  + log(q.tlenkm):f.code_rate_id 
  + log(q.tlenkm):f.payment_type

```{r}
Anova(model_17)
summary(model_17)
```


# !!!! FALTA DIAGNOSI EXHAUSTIVA !!!


````{r}
ll1<-Boxplot(rstudent(model_17));ll1
sel2<-which(hatvalues(model_17)>5*length(model_17$coefficients)/nrow(df));sel2;length(sel2)
ll2<-which(row.names(model_17) %in% names(hatvalues(model_17)[sel2]));ll2
sel3<-which(cooks.distance(model_17)> 0.5 );sel3;length(sel3)
ll3<-which(row.names(df) %in% names(cooks.distance(model_17)[sel3]));ll3
```


--------------------------------------------------------------------------------

# Binary Logistics Regression

```{r}
vars_cexp <- vars_cexp[c(1:4,6:10)]; vars_cexp

table(df$target.tip_is_given, df$f.payment_type)
```

We can see from the table that it is no credible the fact that any of the people that paid in cash did not leave any tip.

```{r}
res.cat <- catdes(df, num.var = which(names(df)=="target.tip_is_given"))
res.cat$quanti.var
res.cat$test.chi2
```

From the quanti.var we can see that tip_is_given depends on tip_amount which seems obvious, due to the fact that they are the same variable treated in different ways.

From the test.chi2 we can see that payment_type has something really clear with the tip_is_given, as we have p-value of 0. Which means that we cannot use payment_type as a predictor.

## Filter

```{r}
ll<-which(df$f.payment_type=="Cash"); length(ll)
dff<-df[-ll,]
set.seed(12345)
llwork<-sample(1:nrow(dff),0.70*nrow(dff),replace=FALSE)
llwork<-sort(llwork);length(llwork)
dffwork<-dff[llwork,]
dfftest<-dff[-llwork,]
```

# maybe no cal posar això de steps to follow????
Steps to follow:

1. Enter all relevant numerical variables in the model
2. See if you need to replace a number with its equivalent factor
3. Add to the best model of step 2, the main effects of the factors and retain the significant net effects.
4. Add interactions: between factor-factor and between factor-numeric (doubles).
5. Diagnosis of waste and observations. Lack of adjustment and / or influential.

## Numerical variables // Enter all relevant numerical variables in the model
### Model 20

```{r}
model_20 <- glm(
  target.tip_is_given~.
  ,family = "binomial"
  ,data=dffwork[,c("target.tip_is_given",vars_cexp)]
);summary(model_20)
Anova(model_20, test="Wald") #binary target
```

```{r}
# Explicació output del summary: --> s'haurà d'esborrar!!!!
# 
# * Deviance Residuals: diferència entre prediccions i observacions.
# * Coeficients: com abans
# * (Dispersion parameter for binomial family taken to be 1): ni cas
# * Residual deviance: suma de quadrats residuals
```

Comments: 

* We can see that the most influent variable, in our case, is the q.hour.
* We can see that the residual deviance is of 1386.6 on 1472  degrees of freedom.

```{r}
vif(model_20)
```

We can see that we have some variables with very high vifs:

* q.trip_distance (66.67)
* q.tlenkm (63.09) --> correlated with the previous
* q.fare_amount (9.48)
* q.traveltime (5.16)

### Model 21

*NOTE: we are aware that we should not have factors in this section, but we have decided to include them due to the fact that we overwrote their numeric values and created their factors in the previous deliverables.*

We know there is not colinearity, so we create a new model:

```{r}
model_21 <- glm(
  target.tip_is_given~
    f.improvement_surcharge+
    f.mta_tax+
    q.passenger_count+
    q.extra+
    q.tolls_amount+
    q.hour+
    q.espeed+
    q.tlenkm+
    q.traveltime 
  ,family = "binomial"
  ,data=dffwork
);summary(model_21)
vif(model_21)

Anova(model_21, test="Wald") #binary target

anova(model_21, model_20, test="Chisq") # only for nested models
```

We can transform tlenkm and remove improvement_surcharge in order to have lower vifs:

### Model 22

```{r}
model_22 <- glm(
  target.tip_is_given~
    f.mta_tax+
    q.passenger_count+
    q.extra+
    q.tolls_amount+
    q.hour+
    q.espeed+
    poly(q.tlenkm,2)+
    q.traveltime 
  ,family = "binomial"
  ,data=dffwork
); summary(model_22)
vif(model_22)
anova(model_21, model_22, test="Chisq") # only for nested models
Anova(model_22, test="Wald") #binary target
```

Now, we can do a step:

### Model 23
```{r}
model_23 <- step(model_22, k=log(nrow(dffwork)))
summary(model_23)
```

Due to the fact that this model is really ppor, we will take also the q.extra variable in order to be able to extract more information. For instance, we could do the marginal plots:

```{r}
model_23 <- glm(
  target.tip_is_given~
    f.mta_tax+
    q.extra
  ,family = "binomial"
  ,data=dffwork
); summary(model_23)
```

### Understanding the model

```{r}
plot(allEffects(model_23))
```

* For the f.mta_tax: we that if the value of the variable is "Yes", it is more probable that the target.tip_is_given value will be "Yes" as well.
* For the q.extra: as we have said before, this variable does not really affect to the target, but we will include it in order to be able to do more plots. At most, we could say that it is inversely proportional to the target.

```{r}
marginalModelPlots(model_23)
```

We can observe that q.extra is a candidate to be a factor.

```{r}
residualPlots(model_23)
```

We see that the smoothers are relatively plain, so we could say that, for now, everything is ok.


We are going, though, to propose a model which brings us more chances:

### Model 24
```{r}
model_24 <- glm(
  target.tip_is_given~
    poly(q.tlenkm, 2)+
    f.mta_tax+
    q.extra+
    q.espeed
  ,family = "binomial"
  ,data=dffwork
); summary(model_24)

residualPlots(model_24)
```

* q.tlenkm: 
  + we see that the smoothe is plain, so it is ok.
  + the "weird" shapes that appear are because of the binery response model.
* q.extra:
  + we observe that the smoother is plain, so it is ok.
* q.espeed: 
  + we see that the smoother is plain, so it is ok.
  + the "weird" shapes that appear are because of the binery response model.
* the whole model:
  + we see that the smoothe is not completely straight, but as it was said in class, we can work with unfitted values in the model, due to the fact that it is a really dense topic.


```{r}
Anova(model_24, test="Wald")
```

We have to ensure that we do not have any variable with a non significant net effect. 

Thus, we are going to redp the model:

```{r}
model_24 <- glm(
  target.tip_is_given~
    poly(q.tlenkm, 2)+
    f.mta_tax
  ,family = "binomial"
  ,data=dffwork
); summary(model_24)

vif(model_24)
residualPlots(model_24)
Anova(model_24, test="Wald")
```

With Anova(model_24), we see that it is fulfilled.


## Factors //See if you need to replace a number with its equivalent factor

We look if any of the numeric variables can be substituted by a factor.

The first thing we will do, it would be change the "q.mta_tax" (if it existed in our dataset) for a "f.mta_tax". Due to the fact that mta_tax is already a factor, we do not need to do this step.

Given that the other variable that could be a factor depends on a polynomial, we keep as it is. The code that should be done in case of a new model with an added factor, would be the following:


```{r}
model_25 <- glm(
  target.tip_is_given~
    poly(q.tlenkm, 2)+
    f.mta_tax
  ,family = "binomial"
  ,data=dffwork
); summary(model_25)

BIC(model_24, model_25) # same model --> same bic
```

Thanks to the BIC(model_24, model_25) we could see te changes generated by the new model. The less the BIC is, the better the model will be. We need to remember that, in case of have done an exchange of a numeric variable to a factor, we could not have done it with an anova test, due to the fact that there is an exchange, which means that any model is bigger than the other.

## Decision //Add to the best model of step 2, the main effects of the factors and retain the significant net effects.

We decide to keep with the model_25.


## Interactions //Add interactions: between factor-factor and between factor-numeric (doubles).

### factor-numeric

Now that we have a defined model, we are going to do some interactions with all of the factor variables we think are the relevant:

```{r}
model_26 <- glm(
  target.tip_is_given~
    ( poly(q.tlenkm, 2) ) * 
    (
      f.mta_tax+
      f.vendor_id+
      f.period+
      f.espeed+
      f.paid_tolls+
      f.tt+
      f.extra
    )
  ,family = "binomial"
  ,data=dffwork
); summary(model_26)

Anova(model_26, test="Wald")
```

We remove the non significant variables:
```{r}
model_27 <- step(model_26, k=log(nrow(dffwork)))
```

From what we can see, it only stays with the tax, but in order to have more freedom, we will keep what we had before.

Hence:
```{r}
model_27 <- glm(
  target.tip_is_given~
    ( poly(q.tlenkm, 2) ) * 
    ( f.mta_tax )
  ,family = "binomial"
  ,data=dffwork
); summary(model_27)

Anova(model_27, test="Wald")
```

We do a comparison:
```{r}
BIC(model_27, model_25)
```

We keep with the 25.

We can see now the effects of it:
```{r}
plot(allEffects(model_25))
```

* We can observe that only the tips is given in certain range of driven km, due to the fact that for few km, it makes no sense to give it, and for many km it is too much.
* As we have previously commented, it is more likely to give some tips if a tax is present.

### factor-factor

Although, for this deliverable it is asked to do some interactions between factors, we will do it even though the results could not be realistic:

```{r}
# interaccions dobles entre factors:
model_factors_1 <- glm(
  target.tip_is_given~
    ( poly(q.tlenkm, 2) + q.extra) + ( f.mta_tax + f.vendor_id + f.espeed)^2
  ,family = "binomial"
  ,data=dffwork
); summary(model_factors_1)
model_factors_1_step <- step(model_factors_1, k=log(nrow(dffwork)))
```

We stick with what we had.


```{r}
# interaccions dobles entre factor-numèrica
model_factors_2 <- glm(
  target.tip_is_given~
    ( poly(q.tlenkm, 2) + q.extra) * ( f.mta_tax + f.vendor_id + f.espeed)
  ,family = "binomial"
  ,data=dffwork
); summary(model_factors_2)
model_factors_2_step <- step(model_factors_2, k=log(nrow(dffwork)))
```

We stick with what we had.


```{r}
# interaccions dobles entre factor-numèrica + dobles entre factors
model_factors_3 <- glm(
  target.tip_is_given~
    ( poly(q.tlenkm, 2) + q.extra) * ( f.mta_tax + f.vendor_id + f.espeed)^2
  ,family = "binomial"
  ,data=dffwork
); summary(model_factors_3)
model_factors_3_step <- step(model_factors_3, k=log(nrow(dffwork)))
```

We stick with what we had.



Conclusion: we stick with the idea that the best model is model_25.

Now, we are going to do some diagnosis:

# Diagnosis //Diagnosis of waste and observations. Lack of adjustment and / or influential.

```{r}
Boxplot(rstudent(model_25), id.n=15)
sout <- which(abs(rstudent(model_25))>2); length(sout) # posem 2 en comptes de 2.5 perquè no tenim observacions en aquell rang
llout <- which(row.names(dffwork) %in% names(rstudent(model_25)[sout])); llout
table(dffwork[llout,]$f.mta_tax, dffwork[llout,]$target.tip_is_given)
```

We see that they are samples that contain mta_tax, but in the other hand they do not have tip.


We are going to determine which are the potencially influent observations:

```{r}
quantile(hatvalues(model_25), seq(0,1,0.1))
mean(hatvalues(model_25))

hh <- 5*mean(hatvalues(model_25)); hh
shat <- which(hatvalues(model_25)>hh); length(shat); shat

summary(dffwork[shat,])

```

They tend to:

* have rate=Rate-Other
* be in the same location (they have very similar latitudes and longitudes)
* have extra=0
* don't have mta_tax
* be at night
* be one passenger
* be long (distance) but short (time)
                                                                                                      
Now, to decide the influences, we are going to take a look at the cook distances:           

```{r}
Boxplot(cooks.distance(model_25))
scoo <- which(cooks.distance(model_25) > 0.02); length(scoo)
llcoo <- which(row.names(dffwork) %in% names(cooks.distance(model_25)[scoo])); llcoo
llista<-influencePlot(model_25, id=c(list="noteworthy", n=10))

summary(dffwork[llcoo,])
```

They tend to:

* have rate=Rate-1
* be in the same location (they have very similar latitudes and longitudes)
* be one passanger
* be between 20 and 60 min long
* have mta_tax
* be long (distance) but short (time)


We redo the model now:

```{r}
llout<-row.names(llista)
ll<-which(row.names(dffwork)%in%llout);
dffwork<-dffwork[-ll,]

model_25 <- glm(
  target.tip_is_given~
    poly(q.tlenkm, 2)+
    f.mta_tax
  ,family = "binomial"
  ,data=dffwork
); summary(model_25)
influencePlot(model_25, id=c(list="noteworthy", n=10))
```

!!!!! falta recalcular interaccions --> si n'hi ha millors, fer el model

# Confusion Table
```{r}
fit.tip_is_given <- factor(ifelse(predict(model_25, type="response")<0.5,0,1), labels=c("fit.no", "fit.yes"))
tt <- table(fit.tip_is_given,dffwork$target.tip_is_given); tt
100*sum(diag(tt)/sum(tt))
```

We have a precision of 83.05%.

```{r}
model_25_better <- glm(
  target.tip_is_given~1
  ,family = "binomial"
  ,data=dffwork
)
fit0 <- predict(model_25_better, type="response")
fit.tip_is_given0 <- factor(ifelse(fit0<0.5,0,1), labels=c("fit.yes"))
tt0 <- table(fit.tip_is_given0, dffwork$target.tip_is_given); tt0
100*sum(tt0[1,2]/sum(tt0))
```

We have a precision of 82.91%.




