---
title: "Deliverable 2"
author: "Júlia Gasull i Claudia Sánchez"
date: \today
output:
  html_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: PCA, CA and Clustering
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# First setups
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```

## Load Required Packages for this deliverable
We load the necessary packages and set working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}

setwd("~/Documents/uni/FIB-ADEI-LAB/deliverable2")
filepath<-"~/Documents/uni/FIB-ADEI-LAB/deliverable2"
#setwd("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2")
#filepath<-"C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2"

# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

## Load processed data from first deliverable
```{r}
load(paste0(filepath,"/Taxi5000_del1.RData"))
summary(df)
```


## Clean data
```{r}
# remove some columns
df$lpep_pickup_datetime <- NULL
df$Lpep_dropoff_datetime <- NULL
df$Store_and_fwd_flag <- NULL
df$Ehail_fee <- NULL
df$CashTips <- NULL

# imputation
library(missMDA)
long_lat<-names(df)[c(3:6)]
imp_long_lat<-imputePCA(df[,long_lat])
df[,long_lat]<-imp_long_lat$completeObs

summary(df)
```


--------------------------------------------------------------------------------

# PCA analysis
We have already seen profiling in the previous installment. So now, let’s proceed to look at the main components.
```{r}
library(FactoMineR); names(df)
pca_vars<-names(df)[c(3:6,7:10,12,15,18,20:22,1:2,16,19,30)]; pca_vars
res.pca<-PCA(df[,pca_vars],quanti.sup=c(1:4,10),quali.sup=c(15:19))
```
Multivariant outliers should be included as supplementary observations:
```{r}
#to-do
```

Now, let's see the plots one by one: (maybe later is better) !!
```{r}
plot.PCA(res.pca,choix=c("var"),invisible=c("var"),cex=0.8)
plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup"),cex=0.8)
plot.PCA(res.pca,choix=c("ind"),invisible=c("ind"),cex=0.5)
```
FALTEN COMENTARIS!

## Eigenvalues and dominant axes analysis
### How many axes we have to interpret according to Kaiser?
```{r}
summary(res.pca,nb.dec=2,nbind=1,nbelements=1000,ncp=2)

# Eigenvalues
#                       Dim.1  Dim.2  Dim.3  Dim.4  Dim.5  Dim.6  Dim.7  Dim.8  Dim.9
# Variance               4.04   1.26   1.02   0.95   0.77   0.75   0.12   0.08   0.00
# % of var.             44.88  13.99  11.36  10.58   8.58   8.33   1.30   0.94   0.04
# Cumulative % of var.  44.88  58.87  70.23  80.81  89.39  97.72  99.01  99.96 100.00
```
According to Kasier, we must use all those dimensions that have a value greater than 1. Therefore, in this case, we will use up to dimension 3, with 70.23% of the accumulated variables.


### How many axes we have to interpret according to Elbow's rule?
As a brief definition, we would say that the elbow rule is based on selecting dimensions until the difference in variance of that of the next factorial plane is almost the same as that of the current plane.

So let's look at exactly where we have this minimal difference:
```{r}
round(res.pca$eig,3)
barplot(res.pca$eig[,1],main="own values",names.arg=paste("dim",1:nrow(res.pca$eig)), border="DarkSlateBlue", col="white")
```
We could say, then, that there is little difference between dimension 3 and 4, or between 5 and 6. Therefore, we could be left with 3 dimensions (as with Kasier) or 5.

## Individuals point of view: Are they any individuals "too contributive"? To better understand the axes meaning use the extreme individuals. Detection of multivariant outliers and influent data.
## Interpreting the axes: Variables point of view coordinates, quality of representation, contribution of the variables  
## Perform a PCA taking into account also supplementary variables the supplementary variables can be quantitative and/or categorical 

# K-Means Classification
## Description of clusters
# Hierarchical Clustering
## Description of clusters

# CA analysis for your data should contain your factor version of the numeric target (previous) in K= 7 (maximum 10) levels and 2 factors:

## Eigenvalues and dominant axes analysis. How many axes we have to consider
## Are there any row categories that can be combined/avoided to explain the discretization of the numeric target.

# MCA analysis for your data should contain:

## Eigenvalues and dominant axes analysis. How many axes we have to consider for next Hierarchical Classification stage?
## Individuals point of view: Are they any individuals "too contributive"? Are there any groups?
## Interpreting map of categories: average profile versus extreme profiles (rare categories)
## Interpreting the axes association to factor map.
## Perform a MCA taking into account also supplementary variables (use all numeric variables) quantitative and/or categorical. How supplementary variables enhance the axis interpretation?

# Hierarchical Clustering (from MCA)

## Description of clusters
## Parangons and class-specific individuals.
## Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on Duration target.
## Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on the binary target.
