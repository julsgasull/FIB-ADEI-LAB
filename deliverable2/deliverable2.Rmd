---
title: "Deliverable 2"
author: "Júlia Gasull i Claudia Sánchez"
date: \today
output:
  html_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: PCA, CA and Clustering
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# First setups
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```

## Load Required Packages for this deliverable
We load the necessary packages and set working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}

#setwd("~/Documents/uni/FIB-ADEI-LAB/deliverable2")
#filepath<-"~/Documents/uni/FIB-ADEI-LAB/deliverable2"
setwd("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2")
filepath<-"C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2"

# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

## Load processed data from first deliverable
```{r}
load(paste0(filepath,"/Taxi5000_del1.RData"))
# summary(df)
```

## Clean data
```{r}
# remove some columns
names(df)
df$lpep_pickup_datetime <- NULL
df$Lpep_dropoff_datetime <- NULL
df$Store_and_fwd_flag <- NULL
df$Ehail_fee <- NULL
df$CashTips <- NULL
df$Sum_total_amount <- NULL
df$yearGt2015 <- NULL

# imputation
library(missMDA)
long_lat<-names(df)[c(3:6)]
imp_long_lat<-imputePCA(df[,long_lat])
df[,long_lat]<-imp_long_lat$completeObs

#summary(df); names(df)
```


--------------------------------------------------------------------------------

## Principal Component Analysis (PCA)
```{r}

# com he fet el pca al final, la unica diferencia es el tema del graph, que pot ser si seria necessari treure'l per poder detectar outliers i correlacions
library(FactoMineR)
names(df)
vars_res<-names(df)[c(15,27)]
vars_quantitatives<-names(df)[c(3:10,12,20:22)]
vars_categorical<-names(df)[c(1,2,16:17,19,25,28)]

res.pca <- PCA(df[,c(1:10,12,13,15:17,19,21,22,25,27)], quanti.sup=c(3:6, 13), quali.sup = c(1,2,14:16,19:20))
```


# PCA analysis
MIRAR!!! http://www.sthda.com/english/wiki/wiki.php?id_contents=7851&fbclid=IwAR01E5XVvCrSKnpkCdAppbpvv7YMGvxSWaSSwb4SIgrXjrxoIpMIlNblYFY

We have already seen profiling in the previous installment. So now, let’s proceed to look at the main components.
```{r}
#library(FactoMineR)#; names(df)
#pca_vars<-names(df)[c(3:6,7:10,12,15,18,20:22,1:2,16,19,30)]#; pca_vars

#res.pca<-PCA(
 #   df[,pca_vars],
  #  quanti.sup = c(1:4,10),
   # quali.sup = c(15:19), 
    #graph = FALSE)
```

Multivariant outliers should be included as supplementary observations:
```{r}
plot.PCA(res.pca,choix=c("var"), invisible=c("quanti.sup"))
plot.PCA(res.pca,choix=c("var"), invisible=c("var"))
plot.PCA(res.pca,choix=c("ind"), invisible=c("ind"))
#TO DO: explicar quisn son multivariant outliers
```

## Eigenvalues and dominant axes analysis
```{r}
#eigenvalues <- res.pca$eig
summary(res.pca, nb.dec=2,nbind=1, nbelements = 1000, ncp=2)
```

Eigenvalues correspond to the amount of the variation explained by each principal component (PC). Eigenvalues are large for the first PC and small for the subsequent PCs.

### How many axes we have to interpret according to Kaiser?
A PC with an eigenvalue > 1 indicates that the PC accounts for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point to determine the number of PCs to retain, using the Kaiser criteria.
```{r}
#head(eigenvalues[, 1:2])
```

In this case, then, we will use up to dimension 3, and they will explain 65.73% of the total inertia.

### How many axes we have to interpret according to Elbow's rule?
As a brief definition, we would say that the elbow rule is based on selecting dimensions until the difference in variance of that of the next factorial plane is almost the same as that of the current plane.

So let's look at exactly where we have this minimal difference:
```{r}
fviz_screeplot(
  res.pca, 
  barfill = "darkslateblue",
  barcolor = "darkslateblue",
  linecolor = "red",
  ggtheme = theme_gray())
```

We could say, then, that there is little difference between dimension 3 and 4, or between 5 and 6. Therefore, we could be left with 3 dimensions (as with Kasier) or 5.

## Individuals point of view
### Contribution
```{r}
head(res.pca$ind$contrib) # contribition of individuals to the princial components
fviz_pca_ind(res.pca, col.ind="contrib", geom = "point") +
scale_color_gradient2(low="darkslateblue", mid="white",
                      high="red", midpoint=0.40)
```

We can see that there are some individuals that are too contributive. So now, let's try to understand them better with extreme individuals.

### Extreme individuals

#### In dimension 1:
```{r}
rang<-order(res.pca$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))

df[which(row.names(df) %in% row.names(df)[rang[(length(rang)-10):length(rang)]]), 1:30]
df[which(row.names(df) %in% row.names(df)[rang[1:10]]),1:30]
```

#### In dimension 2:
```{r}
rang<-order(res.pca$ind$coord[,2])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))

df[which(row.names(df) %in% row.names(df)[rang[(length(rang)-10):length(rang)]]),1:30]
df[which(row.names(df) %in% row.names(df)[rang[1:10]]),1:30]
```

### Detection of multivariant outliers and influent data.
```{r}
# no sé què posar aquí
```


## Interpreting the axes: Variables point of view coordinates, quality of representation, contribution of the variables  
```{r}
res.des <- dimdesc(res.pca)
res.des$Dim.1
```
In the first dimension we see that for the quantitative variables the most positively related, from more to less, are Trip_distance (0.95), Fare_amount (0.94), Total_amount (0.93) and traveltime (0.80). If we take look at the quantitatives ones, we that the most related is Trip_distance_range (0.69) and if we take a look at the categories we see that for the Trip_distance_range category long distance trips show a mean 2.23 units over the global mean and short distance ones show a mean -1.94 units under the global mean, so we can reject the H0 done in the t.Student test.

```{r}
res.des$Dim.2
```

For the second dimension we see that Extra and Passenger_count are the most positively related variables with 0.74 and 0.53 respectively. If we see the qualitative variables we notce that period is the most related with 0.18 even though it is not a very remarkable data. And we see that for this category, period afternoon mean is 0.69 units over the global mean and period morning mean, on the contrary, is -0.61 units under the global mean, so we can reject the H0 done in the t.Student test.

```{r}
res.des$Dim.3
```

For the last dimension we took into account, the third one, we see that the most related variables are Passenger_count (0.53), Tolls_amount (0.53) and espeed (0.51), we also see that traveltime time (-0.40) is inversely related. For the categorical we see that period is the category that is more related with 0.36 even though it is not a big relation.
Finally, we see that the first dimension is the one with the biggest correlations.

## Perform a PCA taking into account also supplementary variables the supplementary variables can be quantitative and/or categorical 

```{r}

plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],pch=19,col="grey30")
points(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],pch=15,col="magenta")
lines(res.pca$quali.sup$coord[3:4,1],res.pca$quali.sup$coord[3:4,2],lwd=2,lty=2,col="black")
text(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],labels=names(res.pca$quali.sup$coord[,1]),col="magenta",cex=0.8)

res.pca$quali.sup$coord
```



# K-Means Classification
## Description of clusters
# Hierarchical Clustering
## Description of clusters

# CA analysis for your data should contain your factor version of the numeric target (previous) in K= 7 (maximum 10) levels and 2 factors:

## Eigenvalues and dominant axes analysis. How many axes we have to consider
## Are there any row categories that can be combined/avoided to explain the discretization of the numeric target.

# MCA analysis for your data should contain:

## Eigenvalues and dominant axes analysis. How many axes we have to consider for next Hierarchical Classification stage?
## Individuals point of view: Are they any individuals "too contributive"? Are there any groups?
## Interpreting map of categories: average profile versus extreme profiles (rare categories)
## Interpreting the axes association to factor map.
## Perform a MCA taking into account also supplementary variables (use all numeric variables) quantitative and/or categorical. How supplementary variables enhance the axis interpretation?

# Hierarchical Clustering (from MCA)

## Description of clusters
## Parangons and class-specific individuals.
## Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on Duration target.
## Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on the binary target.
