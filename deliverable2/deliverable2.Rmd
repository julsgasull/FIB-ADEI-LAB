---
title: "Deliverable 2"
author: "Júlia Gasull i Claudia Sánchez"
date: \today
output:
  html_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: PCA, CA and Clustering
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# First setups
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```

## Load Required Packages for this deliverable
We load the necessary packages and set working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}

setwd("~/Documents/uni/FIB-ADEI-LAB/deliverable2")
filepath<-"~/Documents/uni/FIB-ADEI-LAB/deliverable2"
#setwd("C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2")
#filepath<-"C:/Users/Claudia Sánchez/Desktop/FIB/TARDOR 2020-2021/ADEI/DELIVERABLE1/FIB-ADEI-LAB/deliverable2"

# Load Required Packages
options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

## Load processed data from first deliverable
```{r}
load(paste0(filepath,"/Taxi5000_del1.RData"))
# summary(df)
```


## Clean data
```{r}
# remove some columns
df$lpep_pickup_datetime <- NULL
df$Lpep_dropoff_datetime <- NULL
df$Store_and_fwd_flag <- NULL
df$Ehail_fee <- NULL
df$CashTips <- NULL

# imputation
library(missMDA)
long_lat<-names(df)[c(3:6)]
imp_long_lat<-imputePCA(df[,long_lat])
df[,long_lat]<-imp_long_lat$completeObs

#summary(df); names(df)
```


--------------------------------------------------------------------------------

# PCA analysis
MIRAR!!! http://www.sthda.com/english/wiki/wiki.php?id_contents=7851&fbclid=IwAR01E5XVvCrSKnpkCdAppbpvv7YMGvxSWaSSwb4SIgrXjrxoIpMIlNblYFY

We have already seen profiling in the previous installment. So now, let’s proceed to look at the main components.
```{r}
library(FactoMineR)#; names(df)
pca_vars<-names(df)[c(3:6,7:10,12,15,18,20:22,1:2,16,19,30)]#; pca_vars

res.pca<-PCA(
    df[,pca_vars],
    quanti.sup = c(1:4,10),
    quali.sup = c(15:19), 
    graph = FALSE)
```

Multivariant outliers should be included as supplementary observations:
```{r}
#to-do
```

## Eigenvalues and dominant axes analysis
```{r}
eigenvalues <- res.pca$eig
```

Eigenvalues correspond to the amount of the variation explained by each principal component (PC). Eigenvalues are large for the first PC and small for the subsequent PCs.

### How many axes we have to interpret according to Kaiser?
A PC with an eigenvalue > 1 indicates that the PC accounts for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point to determine the number of PCs to retain.
```{r}
head(eigenvalues[, 1:2])
```

In this case, then, we will use up to dimension 3.

### How many axes we have to interpret according to Elbow's rule?
As a brief definition, we would say that the elbow rule is based on selecting dimensions until the difference in variance of that of the next factorial plane is almost the same as that of the current plane.

So let's look at exactly where we have this minimal difference:
```{r}
fviz_screeplot(
  res.pca, 
  barfill = "darkslateblue",
  barcolor = "darkslateblue",
  linecolor = "red",
  ggtheme = theme_gray())
```

We could say, then, that there is little difference between dimension 3 and 4, or between 5 and 6. Therefore, we could be left with 3 dimensions (as with Kasier) or 5.

## Individuals point of view
### Contribution
```{r}
head(res.pca$ind$contrib) # contribition of individuals to the princial components
fviz_pca_ind(res.pca, col.ind="contrib", geom = "point") +
scale_color_gradient2(low="darkslateblue", mid="white",
                      high="red", midpoint=0.40)
```

We can see that there are some individuals that are too contributive. So now, let's try to understand them better with extreme individuals.

### Extreme individuals

#### In dimension 1:
```{r}
rang<-order(res.pca$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))

df[which(row.names(df) %in% row.names(df)[rang[(length(rang)-10):length(rang)]]), 1:30]
df[which(row.names(df) %in% row.names(df)[rang[1:10]]),1:30]
```

#### In dimension 2:
```{r}
rang<-order(res.pca$ind$coord[,2])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))

df[which(row.names(df) %in% row.names(df)[rang[(length(rang)-10):length(rang)]]),1:30]
df[which(row.names(df) %in% row.names(df)[rang[1:10]]),1:30]
```

### Detection of multivariant outliers and influent data.
```{r}
# no sé què posar aquí
```


## Interpreting the axes: Variables point of view coordinates, quality of representation, contribution of the variables  
## Perform a PCA taking into account also supplementary variables the supplementary variables can be quantitative and/or categorical 

# K-Means Classification
## Description of clusters
# Hierarchical Clustering
## Description of clusters

# CA analysis for your data should contain your factor version of the numeric target (previous) in K= 7 (maximum 10) levels and 2 factors:

## Eigenvalues and dominant axes analysis. How many axes we have to consider
## Are there any row categories that can be combined/avoided to explain the discretization of the numeric target.

# MCA analysis for your data should contain:

## Eigenvalues and dominant axes analysis. How many axes we have to consider for next Hierarchical Classification stage?
## Individuals point of view: Are they any individuals "too contributive"? Are there any groups?
## Interpreting map of categories: average profile versus extreme profiles (rare categories)
## Interpreting the axes association to factor map.
## Perform a MCA taking into account also supplementary variables (use all numeric variables) quantitative and/or categorical. How supplementary variables enhance the axis interpretation?

# Hierarchical Clustering (from MCA)

## Description of clusters
## Parangons and class-specific individuals.
## Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on Duration target.
## Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on the binary target.
