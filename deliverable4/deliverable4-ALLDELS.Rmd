---
title: "Deliverable 4"
author: "Júlia Gasull i Claudia Sánchez"
date: \today
output:
  html_document:
    toc: no
    toc_depth: '4'
  word_document:
    toc: no
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: Final deliverable
classoption: a4paper
editor_options: 
  chunk_output_type: console
---


# First setups
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'hide'}
if(!is.null(dev.list())) dev.off()  # Clear plots
rm(list=ls())                       # Clean workspace
```

## Some useful functions
```{r}
calcQ <- function(x) { # Function to calculate the different quartiles
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3], 
       q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) 
}

countNA <- function(x) { # Function to count the NA values
  mis_x <- NULL
  for (j in 1:ncol(x)) {mis_x[j] <- sum(is.na(x[,j])) }
  mis_x <- as.data.frame(mis_x)
  rownames(mis_x) <- names(x)
  mis_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {mis_i <- mis_i + as.numeric(is.na(x[,j])) }
  list(mis_col=mis_x,mis_ind=mis_i) 
}

countX <- function(x,X) { # Function to count a specific number of appearences
  n_x <- NULL
  for (j in 1:ncol(x)) {n_x[j] <- sum(x[,j]==X) }
  n_x <- as.data.frame(n_x)
  rownames(n_x) <- names(x)
  nx_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {nx_i <- nx_i + as.numeric(x[,j]==X) }
  list(nx_col=n_x,nx_ind=nx_i) 
}
```

# Data description
* Description http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml
* Data Dictionary - SHL Trip Records -This data dictionary describes SHL trip data in visit http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml

## Variables
* VendorID
  * A code indicating the LPEP provider that provided the record.     
  * Values: 
    * 1= Creative Mobile Technologies, LLC
    * 2= VeriFone Inc.   
* lpep_pickup_datetime	
  * The date and time when the meter was engaged.    
* lpep_dropoff_datetime	
  * The date and time when the meter was disengaged.     
* Passenger_count	
  * The number of passengers in the vehicle. 
  * This is a driver-entered value.    
* Trip_distance
  * The elapsed trip distance in miles reported by the taximeter.   
* Pickup_longitude
  * Longitude where the meter was engaged.   
* Pickup_latitude
  * Latitude where the meter was engaged.   
* RateCodeID
  * The final rate code in effect at the end of the trip.
  * Values: 
      * 1=Standard rate  
      * 2=JFK 
      * 3=Newark 
      * 4=Nassau or Westchester 
      * 5=Negotiated fare 
      * 6=Group ride   
* Store_and_fwd_flag	
  * This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka "store and forward," because the vehicle did not have a connection to the server: 
  * Values
    * Y= store and forward trip  
    * N= not a store and forward trip   
* Dropoff_longitude	
  * Longitude where the meter was timed off.   
* Dropoff_latitude	
  * Latitude where the meter was timed off.   
* Payment_type
  * A numeric code signifying how the passenger paid for the trip.
  * Values:
    * 1= Credit card 
    * 2= Cash 
    * 3= No charge 
    * 4= Dispute 
* Fare_amount	
  * The time-and-distance fare calculated by the meter.   
* Extra	 
  * Miscellaneous extras and surcharges.  
  * Currently, this only includes the $0.50 and $1 rush hour and overnight charges. 
* MTA_tax	 
  * $0.50 MTA tax that is automatically triggered based on the metered rate in use.   
* Improvement_surcharge	
  * $0.30 improvement surcharge assessed on hailed trips at the flag   drop. 
  * The improvement surcharge began being levied in 2015.   
* Tip_amount
  * This field is automatically populated for credit card tips. 
  * Cash tips are not included.   
* Tolls_amount	
  * Total amount of all tolls paid in trip.    
* Total_amount	
  * The total amount charged to passengers. 
  * Does not include cash tips.   
* Trip_type	
  * A code indicating whether the trip was a street-hail or a dispatch that is automatically assigned based on the metered rate in use but can be altered by the driver. 
  * Values:
    * 1= Street-hail 
    * 2= Dispatch  

# Load Required Packages for this deliverable
We load the necessary packages and set working directory
```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
setwd("~/Documents/uni/FIB-ADEI-LAB/deliverable4")
filepath<-"~/Documents/uni/FIB-ADEI-LAB/deliverable4"

options(contrasts=c("contr.treatment","contr.treatment"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car","lmtest","ggplot2","moments","factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr")
missingPackages <- requiredPackages[!(requiredPackages %in% installed.packages()[,"Package"])]
if(length(missingPackages)) install.packages(missingPackages)
lapply(requiredPackages, require, character.only = TRUE)
```

# Select a sample of 5000 records
From the proposed database, we need to select a sample of 5000 records randomly so we can start analyzing our data.

!!!!! PER DESCOMENTAR AL FINAL
```{r echo = T, results = 'hide'}
#df<-read.table(paste0(filepath,"/green_tripdata_2016-01.csv"),header=T, sep=",")
#set.seed(180998)
#sam<-as.vector(sort(sample(1:nrow(df),5000)))
#df<-df[sam,]
```

!!! ESBORRAR AL DFINAL
```{r}
load(paste0(filepath,"/Taxi5000_raw.RData"))
```


# Rename variables and clean data
```{r}
summary(df)
names(df)[names(df) == "VendorID"] <- "q.vendor_id"
names(df)[names(df) == "lpep_pickup_datetime"] <- "qual.lpep_pickup_datetime"
names(df)[names(df) == "Lpep_dropoff_datetime"] <- "qual.lpep_dropoff_datetime"
names(df)[names(df) == "Store_and_fwd_flag"] <- "qual.store_and_fwd_flag"
names(df)[names(df) == "RateCodeID"] <- "q.rate_code_id"
names(df)[names(df) == "Pickup_longitude"] <- "q.pickup_longitude"
names(df)[names(df) == "Pickup_latitude"] <- "q.pickup_latitude"
names(df)[names(df) == "Dropoff_longitude"] <- "q.dropoff_longitude"
names(df)[names(df) == "Dropoff_latitude"] <- "q.dropoff_latitude"
names(df)[names(df) == "Passenger_count"] <- "q.passenger_count"
names(df)[names(df) == "Trip_distance"] <- "q.trip_distance"
names(df)[names(df) == "Fare_amount"] <- "q.fare_amount"
names(df)[names(df) == "Extra"] <- "q.extra"
names(df)[names(df) == "MTA_tax"] <- "q.mta_tax"
names(df)[names(df) == "Tip_amount"] <- "q.tip_amount"
names(df)[names(df) == "Tolls_amount"] <- "q.tolls_amount"
df$Ehail_fee <- NULL # deleting it --> only NA's
names(df)[names(df) == "improvement_surcharge"] <- "q.improvement_surcharge"
names(df)[names(df) == "Total_amount"] <- "q.target.total_amount"
names(df)[names(df) == "Payment_type"] <- "q.payment_type"
names(df)[names(df) == "Trip_type"] <- "q.trip_type"
summary(df); names(df)
```


# DELIVERABLE I
## Initiating missings, outliers and errors 
Initialization of counts for missings, outliers and errors. All numerical variables have to be checked before
```{r}
imis<-rep(0,nrow(df)); mis1<-countNA(df); imis<-mis1$mis_ind 
jmis<-rep(0,2*ncol(df))

iouts<-rep(0,nrow(df))
jouts<-rep(0,2*ncol(df))

ierrs<-rep(0,nrow(df))
jerrs<-rep(0,2*ncol(df))
```

## Univariate Descriptive Analysis

### Qualitative Variables (Factors) / Categorical
**Description**: Original numeric variables corresponding to qualitative concepts have to be converted to factors. New factors grouping original levels will be considered very positively.

We need to do an analysis of all the variables to be able to identify missings, errors and outliers. We will also try to factorize each variable to make it easier to understand the sample.

#### New variable: Period
```{r}
df$q.hour<-as.numeric(substr(strptime(df$qual.lpep_pickup_datetime, "%Y-%m-%d %H:%M:%S"),12,13))
df$f.period<-1
df$f.period[df$q.hour>7]<-2
df$f.period[df$q.hour>10]<-3
df$f.period[df$q.hour>16]<-4
df$f.period[df$q.hour>20]<-1
df$f.period<-factor(df$f.period,labels=paste("period",c("night","morning","valley","afternoon")))
barplot(summary(df$f.period),main="period barplot",col="darkslateblue")
```

#### VendorID
This variable expresses the Creative Mobile Technologies, LLC as 1 and Verifone Inc as 2, so we create a factor to make it more readable. With the initial summary we see that this variable does not have any missing value, so we proceed to factor it.
```{r}
names(df)[names(df) == "q.vendor_id"] <- "f.vendor_id"
df$f.vendor_id<-factor(df$f.vendor_id,labels=c("vendor_id_mobile","vendor_id_verifone"))
barplot(summary(df$f.vendor_id),main="vendor_id barplot",col="darkslateblue")
```

#### RateCodeID
This variable expresses the different RateCodeIDs that we can have as numerical values, so we need to categorize them in order to be able to work with them.
```{r}
names(df)[names(df) == "q.rate_code_id"] <- "f.rate_code_id"
df$f.rate_code_id<-factor(df$f.rate_code_id)
barplot(summary(df$f.rate_code_id),main="rate_code_id barplot",col="darkslateblue")
```

We see that most samples are in RateCodeID = 1, which is what we are interested in. Therefore, we factorize and create only two groups, the one with RateCodeID = 1 and the rest.
```{r}
df$f.rate_code_id[df$f.rate_code_id != 1] = 2
df$f.rate_code_id <- factor(df$f.rate_code_id, labels=c("rate_code_id_1","rate_code_id_other"))
barplot(summary(df$f.rate_code_id),main="new rate_code_id barplot",col="darkslateblue")
```
Now is more balanced.

#### Store_and_fwd_flag
This is a categorical variable with the values Y and N, so we need to factor it.
```{r}
names(df)[names(df) == "qual.store_and_fwd_flag"] <- "f.store_and_fwd_flag"
df$f.store_and_fwd_flag<-factor(df$f.store_and_fwd_flag, labels=c("flag-no","flag-yes"))
summary(df$f.store_and_fwd_flag)
```

#### Payment_type
This variable is categorical but it is expressed as numerical, so we need to factor it in order to be able to work with it.
```{r}
names(df)[names(df) == "q.payment_type"] <- "f.payment_type"
df$f.payment_type<-factor(df$f.payment_type,labels=c("credit card","cash","no charge","dispute"))
barplot(summary(df$f.payment_type),main="payment_type barplot",col="darkslateblue")
```

As we can see, there are few values with "No charge" or "Dispute" category, so we decided to categorize it into a new category ("No paid").
```{r}
levels(df$f.payment_type) <- c("credit card","cash","no paid","no paid")
barplot(summary(df$f.payment_type),main="new payment_type barplot",col="darkslateblue")
```

Now is more balanced.

#### Trip_type
This variable is categorical but it is expressed as numerical, so we need to factor it in order to be able to work with it.
```{r}
names(df)[names(df) == "q.trip_type"] <- "f.trip_type"
df$f.trip_type<-factor(df$f.trip_type,labels=c("trip_street_hail","trip_dispatch"))
barplot(summary(df$f.trip_type),main="trip_type barplot",col="darkslateblue")
```

### Quantitative Variables
**Description**: Original numeric variables corresponding to real quantitative concepts are kept as numeric but additional factors should also be created as a discretization of each numeric variable.

We only keep the hours (variables 2 and 3) to be able to work with time slots in the future.

Create new variables derived from the original ones, as effective speed, travel time, hour of request, period of request, effective trip distance (in km) 

#### New variables: Trip Length in km, Travel time un min and Effective speed
##### Trip length in km
```{r}
df$q.tlenkm<-df$q.trip_distance*1.609344 # Miles to km
```
##### Travel time in min
```{r}
df$q.travel_time<-(as.numeric(as.POSIXct(df$qual.lpep_dropoff_datetime)) - as.numeric(as.POSIXct(df$qual.lpep_pickup_datetime)))/60
```
##### Effective speed in km/h
```{r}
df$q.espeed<-(df$q.tlenkm/(df$q.travel_time))*60
```
###### Missing data
```{r}
sel<-which(is.na(df$q.espeed<=0))
imis[sel]<-imis[sel]+1
jmis[25]<-length(sel)
```
###### Error detection
We detect as error those speeds smaller than 0 and bigger than 200
```{r}
summary(df$q.espeed)
sel<-which((df$q.espeed<=0)|(df$q.espeed > 200))
ierrs[sel]<-ierrs[sel]+1
jerrs[25]<-length(sel)
```
Sel contains the rownames of the individuals with "0" as  value for longitude
```{r}
df[sel,"q.espeed"]<-NA 
```
###### Outlier detection
```{r}
Boxplot(df$q.espeed)
var_out<-calcQ(df$q.espeed)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")

llout<-which((df$q.espeed<=3)|(df$q.espeed>80))
iouts[llout]<-iouts[llout]+1
jouts[25]<-length(llout)
df[llout,"q.espeed"]<-NA 
```

#### Lpep pickup datetime
We just keep the hours
```{r}
df$qual.pickup<-substr(strptime(df$qual.lpep_pickup_datetime, "%Y-%m-%d %H:%M:%S"), 12, 13)
```

#### Lpep dropoff datetime
We just keep the hours
```{r}
df$qual.dropoff<-substr(strptime(df$qual.lpep_dropoff_datetime, "%Y-%m-%d %H:%M:%S"), 12, 13)
```

#### Passenger count
```{r}
summary(df$q.passenger_count)
```
We set the 0 as an error because it is not possible to have a trip without passengers
```{r}
sel<-which(df$q.passenger_count == 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[10]<-length(sel)
```
Sel contains the rownames of the individuals with "0" as value for passengers
```{r}
df[sel,"q.passenger_count"]<-NA
```

#### Trip distance
```{r}
summary(df$q.trip_distance)
```

We see on the summary that there are not NA values, so we proceed to the outlier and error detection.

##### Outlier detection
In order to evalute or data, we decide to set the maximum trip distance to 30, so we proceed to delete the outliers.
```{r}
Boxplot(df$q.trip_distance)
var_out<-calcQ(df$q.trip_distance)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
abline(h=30,col="blue",lwd=2)

llout<-which(df$q.trip_distance>30)
iouts[llout]<-iouts[llout]+1
jouts[11]<-length(llout)
```

##### Error detection
We decide that an incorrect trip distance is the one with 0 miles or less. In order to be aware of this error we store it at ierrs, and jerrs. ierrs stores the number of errors in a row, and jerrs stores the total amount of errors in a variable.
```{r}
sel<-which(df$q.trip_distance <= 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[11]<-length(sel)
```

##### Errors and outliers
Now, we set NA values in order to remove errors and outliersfrom the dataset
```{r}
setNA<-which((df$q.trip_distance<=0) | (df$q.trip_distance > 30))
df[setNA,"q.trip_distance"]<-NA
```

##### Caterogial variable for Trip_distance
We are going to set a categorical variable for the f.trip_distance_range 
We decided to create 3 levels: "trip_dist_short", "trip_dist_medium" and"trip_dist_long".
- trip_dist_short: <= 2.5
- trip_dist_medium: 2.5 < q.trip_distance <= 5
- trip_dist_long: > 5
```{r}
df$f.trip_distance_range[df$q.trip_distance <= 2.5] = "trip_dist_short"
df$f.trip_distance_range[(df$q.trip_distance > 2.5) & (df$q.trip_distance <= 5)] = "trip_dist_medium"
df$f.trip_distance_range[df$q.trip_distance > 5] = "trip_dist_long"
df$f.trip_distance_range <- factor(df$f.trip_distance_range)
```

We see a barplot for the factor we created.
```{r}
barplot(table(df$f.trip_distance_range),main="trip_distance_range Barplot",col="darkslateblue")
```


#### Pickup longitude
We know that New York's longitude is -73.9385, so values that differ a lot from this value is an error or an outlier.
```{r}
summary(df$q.pickup_longitude)
```
0.00 looks to be an error
Seeing the individuals with this "0" value: df[which(df[,"q.pickup_longitude"]==0),] it is a quantitive variable. Non-possible values will be recoded as errors, so will be transformed to NA.
```{r}
sel<-which(df$q.pickup_longitude == 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[6]<-length(sel)
df[sel,"q.pickup_longitude"]<-NA   
```
Non-possible values are replaced by NA, missing value symbol in R.

We are deleting trips from outside New York. This means we are not using longitudes bigger than -73.80 and smaller than -74.02.
```{r}
llout <-which((df$q.pickup_longitude < -74.02) | (df$q.pickup_longitude > -73.80))
iouts[llout]<-iouts[llout]+1
jouts[6]<-length(llout)
df[llout,"q.pickup_longitude"]<-NA
```

#### Pickup latitude
We know that New York's latitude is 40.6643, so values that differ a lot from this value is an error or an outlier.
```{r}
summary(df$q.pickup_latitude)
```
0.00 looks to be an error.
Seeing the individuals with this "0" value: df[which(df[,"q.pickup_latitude"]==0),] it is a quantitive variable. non-possible values will be recoded as errors, so will be transformed to NA.
```{r}
sel<-which(df$q.pickup_latitude == 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[7]<-length(sel)
df[sel,"q.pickup_latitude"]<-NA  
```
Non-possible values are replaced by NA, missing value symbol in R. 
We are deleting trips from outside New York. This means we are not using latitudes bigger than 40.54 and smallerthan 40.86
```{r}
llout <-which((df$q.pickup_latitude < 40.54) | (df$q.pickup_latitude > 40.86))
iouts[llout]<-iouts[llout]+1
jouts[7]<-length(llout)
df[llout,"q.pickup_latitude"]<-NA
```

#### Dropoff longitude
We know that New York's longitude is -73.9385, so values that differ a lot from this value is an error or an outlier.
```{r}
summary(df$q.dropoff_longitude)
```
0.00 looks to be an error
Seeing the individuals with this "0" value: df[which(df[,"q.dropoff_longitude"]==0),] it is a quantitive variable.  
Non-possible values will be recoded as errors, so will be transformed to NA.
```{r}
sel<-which(df$q.dropoff_longitude == 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[8]<-length(sel)
df[sel,"q.dropoff_longitude"]<-NA 
```
Non-possible values are replaced by NA, missing value symbol in R.
We are deleting trips from outside New York. This means we are not using longitudes bigger than -73.80 and smaller than -74.02.
```{r}
llout <-which((df$q.dropoff_longitude < -74.02) | (df$q.dropoff_longitude > -73.80))
iouts[llout]<-iouts[llout]+1
jouts[8]<-length(llout)
df[llout,"q.dropoff_longitude"]<-NA
```

#### Dropoff latitude
We know that New York's latitude is 40.6643, so values that differ a lot from this value is an error or an outlier.
```{r}
summary(df$q.dropoff_latitude)
```
0.00 looks to be an error
Seeing the individuals with this "0" value: df[which(df[,"q.dropoff_latitude"]==0),] it is a quantitive variable. Non-possible values will be recoded as errors, so will be transformed to NA.
```{r}
sel<-which(df$q.dropoff_latitude == 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[9]<-length(sel)
```
Sel contains the rownames of the individuals with "0" as value for longitude
```{r}
df[sel,"q.dropoff_latitude"]<-NA   
```
Non-possible values are replaced by NA, missing value symbol in R. We are deleting trips from outside New York. This means we are not using latitude bigger than 40.54 and smaller than 40.86
```{r}
llout <-which((df$q.dropoff_latitude < 40.54) | (df$q.dropoff_latitude > 40.86))
iouts[llout]<-iouts[llout]+1
jouts[9]<-length(llout)
```
Now that we have the outliers, we are setting them as NA
```{r}
df[llout,"q.dropoff_latitude"]<-NA
```

#### Fare amount
We know that the fare should be positive, as it is the price of the trip, so we'll treat as error those values. The next we'll do is decide the outliers.

```{r}
summary(df$q.fare_amount)
sel<-which(df$q.fare_amount <= 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[12]<-length(sel)
df[sel,"q.fare_amount"]<-NA    
```
Non-possible values are replaced by NA, missing value symbol in R

##### Outlier detection
```{r}
Boxplot(df$q.fare_amount)
var_out<-calcQ(df$q.fare_amount)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
abline(h=60,col="blue",lwd=2)
```

We decide to set outliers for fare amounts bigger than 60, because the majority of the values are concentrated between 0 and 60.
```{r}
llout<-which(df$q.fare_amount>60)
iouts[llout]<-iouts[llout]+1
jouts[12]<-length(llout)
df[llout,"q.fare_amount"]<-NA 
```

#### Extra
As this variable is price related, it cannot have negative values, so this individuals will be treated as errors.
```{r}
table(df$q.extra)
```
As it is a price related variable, negative values should be treated as errors, and the other values are the ones defined for this variable, so there are not outliers.
```{r}
sel<-which(df$q.extra < 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[13]<-length(sel)
df[sel,"q.extra"]<-NA 
```

#### Mta tax
This variable corresponds to a tax that must be charged in every trip and its cost is $0.50, so values different from this are errors, and we don't have to take into account outliers because after the errors detection all values should be the MTA_tax.
```{r}
table(df$q.mta_tax)
```

**Important note:** We assume that when this tax is smaller than 0, it is an error. If tax is 0, we say that payment in these cases is equivalent to “no paid”.
```{r}
sel<-which(df$q.mta_tax < 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[14]<-length(sel)
df[sel,"q.mta_tax"]<-NA 
```

#### Improvement surcharge
This variable corresponds to a charge that must be charged in every trip and its cost is $0.30, so values smaller than 0 are errors, and we don't have to take into account outliers because after the errors detection all values should be the Improvement surcharge.
```{r}
table(df$q.improvement_surcharge)
```
We see that the 0 individuals are errors.
```{r}
sel<-which(df$q.improvement_surcharge < 0)
ierrs[sel]<-ierrs[sel]+1
jerrs[17]<-length(sel)
df[sel,"q.improvement_surcharge"]<-NA 
```

#### Tip amount
As this is a price related variable, negative values should be considered as errors, and big tips should be considered as outliers. Also tip amounts bigger than 0 for individuals with payment_type = "Cash" should be considered as errors as well.
```{r}
summary(df$q.tip_amount)
```
We proceed to check if the 0 values are related with payment_type = "credit card" and the passenger did not tip.
```{r}
table(df$q.tip_amount>0, df$f.payment_type)
```

Now, we proceed to the outlier detection.

#### Outlier detection
```{r}
Boxplot(df$q.tip_amount)
var_out<-calcQ(df$q.tip_amount)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
abline(h=40,col="blue",lwd=2)

llout<-which(df$q.tip_amount>40)
iouts[llout]<-iouts[llout]+1
jouts[15]<-length(llout)
df[llout,"q.tip_amount"]<-NA 
```

#### Tolls amount
As this is a price related variable, negative values should be considered as errors.
```{r}
table(df$q.tolls_amount)
```
We see that there are not negative values, so we do not have errors. We proceed now to the outlier detection.
```{r}
Boxplot(df$q.tolls_amount)
var_out<-calcQ(df$q.tolls_amount)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
```
As we see in the boxplot and the table, the majority of the individuals are 0, so the values bigger than 5.54 will be outliers.
```{r}
llout<-which(df$q.tolls_amount>5.54)
iouts[llout]<-iouts[llout]+1
jouts[16]<-length(llout)
df[llout,"q.tolls_amount"]<-NA 
```

#### Total amount
This is a price related variable, so negative values should be treated as errors. Also, we need to sum the "q.fare_amount", "q.extra","q.mta_tax", "q.improvement_surcharge", "q.tip_amount" and the "q.tolls_amount" in order to see if the q.target.total_amount matches with this sum.
```{r}
summary(df$q.target.total_amount)
```
Negative values seem to be errors 
- 0 Total_amount is possible when Payment_type =="No charge"

We proceed to check if total amount is correctsumming the other variables and checking negatives values:
```{r}
sum_total_amount = (
  df$q.fare_amount + 
  df$q.extra + 
  df$q.mta_tax + 
  df$q.improvement_surcharge + 
  df$q.tip_amount + 
  df$q.tolls_amount
)

sel<-which((df$q.target.total_amount != sum_total_amount) | (df$q.target.total_amount<0))
if (length(sel)>0) {
  ierrs[sel]<-ierrs[sel]+1
  jerrs[18]<-length(sel)
}
df[sel,"q.target.total_amount"]<-NA
```
#### Outlier detection
```{r}
Boxplot(df$q.target.total_amount)
var_out<-calcQ(df$q.target.total_amount)
abline(h=var_out$souts,col="red")
abline(h=var_out$souti,col="red")
abline(h=150,col="blue",lwd=2)

llout<-which(df$q.target.total_amount>150)
iouts[llout]<-iouts[llout]+1
jouts[18]<-length(llout)
df[llout,"q.target.total_amount"]<-NA 
```

--------------------------------------------------------------------------------

## Data Quality Report

### Per variable
Per each variable, we have to count the following:

* number of missing values
* number of errors (including inconsistencies)
* number of outliers
* rank variables according the sum of missing values (and errors).

#### Number of missing values of each variable (with ranking)
```{r}
missings_ranking_sortlist <- sort.list(mis1$mis_col, decreasing = TRUE)
for (i in missings_ranking_sortlist) {
  print(paste(names(df)[i], " : ", mis1$mis_col$mis_x[i]))
}
```

#### Number of errors per each variable (with ranking)
```{r}
errors_ranking_sortlist <- sort.list(jerrs, decreasing = TRUE)
for (i in errors_ranking_sortlist) {
  if(!is.na(names(df)[i])) { print(paste(names(df)[i], " : ", jerrs[i])) }
}
```

#### Number of outliers per each variable (with ranking)
```{r}
errors_ranking_sortlist <- sort.list(jouts, decreasing = TRUE)
for (i in errors_ranking_sortlist) {
  if(!is.na(names(df)[i])) print(paste(names(df)[i], " : ", jouts[i]))
}
```

### Per individual
Per each individuals, we have to count the following:

* number of missing values
* number of errors
* number of outliers

#### Number of missing values
```{r}
barplot(table(imis),main="missings per individual barplot",col="darkslateblue")
```

We see that there are no native missing values (remember we deleted Ehail_fee).


#### Number of errors
As we can see, most individuals have no mistakes.
```{r}
barplot(table(ierrs),main="errors per individual earplot",col="darkslateblue")
```

#### Number of outliers
```{r}
barplot(table(iouts),main="Outliers per individual Barplot",col="darkslateblue")
```

### Create variable adding the total number missing values, outliers and errors
```{r}
total_missings <- 0; total_outliers <- 0; total_errors <- 0;
for (m in imis) {total_missings <- total_missings + m} 
for (o in iouts) {total_outliers <- total_outliers + o}
for (e in ierrs) {total_errors <- total_errors + e}
```
Now, let's print this variables:
```{r}
total_missings
total_outliers
total_errors
```

--------------------------------------------------------------------------------
## Delete some unecessary variables
```{r}
df$qual.lpep_pickup_datetime <- NULL
df$qual.lpep_dropoff_datetime <- NULL
names(df)
```
--------------------------------------------------------------------------------

## Imputation
```{r}
library(missMDA)
```

What we do with imputation is be able to eliminate all those values that may be missings, outliers or errors to turn them into values that can be realistic within our sample.

### Numeric variables
We will now do the study by variables and try to impute the necessary observations.

**Note**: we do not include MTA_tax (14) nor improvement_surcharge(18). We proceed to delete NA values from Total_amount because it is our target variable, so we do not impute it, but we need to have this variable without NAs.
```{r}
df <- df[!is.na(df$q.target.total_amount),]
names(df)
vars_quantitatives <- names(df)[c(4,5,6,7,8,9,10,11,12,13,14,15,16,21,22,23)]
#  [1] "q.pickup_longitude"      "q.pickup_latitude"      
#  [3] "q.dropoff_longitude"     "q.dropoff_latitude"     
#  [5] "q.passenger_count"       "q.trip_distance"        
#  [7] "q.fare_amount"           "q.extra"                
#  [9] "q.mta_tax"               "q.tip_amount"           
# [11] "q.tolls_amount"          "q.improvement_surcharge"
# [13] "q.target.total_amount"   "q.tlenkm"               
# [15] "q.travel_time"           "q.espeed"   
```

```{r}
summary(df[,vars_quantitatives])
res.imputation<-imputePCA(df[,vars_quantitatives],ncp=5)
summary(res.imputation$completeObs)
```

We proceed now to fix all the numeric variables that have errors or outliers:

#### q.pickup_longitude
```{r}
summary(res.imputation$completeObs[,"q.pickup_longitude"])
```
#### q.pickup_latitude    
```{r}
summary(res.imputation$completeObs[,"q.pickup_latitude"])
```
#### q.dropoff_longitude
```{r}
summary(res.imputation$completeObs[,"q.dropoff_longitude"])
```
#### q.dropoff_latitude  
```{r}
summary(res.imputation$completeObs[,"q.dropoff_latitude"])
```
#### q.passenger_count
We decided to create categorical for this variable so we categorize it for single passengers, couple and groups (3 or more)
```{r}
df$f.passenger_groups[res.imputation$completeObs[,"q.passenger_count"]  == 1] = "passenger_single"
df$f.passenger_groups[res.imputation$completeObs[,"q.passenger_count"] > 1 & res.imputation$completeObs[,"q.passenger_count"] <= 2] = "passenger_couple"
df$f.passenger_groups[res.imputation$completeObs[,"q.passenger_count"] >= 3] = "passenger_group"
df$f.passenger_groups <- factor(df$f.passenger_groups)
```
We see the barplot in order to see the distribution of passenger per trip
```{r}
barplot(table(df$f.passenger_groups),main="passenger_groups barplot",col="darkslateblue")
```

#### q.trip_distance   
```{r}
ll<-which(res.imputation$completeObs[,"q.trip_distance"] < 0)
res.imputation$completeObs[ll,"q.trip_distance"] <- 1
ll<-which(res.imputation$completeObs[,"q.trip_distance"] > 30)
res.imputation$completeObs[ll,"q.trip_distance"] <- 30
```

#### q.fare_amount
```{r}
ll<-which(res.imputation$completeObs[,"q.fare_amount"] > 60)
res.imputation$completeObs[ll,"q.fare_amount"] <- 60
```
#### q.extra
If we execute a table, we'll see that we have 0, 0'5 and 1 values, so we proceed to categorize this variable to see if has extra or not.
```{r}
table(df$q.extra)
df$f.extra[df$q.extra == 0] = 0
df$f.extra[df$q.extra > 0] = 1
df$f.extra<-factor(df$f.extra, labels=c("extra_no","extra_yes"))
```
We see the barplot in order to see the distribution.
```{r}
barplot(table(df$f.extra),main="extra barplot",col="darkslateblue")
```
#### q.mta_tax
If we execute a summary, we'll see that every value should be 0.5 or 0, so we proceed to categorize this variable in order to see if the tax has been paid or not.
```{r}
table(df$q.mta_tax)
df$f.mta_tax<-factor(df$q.mta_tax, labels =c("mta_no","mta_yes"))
```
We see the barplot in order to see the distribution.
```{r}
barplot(table(df$q.mta_tax),main="mta_tax barplot",col="darkslateblue")
```
#### q.tip_amount 
```{r}
ll<-which(res.imputation$completeObs[,"q.tip_amount"] > 17)
res.imputation$completeObs[ll,"q.tip_amount"] <- 17
```
We see that we have correct data, so we proceed to create the binary factor TipIsGiven.
```{r}
df$f.target.tip_is_given[(res.imputation$completeObs[,"q.tip_amount"] > 0)] = "tip_yes"
df$f.target.tip_is_given[(res.imputation$completeObs[,"q.tip_amount"] == 0)] = "tip_no"
df$f.target.tip_is_given <- factor(df$f.target.tip_is_given)
summary(df$f.target.tip_is_given)
```
#### q.tolls_amount
As we checked before the imputation and detected as errors those individuals with negative amount, the negative values found now are going to be set as 0 because they result negative during the imputation. After treating this values, we proceed to categorize this variable to see if an individual has paid or not  for a toll.
```{r}
ll<-which(res.imputation$completeObs[,"q.tolls_amount"] < 0)
res.imputation$completeObs[ll,"q.tolls_amount"] <- 0

df$f.paid_tolls[res.imputation$completeObs[,"q.tolls_amount"] == 0] = "tolls_no"
df$f.paid_tolls[res.imputation$completeObs[,"q.tolls_amount"] > 0] = "tolls_yes"
df$f.paid_tolls <- factor(df$f.paid_tolls)
```

#### q.improvement_surcharge
If we execute a table, we'll see that every value should be 0.3 or 0, so we proceed to categorize this variable in order to see if the surcharge has been paid or not.
```{r}
table(df$q.improvement_surcharge)
df$f.improvement_surcharge<-factor(df$q.improvement_surcharge, labels=c("improvement_no","improvement_yes"))
```
We see the barplot in order to see the distribution.
```{r}
barplot(table(df$f.improvement_surcharge),main="improvement_surcharge barplot",col="darkslateblue")
```

#### q.tlenkm
```{r}
ll<-which(res.imputation$completeObs[,"q.tlenkm"] <= 1)
res.imputation$completeObs[ll,"q.tlenkm"] <- 1
ll<-which(res.imputation$completeObs[,"q.tlenkm"] > 48.28)
res.imputation$completeObs[ll,"q.tlenkm"] <- 48.28
```
#### q.travel_time     
```{r}
ll<-which(res.imputation$completeObs[,"q.tlenkm"] > 60)
res.imputation$completeObs[ll,"q.tlenkm"] <- 60
```
#### q.espeed
```{r}
ll<-which(res.imputation$completeObs[,"q.espeed"] < 3)
res.imputation$completeObs[ll,"q.espeed"] <- 3
ll<-which(res.imputation$completeObs[,"q.espeed"] > 55)
res.imputation$completeObs[ll,"q.espeed"] <- 55
```

#### Store imputation
We proceed to impute all NAs in our numerical variables that are stored in: `res.imputation$completeObs`
```{r}
df[,vars_quantitatives] <- res.imputation$completeObs
```


### Categorical variables / Factors
```{r}
vars_categorical<-names(df)[c(1,2,3,17,18,20,26,27,28,29,30,31,32)]
summary(df[,vars_categorical])
res.input<-imputeMCA(df[,vars_categorical],ncp=10)
summary(res.input$completeObs)
```

#### Store imputation
We proceed to impute all NAs in our numerical variables that are stored in: `res.input$completeObs`
```{r}
df[,vars_categorical] <- res.input$completeObs
```

--------------------------------------------------------------------------------

### Create some other factors after imputation
#### f.dist
```{r}
df$f.dist[df$q.trip_distance<=1.6] = "(0, 1.6]"
df$f.dist[(df$q.trip_distance>1.6) & (df$q.trip_distance<=3)] = "(1.6, 3]"
df$f.dist[(df$q.trip_distance>3) & (df$q.trip_distance<=5.5)] = "(3, 5.5]"
df$f.dist[(df$q.trip_distance>5.5) & (df$q.trip_distance<=30)] = "(5.5, 30]"
df$f.dist<-factor(df$f.dist)
```

#### f.hour
```{r}
df$f.hour[(df$q.hour>=17) & (df$q.hour<18)] = "17"
df$f.hour[(df$q.hour>=18) & (df$q.hour<19)] = "18"
df$f.hour[(df$q.hour>=19) & (df$q.hour<20)] = "19"
df$f.hour[(df$q.hour>=20) & (df$q.hour<21)] = "20"
df$f.hour[(df$q.hour>=21) & (df$q.hour<22)] = "21"
df$f.hour[(df$q.hour>=22) & (df$q.hour<23)] = "22"
df$f.hour[(df$q.hour<17)] = "other"
df$f.hour[(df$q.hour>=23)] = "other"
df$f.hour<-factor(df$f.hour)
```

#### f.espeed
```{r}
df$f.espeed[(df$q.espeed>=3) & (df$q.espeed<20)]  = "[03,20)"
df$f.espeed[(df$q.espeed>=20) & (df$q.espeed<40)] = "[20,40)"
df$f.espeed[(df$q.espeed>=40) & (df$q.espeed<=55)] = "[40,55]"
df$f.espeed<-factor(df$f.espeed)
```

--------------------------------------------------------------------------------

### Describe these variables, to which other variables exist higher associations
#### Compute the correlation with all other variables. 
We are skipping longitudes and latitudes.
```{r}
library(mvoutlier)
library(FactoMineR)
vars_quantitatives_no_coords <- names(df)[c(8,9,10,11,12,13,14,15,16,21,22,23)]
res <- cor(df[,vars_quantitatives_no_coords])
round(res, 2)
```

#### Rank these variables according the correlation:
```{r}
library(corrplot)
corrplot(res,method="square",type="upper",tl.col="black",tl.cex=0.75,)
```

As we can see in this graph, we have the correlation between all quantitative variables. We must say, however, that there are two variables (espeed and traveltime) which we had to modify when making the imputation.

Now, let's describe each correlation we obtained in the graph (we will only mention one relation once):

* Diagonals: Being exactly the same variable, it is directly related to itself.
* q.passanger_count: not too related to any other not seen before
* q.trip_distance
  + w/ q.fare_amount: More distance, more time, therefore more price.
  + w/ q.tip_amount: If the trip has been longer, there may be more reason to tip.
  + w/ q.target.total_amount: As before, more distance, more time, therefore more price.
  + w/ q.tlenkm: They are exactly the same, only with a metric change.
  + w/ q.travel_time: The further away, the longer.
  + w/ q.espeed: The reason we think these variables are related to a direct and positive proportion is that since short trips have to be, logically cheaper, what taxi drivers do is slow down so that the trip take longer and thus charge more. Therefore, by increasing the distance of the journey, taxi drivers do not need to go so slow and therefore the speed increases.
* q.fare_amount:
  + w/ q.tip_amount: In the USA it is normal to give a tip proportional to the price of the service that has been offered.
  + w/ q.target.total_amount: The variable q.target.total_amount is equivalent to q.fare_amount plus the fees, tips, among others, that have been applied to the trip.
  + w/ q.tlenkm: As before, more distance, more time, therefore more price
  + w/ q.travel_time: More time, more price.
  + w/ q.espeed: As we said before, more speed means more distance, therefore more travel time, causing more price.
* q.extra: not too related to any other not seen before
* q.mta_tax:
  + w/ q.improvement_subcharge: if there's a tax, the most probable thing to happen is that there's an improvement subcharge too
* q.tip_amount:
  + w/ q.target.total_amount: As before, in the USA it is normal to give a tip proportional to the price of the service that has been offered.
  + w/ q.tlenkm: If the trip has been longer, there may be more reason to tip.
  + w/ q.travel_time: The longer it takes, the more price, and therefore the more tip given the proportionality.
  + w/ q.espeed: The more speed, as we said before, the more distance, and therefore the longer it takes. This causes more price and therefore more tip.
* q.tolls_amount: not too related to any other not seen before
* q.improvement_subcharge: not too related to any other not seen before
* q.target.total_amount:
  + w/ q.tlenkm: More distance, more time, therefore more price.
  + w/ q.espeed: As we said before, more speed means more distance, therefore more travel time, causing more price.
* q.tlenkm
  + Same as for q.trip_distance + q.espeed correlation.
* q.travel_time: not too related to any other not seen before
* q.espeed: not too related to any other not seen before

--------------------------------------------------------------------------------

#### Identify individuals considered as multivariant outliers
```{r}
library(mvoutlier)
library(chemometrics)   
multivariant_outliers <- Moutlier(df[, c(9,10,16,23)], quantile = 0.995)
multivariant_outliers$cutoff
par(mfrow=c(1,1))
plot(multivariant_outliers$md, multivariant_outliers$rd, type="n")
text(multivariant_outliers$md, multivariant_outliers$rd, labels=rownames(df[, c(9,10,16,23)]), cex=0.5) 
```

As we can see, above the defined line we have all the possible observations that we call multivariate outliers. These mean that, viewed only from the point of view of a variable, it does not have to be an outlier, but that viewed with various dimensions (variables), it may be so.

Anem a feer una mirada ràpida dels outliers multivariants més pronunciats per entendre com són:
```{r}
df[which(row.names(df)=="1208612"), 1:35]
```

* q.trip_distance = 7.828788 (1km)
* q.fare_amount = $2.5
* q.target.total_amount = $100.3
* q.espeed = 28.72946

We can see that, for only 1km, the whole trip cost $100, and that's not normal.

We are removing them from our dataset:

```{r}
df <- subset(df, !(multivariant_outliers$md>10 | multivariant_outliers$rd>60))

multivariant_outliers <- Moutlier(df[, c(9,10,16,23)], quantile = 0.995)
par(mfrow=c(1,1))
plot(multivariant_outliers$md, multivariant_outliers$rd, type="n")
text(multivariant_outliers$md, multivariant_outliers$rd, labels=rownames(df[, c(9,10,16,23)]), cex=0.5) 
```

We can see now that there are not multivariant outliers in our dataframe.

--------------------------------------------------------------------------------

## Profiling
### Numeric target: q.target.total_amount
Profiling is used to finish profiling our sample.

We will now proceed to the profiling that asks us for our numeric target (Total_amount) and then we have to use the original variables and factors.

In order to observe the relationship of our numerical target with the other variables we use the condes tool that provides us with information about the relationships between the indicated variables and the target.

```{r}
library(FactoMineR)
summary(df$q.target.total_amount)
```

```{r}
vars_res<-names(df)[c(16,30)]
vars_quantitatives<-names(df)[c(8:15,21:23)]
vars_categorical<-names(df)[c(1:3,17,18,20,26:29,31,32)]

res.condes <- condes(df[, c(vars_res,vars_quantitatives, vars_categorical)],1)
```

Let's now look at the correlations between our Total_amount target and the variables in the following groups. We will basically look at p.value, which we know that the smaller the correlation between the variables.


##### Numerical variables
```{r}
res.condes$quanti
```

For the lowest p.values:

* q.fare_amount: The variable q.target.total_amount is equivalent to q.fare_amount plus the fees, tips, among others, that have been applied to the trip.
* q.trip_distance: As before, more distance, more time, therefore more price.
* q.tlenkm: More distance, more time, therefore more price.
* q.tip_amount: The more you pay, since the tip is a proportion of the final price, the more it will increase.
* q.espeed: As we said before, more speed means more distance, therefore more travel time, causing more price.

##### Qualitative variables
```{r}
res.condes$quali
```

For the lowest p.values:

* f.trip_distance_range: Obviously, the longer the journey, the longer it will take and the more price it will have.
* f.paid_tolls: The variable q.target.total_amount is equivalent to f.paid_tolls plus the fees, tips, among others, that have been applied to the trip.
* f.target.tip_is_given: Like before, the more you pay, since the tip is a proportion of the final price, the more it will increase.

##### Categorical variables
```{r}
res.condes$category
```

For the lowest p.values:

* f.trip_distance_range=trip_dist_long: We can see that, the further away, the more correlation, as it takes longer to travel.
* f.paid_tolls=tolls_yes: If tolls are paid, then there's more cost at the end.
* f.target.tip_is_given=tip_yes:We see that it is more likely to tip if the price is high.
* f.payment_type=credit card: We see that it is easier for the guy to be with credit card if the trip costs more.
* f.rate_code_id: As we have seen before, virtually all observations were of type 1. Therefore it is not worth looking at the correlation.
* f.period=period morning: We see that in the morning travel costs less.

### Factor (Y.bin - f.target.tip_is_given)
And now, we are profiling the qualitative target:
```{r}
res.catdes <- catdes(df[, c(vars_res,vars_quantitatives, vars_categorical)],2)
```

Let's now look at the correlations between our f.target.tip_is_given target and the variables in the following groups. We will basically look at p.value, which we know that the smaller the correlation between the variables.

##### Test.Chi2
```{r}
res.catdes$test.chi2
```

For the lowest p.values:

* f.payment_type: We see that it is very likely that there will be a tip if it is paid in a concise manner.
* f.trip_distance_range: As we can see, there is tip as long as the trip is, or very short, or very long.

##### Quantitative variables
```{r}
res.catdes$quanti.var
```

For the lowest p.values:

* q.tip_amount: If there is a tip, it must have value.
* q.target.total_amount: We see that it is more likely to tip if the price is high.
* q.fare_amount: We see that it is more likely to tip if the price is high.
* q.trip_distance: Exactly the same as above.
* q.tlenkm: The more distance, the more time, therefore the more price. So, more chances of there being a tip.


##### > Categorical variables
```{r}
res.catdes$category
```

* f.payment_type: As we saw before, there is only a tip if the payment is done with a credit card.
* f.trip_distance_range: As we can see, there is tip as long as the trip is, or very short, or very long.
* f.mta_tax: We see that it is very likely that there will be a tip if there is a tax included.
* f.improvement_surcharge: We see that it is very likely that there will be a tip if there is the improvement subcharge included.
* f.trip_type: We don't think the type of trip is important.
* f.rate_code_id: As we have seen before, virtually all observations were of type 1. Therefore it is not worth looking at the correlation.
* f.period: We see that in the morning people are not in a very good mood and are more inclined to tip the "valley".

--------------------------------------------------------------------------------

# DELIVERABLE II

## Clean the data
```{r}
df$f.store_and_fwd_flag <- NULL
```


## Principal Component Analysis (PCA)
```{r}
names(df)
vars_res<-names(df)[c(15,29)]
vars_quantitatives<-names(df)[c(3,4,5,6,7,8,9,10,12,20,21,22)]
vars_categorical<-names(df)[c(1,2,16,17,19,25,30)]
```

We have already seen profiling in the previous installment. So now, let’s proceed to look at the main components.
```{r}
library(FactoMineR)
res.pca <- PCA(df[,c(1,2,3,4,5,6,7,8,9,10,12,13,15,16,17,19,21,22,25,26,29)],quanti.sup=c(3:6,13),quali.sup=c(1,2,14:16,19:21))
```

As we know, those variables that have an angle of 90 degrees, are not related. Taking a first look at the PCA obtained, we see that, for example, q.extra and q.travel_time are not at all related. On the other hand, also looking at q.travel_time, we see that it is very positively related to q.extra. If there were a variable that went in the opposite direction, we would say that it is inversely related.

### Multivariant outliers should be included as supplementary observations
We deleted the multivariant outliers.

### Eigenvalues and dominant axes analysis
Eigenvalues correspond to the amount of the variation explained by each principal component (PC). Eigenvalues are large for the first PC and small for the subsequent PCs.

#### How many axes we have to interpret according to Kaiser?
A PC with an eigenvalue > 1 indicates that the PC accounts for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point to determine the number of PCs to retain, using the Kaiser criteria.
```{r}
eigenvalues <- res.pca$eig
head(eigenvalues[, 1:3])
```

In this case, then, we will use up to dimension 3, and they will explain 59.57% of the total inertia.

#### How many axes we have to interpret according to Elbow's rule?
As a brief definition, we would say that the elbow rule is based on selecting dimensions until the difference in variance of that of the next factorial plane is almost the same as that of the current plane.

So let's look at exactly where we have this minimal difference:
```{r}
fviz_screeplot(res.pca,addlabels=TRUE,ylim=c(0,50),barfill="darkslateblue",barcolor="darkslateblue",linecolor="skyblue1")
```

We could say, then, that there is little difference between dimension 3 and 4, or between 5 and 6. Therefore, we could be left with 3 dimensions (as with Kasier) or 5.

### Individuals point of view
#### Contribution
```{r}
fviz_pca_ind(res.pca, col.ind="contrib", geom = "point") +
scale_color_gradient2(low="darkslateblue", mid="white",
                      high="red", midpoint=0.40)
```

We can see that there are some individuals that are too contributive. So now, let's try to understand them better with extreme individuals.

#### Extreme individuals

##### In dimension 1:
```{r}
rang<-order(res.pca$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])

contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:
```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:34]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:34]
```

!! FALTA DESCRIPCIÓ

##### In dimension 2:
```{r}
rang<-order(res.pca$ind$coord[,2])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])

contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:
```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:34]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:34]
```

!! FALTA DESCRIPCIÓ

#### Detection of multivariant outliers and influent data.
Since we’ve commented before that we don’t consider multivariate outliers, no action should be taken here.

### Interpreting the axes: Variables point of view coordinates, quality of representation, contribution of the variables
```{r}
res.des <- dimdesc(res.pca)
```

#### First dimension
```{r}
fviz_contrib(res.pca,fill="darkslateblue",color="darkslateblue",choice="var",axes=1,top=5)
res.des$Dim.1
```

In the first dimension we see that for the **quantitative** variables the most positively related, from more to less, are:

* q.trip_distance  (0.95)
* q.target.total_amount (0.94)
* q.fare_amount (0.91)

If we take look at the **qualitatives** ones, we that the most related is

* f.trip_distance_range (0.73)

Finally, if we take a look at the **categories** we see that for the f.trip_distance_range=trip_dist_long category long distance trips show a mean 2.19 units over the global mean and f.trip_distance_range=trip_dist_short show a mean -1.81 units under the global mean, so we can reject the H0 done in the t.Student test.

#### Second dimension
```{r}
fviz_contrib(res.pca,fill="darkslateblue",color="darkslateblue",choice="var",axes=2,top=5)
res.des$Dim.2
```

For the second dimension we see that or the **quantitative** variables q.travel_time and q.extra are the most positively related ones with 0.84 and 0.44 respectively.

If we see the **qualitative** variables we notice that period is the most related with 0.06 even though it is not a very remarkable data.

And we see that for this **category**, f.period=period afternoon mean is 0.42 units over the global mean and f.period=period valley mean, on the contrary, is -0.17 units under the global mean, so we can reject the H0 done in the t.Student test.

#### Third dimension
```{r}
fviz_contrib(res.pca,fill="darkslateblue",color="darkslateblue",choice="var",axes=3,top=5)
res.des$Dim.3
```

For the last dimension we took into account, the third one, we see that the most related **quantitative** variables are:

* q.passenger_count (0.76)
* q.extra (0.55)

For the inversely related one, we also see that traveltime time (-0.32).

For the **quanlitatives**, we see that f.passenger_groups is the category that is more related with 0.52.

And we see that for this **category**, f.passenger_groups=passenger_group is 1.48 units over the global mean, and f.passenger_groups=passenger_single, on the contrary, is -1.12 units under the global mean.

**We can conclude, then, that the first dimension is the one with the biggest correlations to the target**

### Perform a PCA taking into account also supplementary variables the supplementary variables can be quantitative and/or categorical

We want to take analyze the supplementary factor **kind of rate**, so we want to add lines that join the categories of this factor for the first factorial plane. With the following plot we can see it.

```{r}
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],pch=19,col="grey30")
points(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],pch=15,col="cadetblue1") 
lines(res.pca$quali.sup$coord[3:4,1],res.pca$quali.sup$coord[3:4,2],lwd=2,lty=2,col="coral")
text(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],labels=names(res.pca$quali.sup$coord[,1]),col="cadetblue1",cex=0.5)
```

!!!!!! On queda AnyTip?

--------------------------------------------------------------------------------

## Hierarchical Clustering

```{r}
res.hcpc <- HCPC(res.pca,nb.clust=5, order=TRUE)
```

*Note*: If we chose the default number of cluster it would be 3, as we can guess from the inertia reduction plot, that follows the Elbow's rule (number of black lines plus 1). In our case, due to the amount of data we have, the reason why we chose 5 as the number of clusters is because, after trying different numbers, we thought it was the best way to distribute the data.

### Description of clusters
Number of observations in each cluster:
```{r}
table(res.hcpc$data.clust$clust)
barplot(table(res.hcpc$data.clust$clust), col="darkslateblue", border="darkslateblue", main="[hierarchical] #observations/cluster")
```

### Interpret the results of the classification

#### The description of the clusters by the variables
```{r}
res.hcpc$desc.var$test.chi2 
```
We start wit the description of the categorical variables that characterize the clusters, so in this output we do not have dimensions because it is the total association. We can see the intensity of the variables, in our case the variables that affect more to the clustering are **f.trip_distance_range** and **f.passenger_groups** because are the one with the smallest p.value. The variables associated to the clusters are the ones that appear on the output.

Next, we want to see for each cluster which are the categories that characterize them.The clusters that contain more individuals are the first, the second and the fourth one. Cluster number 4 has less individuals. We proceed to analyze them.
```{r}
res.hcpc$desc.var$category
```

!!! FALTA EXPLICACIÓ CLUSTERS

We now proceed to see the quantitative variables that characterizes the clusters.
```{r}
res.hcpc$desc.var$quanti.var
```

We can see in the output that all the variables that appear are slightly over represented in the clusters. We can notice that the greatest represented is the q.passenger_count with 0.78 units over the global mean, we can also remark the q.trip_distance with 0.60 units over the mean and the q.fare_amount variable with 0.55 units over the mean. 

The least over represented are the q.pickup_latitude with 0.004 units over the mean, the q.dropoff_longitude with 0.005 units over the mean, the dropoff_latitude with 0.006 units over the mean and the q.extra with 0.013 units over the total mean.

We want to know now which variables are associated with the quantitative variables.
```{r}
res.hcpc$desc.var$quanti
```

!!! FALTA EXPLICACIÓ CLUSTERS

#### The description of the clusters by the individuals
```{r}
res.hcpc$desc.ind$para
```
What we obtain are the more representative individuals, paragons, for each cluster. We get the rownames of each paragon in every single cluster.

```{r}
res.hcpc$desc.ind$dist
```
What we obtain are those individuals of each cluster that that far away in the same cluster from the rest of the individuals. We also obtain the rownames of each individual with the bigger distance respect the other ones in the cluster.

##### Examine the values of individuals that characterize classes

We get the grpahical representation for the individuals that characterize classes (para and dist).
```{r}
para1<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[1]]))
dist1<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[1]]))
para2<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[2]]))
dist2<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[2]]))
para3<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[3]]))
dist3<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[3]]))
para4<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[4]]))
dist4<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[4]]))
para5<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[5]]))
dist5<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[5]]))

plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],col="grey50",cex=0.5,pch=16)
points(res.pca$ind$coord[para1,1],res.pca$ind$coord[para1,2],col="blue",cex=1,pch=16)
points(res.pca$ind$coord[dist1,1],res.pca$ind$coord[dist1,2],col="chartreuse3",cex=1,pch=16)
points(res.pca$ind$coord[para2,1],res.pca$ind$coord[para2,2],col="blue",cex=1,pch=16)
points(res.pca$ind$coord[dist2,1],res.pca$ind$coord[dist2,2],col="darkorchid3",cex=1,pch=16)
points(res.pca$ind$coord[para3,1],res.pca$ind$coord[para3,2],col="blue",cex=1,pch=16)
points(res.pca$ind$coord[dist3,1],res.pca$ind$coord[dist3,2],col="firebrick3",cex=1,pch=16)
points(res.pca$ind$coord[para4,1],res.pca$ind$coord[para4,2],col="blue",cex=1,pch=16)
points(res.pca$ind$coord[dist4,1],res.pca$ind$coord[dist4,2],col="palevioletred3",cex=1,pch=16)
points(res.pca$ind$coord[para5,1],res.pca$ind$coord[para5,2],col="blue",cex=1,pch=16)
points(res.pca$ind$coord[dist5,1],res.pca$ind$coord[dist5,2],col="royalblue1",cex=1,pch=16)
```

!! FALTA EXPLICACIÓ


#### Partition quality
We are going to evaluate the partition quality.

##### Gain in inertia (in %)
```{r}
((res.hcpc$call$t$within[1]-res.hcpc$call$t$within[5])/res.hcpc$call$t$within[1])*100
```

The quality of this reduction if of 62.31%.

In case we wanted to achieve an 80% of the clustering representativity we would need 9 clusters.
```{r}
((res.hcpc$call$t$within[1]-res.hcpc$call$t$within[9])/res.hcpc$call$t$within[1])*100
```

#### Save the results into dataframe
```{r}
res.hcpc$call$t$inert.gain[1:5]
df$hcpck<-res.hcpc$data.clust$clust
```

--------------------------------------------------------------------------------

## K-Means Classification

### Description of clusters
```{r}
res.pca <- PCA(df[,c(1,2,3,4,5,6,7,8,9,10,12,13,15,16,17,19,21,22,25,26)],quanti.sup=c(3:6,13),quali.sup=c(1,2,14:16,19:20),ncp=5,graph=FALSE)
ppcc<-res.pca$ind$coord[,1:3] ## 3 components principals (kaiser)
dim(ppcc)
```

#### Optimal number of clusters
```{r}
library("factoextra")
# fviz_nbclust(ppcc, kmeans, method = "gap_stat") ## !!!!Descomentar pel deliverable, triga molt.
```

According to the previous plot, the optimal number of clusters per k-means is 3, so we guess maybe something is wrong or missing.


### Classification

```{r}
dist<-dist(ppcc)
kc<-kmeans(dist, 5, iter.max=30, trace=TRUE) #caclulate the distances, it turns into a matrix
```
We see from the output that in 4 iterations it has converged.
We now procceed to save in the data frame the number of clusters.
```{r}
df$claKM<-0
df$claKM<-kc$cluster
df$claKM<-factor(df$claKM)
barplot(table(df$claKM),col="darkslateblue",border="darkslateblue",main="[k-means]#observations/cluster")
```

#### Gain in inertia (in %)
The american school does the partition quality evaluation in 5 clusters is done very fast, and after executing the following chunk we get an explicability of the 80.37%
```{r}
100*(kc$betweenss/kc$totss)
```

#### k-means clusters characteristics
If we want to know the characteristics of each cluster, as we did with the hierarchical, we need to execute a catdes to obtain these characteristics. In the following output we get them.
```{r}
dim(df)
res.cat <-catdes(df,30)
res.cat
```
We proceed to explain the data obtained.

#### The description of the clusters by the variables

We start wit the description of the categorical variables that characterize the clusters, so in this output we do
not have dimensions because it is the total association. We can see the intensity of the variables, in our case the
variables that affect more to the clustering are **Trip_distance_range**, **paidTolls** and **hcpck**  because are the one with the smallest p.value.

Next, we want to see for each cluster which are the categories that characterize them.

!!!!!!!!No heu de deixar de veure la relació dels clusters amb els 2 targets que teniu Total_amount i AnyTip.

!!! FALTA DESCRIPCIÓ CLASSES


#### Comparison of clusters (confusion table)
We want to compare the hierarchical clustering, previously done, and the k-means clustering, so proceed to do the following.
```{r}
table(df$hcpck,df$claKM)

## we must do a relabel
df$hcpck<-factor(df$hcpck,labels=c("kHP-1","kHP-2","kHP-3","kHP-4","kHP-5"))
df$claKM<-factor(df$claKM,levels=c(2,4,5,3,1),labels=c("kKM-2","kKM-4","kKM-5","kKM-3","kKM-1"))
tt<-table(df$hcpck,df$claKM); tt
100*sum(diag(tt)/sum(tt))
```
We have a concordance of the 47.94% so we can say that they are different, if we had a greater concordance, this would mean that they would be more similar.

--------------------------------------------------------------------------------

## CA analysis

### Are there any row categories that can be combined/avoided to explain the discretization of the numeric target.

#### CA analysis for your data should contain your factor version of the numeric target (previous) in K= 7 (maximum 10) levels and 2 factors.

The first thing we need to do is factor our numeric target variable, Total_amount, and name it f.cost. We are going to set 6 different categories.

```{r}
df$f.cost[df$q.target.total_amount<=8] = "[0,8]"
df$f.cost[(df$q.target.total_amount>8) & (df$q.target.total_amount<=11)] = "(8,11]"
df$f.cost[(df$q.target.total_amount>11) & (df$q.target.total_amount<=18)] = "(11,18]"
df$f.cost[(df$q.target.total_amount>18) & (df$q.target.total_amount<= 30)] = "(18,30]"
df$f.cost[(df$q.target.total_amount>30) & (df$q.target.total_amount<= 50)] = "(30,50]"
df$f.cost[df$q.target.total_amount>50] = "(50,129)"
df$f.cost<-factor(df$f.cost)
table(df$f.cost)
```
Once we have this factor, proceed to create a variable that associates the cost with the passenger groups, and we we a contingency table with 5 rows, one per kind of cost and 3 columns, one per each kind of group.

```{r}
tt<-table(df[,c("f.cost","f.passenger_groups")]);tt
chisq.test(tt,  simulate.p.value = TRUE) #to see if the rows and columns are independents. H0: Rows and columns are independent
```

We get a p-value lower than 0.05 so we cannot assume the H0. ( 0.3898 < 0.05 = TRUE).

We are now going to take a look to the simple correspondences.
```{r}
res.ca <- CA(tt)
```

Those observations far away from the gravity center will mean that represent less observations on the sample. If rows and columns are nearby, this will mean that there is a correspondence between them, which means that they occur simultaneously in the sample.

```{r}
summary(res.ca)
```

We conclude that we can not reject the H0 for these pair of factors, and now we are going to see if we can see if there is independence between the cost and the travel time, so the first thing we are going to do is factor the travel time.
```{r}
df$f.travel_time[df$q.travel_time<=5] = "[0,5]"
df$f.travel_time[(df$q.travel_time>5) & (df$q.travel_time<=10)] = "(5,10]"
df$f.travel_time[(df$q.travel_time>10) & (df$q.travel_time<=15)] = "(10,15]"
df$f.travel_time[(df$q.travel_time>15) & (df$q.travel_time<= 20)] = "(15,20]"
df$f.travel_time[(df$q.travel_time>20) & (df$q.travel_time<= 70)] = "(20,60]"
df$f.travel_time<-factor(df$f.travel_time)
table(df$f.travel_time)
```

Once we have this factor, proceed to create a variable that associates the cost with the traveltime.

```{r}
new_f.cost <- ordered(df$f.cost, levels= c("[0,8]", "(8,11]", "(11,18]", "(18,30]", "(30,50]","(50,129)"))
new_f.travel_time <- ordered(df$f.travel_time, levels= c("[0,5]", "(5,10]", "(10,15]", "(15,20]","(20,60]"))
tt<-table(new_f.cost, new_f.travel_time);tt
chisq.test(tt) #to see if the rows and columns are independents. H0: Rows and columns are independent
```

We get a p-value smaller than 0.05 so we can reject the H0. ((< 2.2e-16) < 0.05). So there is dependence between the traveltime and the cost, as we suspected.

We are now going to take a look to the simple correspondences.
```{r}
res.ca <- CA(tt)

plot(res.ca$row$coord[,1],res.ca$row$coord[,2],pch=19,col="blue",xlim=c(-1.5,1.5),ylim=c(-1.5,1.5),xlab="Axis 1",ylab="Axis 2", main="CA f.cost vs f.travel_time")
points(res.ca$col$coord[,1],res.ca$col$coord[,2],lwd=2,col="red")
text(res.ca$row$coord[,1],res.ca$row$coord[,2],lwd=2,col="blue",labels=levels(df$f.cost))
text(res.ca$col$coord[,1],res.ca$col$coord[,2],lwd=2,col="red",labels=levels(df$f.travel_time))
lines(res.ca$row$coord[,1],res.ca$row$coord[,2],lwd=2,col="blue")
lines(res.ca$col$coord[,1],res.ca$col$coord[,2],lwd=2,col="red")
```

We can see in the plot, clearly that there are some categories that occur simultaneously in the sample, for instant the trips up to 5 minutes with the cost up to 8, the trips between 5-10 minutes and the costs between 8-11, the same happen with the trips between 10-15 minutes and the costs between 11-18. There is a clear relation between the f.cost and f.travel_time categories.

```{r}
summary(res.ca)
```

The first thing we can see from the summary is that we have a chi square statistic of 6210.782, great enough to reject the H0, which means the intensity of the relation is high. If we take a look at the variances from the different dimensions, we can see that all together sum more than 1.

### Eigenvalues and dominant axes analysis. How many axes we have to consider?

```{r}
mean(res.ca$eig[,1])
```

Following the kaiser kriteria and the value got in the output, we should retain dimensions with a variance greater than 0.3399815. In this case, the first dimension fulfills this because its variance is 0.764, but it is not enough to work with data so, we would choose 2 o 3 dimensions for this case.

--------------------------------------------------------------------------------

## MCA analysis
The Multiple correspondence analysis (MCA) is an extension of the simple correspondence analysis for summarizing and visualizing a data table containing more than two categorical variables.

MCA is generally used to analyse a data set from survey. The goal is to identify:

* A group of individuals with similar profile in their answers to the questions
* The associations between variable categories

First, we load the libraries we'll use:
```{r}
library(FactoMineR)
library(factoextra)
```

Now, we can start computing the MCA for our categorical variables:

```{r}
names(df)
vars_con <- names(df)[c(10,11,14,15,18,20)]; length(vars_con)
vars_dis <- names(df)[c(1,2,16,17,19,25,16,27,28,29,30,31,32,33,34,37,38)]; length(vars_dis)
res.mca <- MCA( 
  df[,c(vars_dis,vars_con)],
  quanti.sup=c(18:23),
  quali.sup=c(10,16),
  graph=FALSE
)
```

Let’s look at the supplementary quantitative variable q.target.total_amount. We can see that it is closer to the Dim2 than to the Dim1.
```{r}
fviz_mca_var(res.mca, choice="quanti.sup", repel=TRUE, ggtheme=theme_minimal())
```

We can see that improvement subcharge and mta_tax are moving to the -1 value of dimension 2, while, for example, total amount and trip distance are moving to the 1 value of dimension 1. On the other hand, extra and hour are moving negatively towards both dimensions.

Cloud of individuals:
```{r}
fviz_mca_ind(res.mca,geom=c("point"),col.ind="darkslateblue")
```

### Eigenvalues and dominant axes analysis

**How many axes we have to consider for next Hierarchical Classification stage?**

We consider, according to the generalized Kaiser theorem, all those dimensions such that their eigenvalue is greater than the mean. We see that the average gives us 0.06666667. Therefore, we will take up to dimension 13, which represents the 73.66% of the sample.
```{r}
mean(res.mca$eig[,1])
head(get_eigenvalue(res.mca), 15)
```

We can also visualize the percentages of inertia explained by each MCA dimensions:
```{r}
fviz_screeplot(res.mca,addlabels=TRUE,ylim=c(0,15),barfill="darkslateblue",barcolor="darkslateblue",linecolor="skyblue1")
```

### Individuals point of view
Are they any individuals "too contributive"?
```{r}
fviz_mca_ind(res.mca,geom=c("point"),col.ind="contrib",gradient.cols=c("darkslateblue", "red"))
```

Are there any groups?
```{r}
fviz_mca_ind(res.mca, label="none", habillage="f.vendor_id", palette=c("darkslateblue", "red"))
fviz_mca_ind(res.mca, label="none", habillage="f.rate_code_id", palette=c("darkslateblue", "red"))
fviz_mca_ind(res.mca, label="none", habillage="f.trip_type", palette=c("darkslateblue", "red"))
```

We can see that individuals are more grouped according to some variables than others. For example, the vendor_id_mobile is along the entire dimension 1 but also in the center of gravity. In contrast, the rate_code_id_other-Other is only in the first dimension and does not touch the second at all.

### Interpreting map of categories: average profile versus extreme profiles (rare categories)

Before looking at the categories, let's look at its variables:

As we can see in the plot "Variables representation", the correlation between the f.dist factor taking into account the eta2 and the second factorial axis is a value greater than 0.5. On the other hand, we can see that something similar happens with the f.extra factor and f.rate_code_id in dimension 1.
```{r}
fviz_mca_var(res.mca, choice="mca.cor", repel=TRUE)
```

Now, let’s analyze the categories.

```{r}
fviz_mca_var(res.mca, repel=TRUE)
```

As we can see, the “tolls_yes” category (f.paid_tolls variable) is the one farthest from the center of the plot (in dimension 2). The farther from the center of gravity, the more rarely this feature value appears in the sample represented by the dimension. 

In addition, we see that in dimension 1 we also have two extremes, the "code_rate_id_other" category ("f.code_rate_id" variable) and the "trip_dispatch" category ("f.trip_type" variable), as well as other variables. As we have said, this means that these categories are rarely represented in this dimension.

Regardering the center of mass, we can say that we find the categories most represented by the dimensions.

To give an example, let's suppose we look at the first dimension. An observation that we could find with high probability would be the following:

* f.code_rate_id = code_rate_id_1
* f.trip_type = trip_street_hail

On the other hand, an observation that we could rarely find there would be...

* f.code_rate_id = code_rate_id_other
* f.trip_type = trip_dispatch

We would follow the same logic for dimension 2 considering the f.payment_type variable.

### Interpreting the axes association to factor map
```{r}
res.desc <- dimdesc(res.mca, axes = c(1,2))
```
#### Description of dimension 1
```{r}
res.desc[[1]]
```

There is no info for the **quantitative** variables here.

In the first dimension we see that for the **qualitative** variables the most positively related, from more to less, are:

* q.mta_tax               (-0.97)
* q.improvement_surcharge (-0.96)

If we look at the **categories**, we see that the most related are,

* f.improvement_surcharge=improvement_no    (1.61)
* f.mta_tax=mta_no                          (1.62)
* f.trip_type=trip_dispatch                 (1.67)
* f.rate_code_id=rate_code_id_other         (1.58)
* f.rate_code_id=rate_code_id_1             (-1.58)
* f.trip_type=trip_street_hail              (-1.67)
* f.mta_tax=mta_yes                         (-1.62)
* f.improvement_surcharge=improvement_yes   (-1.61)

#### Description of dimension 2
```{r}
res.desc[[2]]
```

There is no info for the **quantitative** variables here.

For the second dimension we see that for the **qualitative** variables the most positively related, from more to less, are:

* q.target.total_amount    (0.84)
* q.tlenkm                 (0.84)

We see that they are not very large numbers, however.

If we look at the **categories**, we see that the most related are,

* f.travel_time=(20,60]                          (0.54)
* f.cost=(18,30]                                 (0.28)
* f.dist=(5.5, 30]                               (0.64)
* f.trip_distance_range=trip_dist_long           (0.55)
* f.dist=(0, 1.6]                                (-0.53)
* f.trip_distance_range=trip_dist_short          (-0.56)

### Perform a MCA taking into account also supplementary variables (use all numeric variables) quantitative and/or categorical. How supplementary variables enhance the axis interpretation?
```{r}
names(df[,c(1:38)])
res.mca_all <- MCA(
  df[,c(1:38)],
  quanti.sup=c(3,4,5,6,7,8,9,10,11,12,13,14,15,18,20,21,22),
  quali.sup=c(26,37),
  graph=FALSE
)
```

#### Description of dimensions
```{r}
res.desc <- dimdesc(res.mca_all, axes = c(1,2))
```
##### Description of dimension 1
```{r}
res.desc[[1]]
```

In this dimension, since we have taken into account all the variables, we now have information for the **quantitative** variables. We see that, more or less, the most related are:

* q.trip_distance          (0.53)
* q.fare_amount            (0.53)
* q.extra                  (-0.63)

If we look at the **categories**, we see that the most related are,

* f.hour=f.hour_other       (0.56)
* f.extra=extra_no          (0.33)


##### Description of dimension 2
```{r}
res.desc[[2]]
```

In this dimension, since we have taken into account all the variables, we now have information for the **quantitative** variables. We see that, more or less, the most positively related are:

* q.trip_distance          (0.64)
* q.tlenkm                 (0.64)
* q.target.total_amount    (0.63)
* q.fare_amount            (0.59)

If we look at the **categories**, we see that the most related are,

* f.dist=(5.5, 30]                         (0.60)
* f.extra=extra_yes                        (0.28)
* f.trip_distance_range=trip_dist_long     (0.52)


--------------------------------------------------------------------------------

## Hierarchical Clustering (from MCA)

```{r}
res.hcpcMCA <- HCPC(res.mca,nb.clust=5,order=TRUE)
```

*Note*: If we chose the default number of cluster it would be 5, as we can guess from the inertia reduction plot, that follows the Elbow's rule (number of black lines plus 1). In our case, after trying with bigger number of clusters, we decided that the default number of cluster was fine for our case and data.

### Description of clusters
Number of observations in each cluster:
```{r}
table(res.hcpcMCA$data.clust$clust)
barplot(table(res.hcpcMCA$data.clust$clust), col="darkslateblue", border="darkslateblue", main="[hierarchical from mca] #observations/cluster")
```

### Interpret the results of the classification

#### The description of the clusters by the variables
```{r}
res.hcpcMCA$desc.var$test.chi2
```
We start wit the description of the categorical variables that characterize the clusters, so in this output we do not have dimensions because it is the total association. We can see the intensity of the variables, in our case the variables that affect more to the clustering are **f.rate_code_id**, **f.payment_type**, **f.trip_type** and **f.period** because are the one with the smallest p.value. The variables associated to the clusters are the ones that appear on the output.

Next, we want to see for each cluster which are the categories that characterize them. The clusters that contain more individuals are the first, the second and the fourth one. Clusters number 1 and 5 are the ones that have less individuals. We proceed to analyze them.

```{r}
res.hcpcMCA$desc.var$category
```

!!! FALTA DESCRIPCIÓ CLUSTERS


We now proceed to see the quantitative variables that characterizes the clusters.
```{r}
res.hcpcMCA$desc.var$quanti.var 
```

We can see in the output that the variable that appears is slightly over represented in the clusters. We can notice that **q.target.total_amount** is over represented with 0.7 units over the global mean. So it is practically the same as the global mean.


We want to know now which variables are associated with the quantitative variables.
```{r}
res.hcpcMCA$desc.var$quanti
```

We can notice that every cluster has remarked the q.target.total_amount variable except the first one, that does not have any variable to be described.

!! FALTA DESCRIPCIÓ CLUSTERS

#### Partition quality
We are going to evaluate the partition quality.

##### Gain in inertia (in %)
```{r}
## ( between sum of squares / total sum of squares ) * 100
((res.hcpcMCA$call$t$within[1]-res.hcpcMCA$call$t$within[5])/res.hcpcMCA$call$t$within[1])*100
```

The quality of this reduction if of 73.95%


In case we wanted to achieve an 80% of the clustering representativity we would need 6 clusters.
```{r}
((res.hcpcMCA$call$t$within[1]-res.hcpcMCA$call$t$within[6])/res.hcpcMCA$call$t$within[1])*100
```


### Parangons and class-specific individuals.

#### The description of the clusters by the individuals
```{r}
res.hcpcMCA$desc.ind$para  ## representative individuals of each cluster
```

What we obtain are the more representative individuals, paragons, for each cluster. We get the rownames of each paragon in every single cluster.


```{r}
res.hcpcMCA$desc.ind$dist  ## individuals distant from each cluster
```

What we obtain are those individuals of each cluster that that far away in the same cluster from the rest of the individuals. We also obtain the rownames of each individual with the bigger distance respect the other ones in the cluster.

##### Examine the values of individuals that characterize classes

We get the grpahical representation for the individuals that characterize classes (para and dist).
```{r}
## characteristic individuals
para1<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$para[[1]]))
dist1<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$dist[[1]]))
para2<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$para[[2]]))
dist2<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$dist[[2]]))
para3<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$para[[3]]))
dist3<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$dist[[3]]))
para4<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$para[[4]]))
dist4<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$dist[[4]]))
para5<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$para[[5]]))
dist5<-which(rownames(res.mca$ind$coord)%in%names(res.hcpcMCA$desc.ind$dist[[5]]))

plot(res.mca$ind$coord[,1],res.mca$ind$coord[,2],col="grey50",cex=0.5,pch=16)
points(res.mca$ind$coord[para1,1],res.mca$ind$coord[para1,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist1,1],res.mca$ind$coord[dist1,2],col="chartreuse3",cex=1,pch=16)
points(res.mca$ind$coord[para2,1],res.mca$ind$coord[para2,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist2,1],res.mca$ind$coord[dist2,2],col="darkorchid3",cex=1,pch=16)
points(res.mca$ind$coord[para3,1],res.mca$ind$coord[para3,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist3,1],res.mca$ind$coord[dist3,2],col="firebrick3",cex=1,pch=16)
points(res.mca$ind$coord[para4,1],res.mca$ind$coord[para4,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist4,1],res.mca$ind$coord[dist4,2],col="palevioletred3",cex=1,pch=16)
points(res.mca$ind$coord[para5,1],res.mca$ind$coord[para5,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist5,1],res.mca$ind$coord[dist5,2],col="royalblue1",cex=1,pch=16)
```

### Comparison of clusters obtained after K-Means (based on PCA) and/or Hierarchical Clustering (based on PCA) focusing on...

```{r}
df$hcpckMCA<-res.hcpcMCA$data.clust$clust

## With Hierarchical Clustering (PCA)
table(df$hcpck,df$hcpckMCA)
df$hcpckMCA_hcpck<-factor(
  df$hcpckMCA,
  levels=c(1,2,5,4,3),
  labels=c("kHPmca-1","kHPmca-2","kHPmca-5","kHPmca-4","kHPmca-3")
); tt1<-table(df$hcpck,df$hcpckMCA_hcpck); tt1; 100*sum(diag(tt1)/sum(tt1))
```

We have a concordance of the 66.30% so we can say that they are different, if we had a greater concordance, this would mean that they would be more similar.


```{r}
## With k-means (PCA)
table(df$claKM, df$hcpckMCA)
df$hcpckMCA_claKM<-factor(
  df$hcpckMCA,
  levels=c(2,1,5,3,4),
  labels=c("kHPmca-2","kHPmca-1","kHPmca-5","kHPmca-3","kHPmca-4")
);tt2<-table(df$claKM,df$hcpckMCA_claKM); tt2;100*sum(diag(tt2)/sum(tt2))
```

We have a concordance of the 44.90% so we can say that they are different, if we had a greater concordance, this would mean that they would be more similar.

#### Quantitative target (Total_amount)

* hcpc
```{r}
## res.hcpc$desc.var$quanti.var  ## quantitative variables which characterizes the clusters
## ##                               Eta2       P-value
## ## q.target.total_amount 0.572312116  0.000000e+00
```

* kmeans
```{r}
## res.cat <-catdes(df,29)
## res.cat
## ## Link between the cluster variable and the quantitative variables
## ## ================================================================
## ##                                Eta2       P-value
## ## q.target.total_amount   0.065791515 5.688596e-70
```

* hcpc_mca
```{r}
## ## res.hcpcMCA$desc.var$quanti.var 
## ##                             Eta2      P-value
## ## q.target.total_amount   0.696374175 0.000000e+00
```

##### Comment
To compare the variable Total_amount in the three different classifications, we will look at Eta2:

* The closer to 1 is eta2 for a variable, the better the variance between groups is explained by this variable.
* We can see that, in descending order, we have:
  + hcpc_mca (0.696374175)
  + kmeans (0.065791515)
  + hcpc (0.572312116)
* This means that in the last classification the variable to define the clusters is not taken into account so much.


#### Binary target (TipIsGiven)
##### hcpc
```{r}
## res.hcpc$desc.var$category    ## description of each cluster by the categories
# # $`1`
# #                                           Cla/Mod    Mod/Cla    Global     
# # f.target.tip_is_given=tip_no           80.0278843 67.3906663 62.410268 
# # f.target.tip_is_given=tip_yes          64.2939815 32.6093337 37.589732  
# # 
# # $`2`
# #                                         Cla/Mod     Mod/Cla    Global      
# # nothing to see here
# # 
# # $`3`
# #                                        Cla/Mod Mod/Cla   Global     p.value
# # f.target.tip_is_given=tip_no         0.7319624      84 62.41027 0.021898539
# # f.target.tip_is_given=tip_yes        0.2314815      16 37.58973 0.021898539
# # 
# # $`4`
# #                                          Cla/Mod   Mod/Cla    Global  
# # f.target.tip_is_given=tip_yes          26.909722 58.197747 37.589732
# # f.target.tip_is_given=tip_no           11.641687 41.802253 62.410268 
# # 
# # $`5`
# #                                         Cla/Mod  Mod/Cla   Global 
# # f.target.tip_is_given=tip_yes         1.3888889 68.57143 37.58973 
# # f.target.tip_is_given=tip_no          0.3834089 31.42857 62.41027

```

##### kmeans
```{r}
## wtf
```

##### hcpc_mca
```{r}
## res.hcpcMCA$desc.var$category    ## description of each cluster by the categories
# # $`1`
# #                                                  Cla/Mod      Mod/Cla     Global
# # f.target.tip_is_given=tip_no                  66.6434298  70.76239822 62.4102676
# # f.target.tip_is_given=tip_yes                 45.7175926  29.23760178 37.5897324
# # 
# # $`2`
# #                                                   Cla/Mod      Mod/Cla     Global
# # f.target.tip_is_given=tip_yes                 36.57407407  53.15391085 37.5897324
# # f.target.tip_is_given=tip_no                  19.41443012  46.84608915 62.4102676
# # 
# # $`3`
# #                                                  Cla/Mod   Mod/Cla     Global    
# # f.target.tip_is_given=tip_no                    1.010805 100.00000 62.4102676 
# # f.target.tip_is_given=tip_yes                   0.000000   0.00000 37.5897324
# #
# # $`4`
# #                                                   Cla/Mod     Mod/Cla     Global
# # f.target.tip_is_given=tip_yes                 16.66666667  50.5263158 37.5897324
# # f.target.tip_is_given=tip_no                   9.82920878  49.4736842 62.4102676
# # 
# # $`5`
# #                                                    Cla/Mod     Mod/Cla    Global
# # f.target.tip_is_given=tip_no                    3.10212618  83.1775701 62.410268
# # f.target.tip_is_given=tip_yes                   1.04166667  16.8224299 37.589732
```

##### Comment
FALTA

To compare the variable TipIsGiven in the three different classifications, we will look at Cla / Mod, Mod / Cla and Global:

* Cluster 1:
   + hcpc: TipIsGiven = No is overrepresented
   + kmeans: TipIsGiven = Yes is overrepresented
   + hcpc_mca: TipIsGiven = No is overrepresented
* Cluster 2:
   + hcpc: TipIsGiven = No is overrepresented
   + kmeans: TipIsGiven = Yes is overrepresented
   + hcpc_mca: There is no data in the cluster of this variable
* Cluster 3:
   + hcpc: No data in the cluster of this variable
   + kmeans: TipIsGiven = No is overrepresented
   + hcpc_mca: TipIsGiven = No is overrepresented
* Cluster 4:
   + hcpc: TipIsGiven = Yes is overrepresented
   + kmeans: TipIsGiven = Yes is overrepresented
   + hcpc_mca: TipIsGiven = Yes is overrepresented
* Cluster 5:
   + hcpc: TipIsGiven = Yes is overrepresented
   + kmeans: TipIsGiven = No is overrepresented
   + hcpc_mca: TipIsGiven = No is overrepresented

#### Final comment

FALTA 

We think that at first glance, we do not find the relationship between the different clusters of the different types of analysis. As we can see in the data, they are not distributed in the same way with respect to the two variables we had to analyze.

It makes sense to think this, since these variables have not been taken into account in the analyzes, as they had the role of supplementary variables, which means that they only served us as explanatory variables, and not to decide how to form clusters.

--------------------------------------------------------------------------------

# DELIVERABLE 3

## Clean some data
```{r}
names(df)
df$hcpck <- NULL
df$claKM <- NULL
df$hcpckMCA <- NULL
df$hcpckMCA_hcpck <- NULL
df$hcpckMCA_claKM <- NULL
df$qual.dropoff <- NULL
df$qual.pickup <- NULL
#df<-df[!(df$q.target.total_amount=="0"),]
length(df[!(df$q.target.total_amount=="0"),]);
```

## Listing out variables
```{r}
names(df)
vars_con<-names(df)[c(3:15,18,20:22)];vars_con
vars_dis<-names(df)[c(1,2,16,17,19,23:34)]; vars_dis
vars_res<-names(df)[c(15,27)];vars_res
vars_cexp<-names(df)[c(7,8,9,10,11,12,13,14,18,20,21,22)];vars_cexp
```

## Quantitative Logistics Regression

Before we begin to see correlations with our target, we should consider the normality of this.

### (0) Normality
```{r}
hist(df$q.target.total_amount,50,freq=F,col="darkslateblue",border = "darkslateblue")
mm<-mean(df$q.target.total_amount);ss<-sd(df$q.target.total_amount)
curve(dnorm(x,mean=mm,sd=ss),col="red",lwd=2,lty=3, add=T)
shapiro.test(df$q.target.total_amount)
```

We see that the target total_amount is not normally distributed for the following reasons:

* graph: there is no symmetry in the plot
* shapiro: we see that the p-value is too large to accept the assumption that target.total_amount is normally distributed

#### Symmetry
```{r}
skewness(df$q.target.total_amount)
```

Normal data should have 0 skewness: we see that our data is right skewed (3.18).

#### Kurtosis
```{r}
kurtosis(df$q.target.total_amount)
```

Normal data should be 3. We have 21.1, so, in this case, our data is not normal.


### Start to find models

Steps to follow:

1. Enter all relevant numerical variables in the model
2. See if you need to replace a number with its equivalent factor
3. Add to the best model of step 2, the main effects of the factors and retain the significant net effects.
4. Add interactions: between factor-factor and between factor-numeric (doubles).
5. Diagnosis of waste and observations. Lack of adjustment and / or influential.

### (1) Numerical variables

#### Method 1: take the most correlated variables

We use spearman method since out target is not normally distributed

```{r}
round(cor(df[,c("q.target.total_amount",vars_cexp)], method="spearman"),dig=2)
```

We see that the diagonal is full of '1', since this command gives us the correlation between the same variable. Apart from this diagonal, however, there are more high correlations. Let's see which ones are correlated with our target:

* q.fare_amount: 0.97
* q.trip_distance: 0.93
* q.tlenkm: 0.91 (like trip_distance)
* q.traveltime: 0.90
* q.tip_amount: 0.41 (not much, but must be taken into account)
* q.espeed: 0.29 (not much, but must be taken into account)
* q.tolls_amount: 0.15  (not much, but must be taken into account)
* we can see that some of them are not correlated:
  + q.extra (0.03)
  + q.passenger_count (0.01)
  + q.hour (-0.01)

After seeing the correlation, to make an initial model, we should select the ones that are most correlated, which are:

* q.fare_amount
* q.trip_distance (we are not taking tlenkm because of redundance)
* q.traveltime
* q.tip_amount
* q.espeed
* q.tolls_amount

#### Method 2: take the entire dataset with a condes
```{r}
res.con <- condes(df,num.var=which(names(df)=="q.target.total_amount"))
```

```{r}
res.con$quanti
```

As we have seen before, the most correlated variables are:

* q.fare_amount: 0.94
  + it is normal for the rate to go up when the price goes up
* q.trip_distance: 0.90
  + the more distance, the more time, and therefore the more price
* q.tlenkm: 0.88
  + just like the previous one
* q.traveltime: 0.76
  + the longer, the more price
* q.tip_amount: 0.57
  + not so much related, but we can keep in mind that people tend to give a percentage of the total price
* q.espeed: 0.40
* q.tolls_amount: 0.26

```{r}
res.con$quali
```

To talk about factor variables, we need to visualize res.con$quali. So let's see:

* f.trip_distance_range
  + we see that they are totally related, just as we see with que.trip_distance, since the longer distance, the longer time, and therefore the more price
* f.cost
  + is equivalent to our target
* f.tt
  + he longer time, the more price
* f.dist
  + just like with f.trip_distance_range
* f.paid_tolls
  + if you pay more, it means that the trip has lasted longer, and therefore has been longer, and is more likely to have gone through more tolls
* target.tip_is_given
  + just like before, but we can keep in mind that people tend to give a percentage of the total price

#### Method 3: if few explanatory variables are available -> take all of them
```{r}
vars_cexp
cor(df$q.trip_distance,df$q.tlenkm)
```

To give an example, we see that the two distances we have, trip_distance and tlenkm, are closely related, since they represent the same.

#### Model 1
```{r}
model_1 <- lm(target.total_amount~.,data=df[,c("q.target.total_amount",vars_cexp)]);summary(model_1)
```

Model_1 explains 93.4% of the variability of the target. We also see, according to the F-statistic, that it should be rejected.

We cannot use variables that are so correlated at the same time to act as explanatory variables. Therefore, we need to make a model in which we do not have these correlations.

But first, let's see which of them are that correlated:
```{r}
vif(model_1)
```

When the variance inflation factor is greater than 5, we need to consider whether or not we keep a variable.

* q.trip_distance: 137.215426
* q.tlenkm: 116.473412
* q.fare_amount: 10.203484
* q.traveltime: 5.069225

In this case we have to choose how far we stay. Since we work better with km than with miles (or inches, or whatever it is), we could choose the variable q.tlenkm.

#### Model 1 with BIC
```{r}
model_1_bic <- step( model_1, k=log(nrow(df)) )
```

The BIC has been eliminating the variables it has considered, without worsening the AIC. However, since it does not take into account either correlations or concepts, it is probably not optimal.

Let's see how it turned out:

```{r}
vif(model_1_bic)
```

Note that tlenkm still has a vif greater than 5 (9.377307), and so does fare_amount (7.898396).

```{r}
summary(model_1_bic)
```

However, we see that it continues to explain much of the variability of our target (93.39%).

Therefore, we will try to make a model manually based on what model_1_bic has shown us and our knowledge of the data:

#### Model 2

```{r}
model_2 <- lm(target.total_amount~q.passenger_count+q.fare_amount+q.extra+q.tip_amount+q.tolls_amount+q.hour+q.tlenkm+q.traveltime+q.espeed,data=df[,c("q.target.total_amount",vars_cexp)]);summary(model_2)
```

We see that the explainability is now 93.39%.

```{r}
vif(model_2)
```

Even so, owning one is still beyond the reach of the average person.

We try to make a new model without the distance:

#### Model 3

```{r}
model_3 <- lm(target.total_amount~q.passenger_count+q.fare_amount+q.extra+q.tip_amount+q.tolls_amount+q.hour+q.traveltime+q.espeed,data=df[,c("q.target.total_amount",vars_cexp)]);summary(model_3)
```

We see that the explainability is now 92.99%.

```{r}
vif(model_3)
```

The live ones are fine now. Still, we’ve pulled the distance, which conceptually we can’t afford. Therefore, we will try to remove another variable with a high vif (q.fare_amount), instead of q.tlenkm:

#### Model 4

```{r}
model_4 <- lm(target.total_amount~q.passenger_count+q.extra+q.tip_amount+q.tolls_amount+q.hour+q.tlenkm+q.traveltime+q.espeed,data=df[,c("q.target.total_amount",vars_cexp)]);summary(model_4)
```

We see that the explainability is now 86.17%.

```{r}
vif(model_4)
```

Despite having high vifs, we still have high explicability of the variability of our target and, given that the variable we have taken out we can remove with time and distance from the trip, we do not need it.

So we continue to stay with this variable and make new models. We apply BIC to help us a little:
```{r}
model_4_bic <- step( model_4, k=log(nrow(df)) )
```

Following BIC, we have to eliminate variables until the vif's are less than 5. Therefore, the model that meets this is:

#### Model 5

```{r}
model_5 <- lm(target.total_amount~q.passenger_count+q.extra+q.tip_amount+q.tolls_amount+q.tlenkm+q.traveltime,data=df);summary(model_5)
```

We see that the explainability is now 86.09%

```{r}
vif(model_5)
```

There is no vif that exceeds 5.

Let's now discriminate the variables independently:

```{r}
marginalModelPlots(model_5)
```

We see that there is not much mismatch of the marginal variables. If there were any, we would have to transform our explanatory variables.

### Diagnostics

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

Looking at the results, we can say that:

* There is no normality
* And, in terms of the Residual vs Leverage graph, our variables are within the R model, but it's not very reliable, so it doesn't help us much.

All this is due to the fact that our target variable was no longer normally distributed. To solve this, we apply the logarithm:

```{r}
model_6 <- lm(log(target.total_amount)~q.passenger_count+q.extra+q.tip_amount+q.tolls_amount+q.tlenkm +q.traveltime,data=df);summary(model_6)
```

We see that when doing the logarithm, the coefficient of determination is getting lower and lower, now it is 79.51%. We have seen that it has gotten worse than the previous model. Therefore, we discard it. We will work with model_5.

However, let's remember the last three models we used:

* Model 4
  + Coefficient of determination = 86,17%
  + VIFs:
    - q.passenger_count: 1.004128
    - q.extra: 1.065604
    - q.tip_amount: 1.227688
    - q.tolls_amount: 1.063359
    - q.hour: 1.072115
    - q.tlenkm: 6.195063
    - q.traveltime: 4.147204
    - q.espeed: 2.731942
* Model 5
  + Coefficient of determination = 86.09%
  + VIFs:
    - q.passenger_count: 1.003687
    - q.extra: 1.006299
    - q.tip_amount: 1.226347
    - q.tolls_amount: 1.063286
    - q.tlenkm: 2.431645
    - q.traveltime: .249571
* Model 6
  + Coefficient of determination = 79.51%
  + VIFs:
    - q.passenger_count: 1.003687
    - q.extra: 1.006299
    - q.tip_amount: 1.226347
    - q.tolls_amount: 1.063286
    - q.tlenkm: 2.431645
    - q.traveltime: 2.249571

According to the coefficient of explicability, the ranking is: model_4 >> model_5 >> model_6. As for the VIFs, however, the ranking is: model_6 >> model_5 >> model_4. Since VIFs are acceptable on both model_5 and model_6, and not acceptable on model_4, the smartest option is to choose model_5.

So, let's look at the effects of this model:

```{r}
Anova(model_5)
```

We see that now the net effects are significant.

```{r}
library(effects)
plot(allEffects(model_5))
```

We see that our model defines the following:

* q.passenger_count does not depend on target.total_amount
* q.extra grows if target.total_amount grows
* q.tip_amount grows if target.total_amount grows
* q.tolls_amount grows if target.total_amount grows
* q.tlenkm grows if target.total_amount grows
* q.traveltime grows if target.total_amount grows

```{r}
par(mfrow=c(2,2))
plot(model_5, id.n=0 )
par(mfrow=c(1,1))
```

We see that the residues are not completely optimal.

#### Exhaustive
````{r}
ll1<-Boxplot(rstudent(model_5));ll1
sel2<-which(hatvalues(model_5)>5*length(model_5$coefficients)/nrow(df));sel2;length(sel2)
```

```{r}
library(MASS)
boxcox(target.total_amount~q.passenger_count+q.extra+q.tip_amount+q.tolls_amount+q.tlenkm+q.traveltime,data=df)
```

We see the lambda parameter estimation method in the boxcox method. This gives us an idea of the power to which we need to raise the target variable in order to improve the properties of the linear model.

It is worth trying a new model with a square root in the target variable:

```{r}
model_7 <- lm(sqrt(target.total_amount)~q.passenger_count+q.extra+q.tip_amount+q.tolls_amount+q.tlenkm+q.traveltime,data=df);summary(model_7)
```

We see that the coefficient has improved, from 85.09% (model_5) to 86.41% (model_7). But ... is it worth it from a residual point of view?

```{r}
par(mfrow=c(2,2));plot( model_7, id.n=0 );par(mfrow=c(1,1))
```

We see we haven’t won too much. So we stick to model_5.

### (2) Factors

```{r}
model_8<-lm(log(target.total_amount)~ q.extra + q.tip_amount +  q.tolls_amount + f.improvement_surcharge + q.espeed  + log(q.tlenkm), data=df)
summary(model_8)
```

We see that the explainability is now 87.77%. The more influent effects in this models are the length in km of the trip and the tip amount given.

```{r}
Anova(model_8)
vif(model_8)
residualPlots(model_8)
```

```{r}
df$f.extra <- factor(df$q.extra)

model_9<-lm(log(target.total_amount)~f.extra + q.tip_amount + q.tolls_amount + f.improvement_surcharge + q.espeed + log(q.tlenkm),data=df)
BIC(model_8,model_9)
```

We can see from the BIC that the model_9 is better than the model_8, so it is correct to consider extra as factor. Next, we will do the same with the tolls_amount and use the factor we had already created (paid_tolls).

```{r}
model_10<-lm(log(target.total_amount)~ f.extra + q.tip_amount + f.paid_tolls + f.improvement_surcharge + q.espeed + log(q.tlenkm),data=df)
BIC(model_8,model_9,model_10)
```

We see can see that it is correct to use the paid_tolls factor to improve our model. We will try it now with the effective speed.

```{r}
model_11<-lm(log(target.total_amount)~ f.extra + q.tip_amount + f.paid_tolls + f.improvement_surcharge + f.espeed + log(q.tlenkm),data=df)
BIC(model_8,model_9,model_10,model_11)
```
We can see that the best approach is the model_10, so we are going to stick to it for now.

```{r}
model_12 <- model_10

Anova(model_12)
summary(model_12)
```
We can see from the Anova test that f.extra has 2 freedom degrees and globally it does have a significant net effect once the other variables are in the model.

We are going to take a look at the residues.
```{r}
par(mfrow=c(2,2));plot( model_12, id.n=0 );par(mfrow=c(1,1))
```
Looking at the results, we can say that:

* There is no normality
* And, in terms of the Residual vs Leverage graph, our variables are within the R model, but it's not very reliable, so it doesn't help us much.


We proceed to take a look at the influence plot to check our influent residuals for model_12.
```{r}
influencePlot( model_12, id=c(list="noteworthy",n=5))
```

We see this model as a disaster. That is, we have a student waste of the order of 35. We can confirm that this is too much. We have to compare student waste with a normal standard. Therefore, we would say that the model we have so far is a model that has a serious waste problem.

--------------------------------------------------------------------------------

#### Remove multivariant outliers to improve influence plot

Since we’ve realized that this should have been removed from the start, what we’re going to do is put it at the beginning of the last deliverable in order to have a more consistent delivery. For now, however, we leave this section here so as not to have to change the entire delivery.

```{r}
library(mvoutlier)
library(chemometrics)
multivariant_outliers <- Moutlier(df[, c(15,20)], quantile = 0.995)
multivariant_outliers$cutoff
par(mfrow=c(1,1))
plot(multivariant_outliers$md, multivariant_outliers$rd, type="n")
text(multivariant_outliers$md, multivariant_outliers$rd, labels=rownames(df[, c(15,20)]), cex=0.5)

ll_mvoutliers<-c('1237379', '1208612', '1171898', '488540', '211894', '638666', '329000', '1175981', '604912')

df <- df[!(row.names(df) %in% ll_mvoutliers),]

multivariant_outliers <- Moutlier(df[, c(15,20)], quantile = 0.995)
multivariant_outliers$cutoff
par(mfrow=c(1,1))
plot(multivariant_outliers$md, multivariant_outliers$rd, type="n")
text(multivariant_outliers$md, multivariant_outliers$rd, labels=rownames(df[, c(15,20)]), cex=0.75)

```

--------------------------------------------------------------------------------

In order for this not to happen to us, we need to work on the variable q.tlenkm.

So let's create a new model that does not give so many problems:

```{r}
model_13<-lm(log(target.total_amount)~ f.extra + q.tip_amount + f.paid_tolls + f.improvement_surcharge + q.espeed + log(q.tlenkm),data=df); summary(model_13)
vif(model_13)

influencePlot( model_13, id=c(list="noteworthy",n=5))
```

After doing certain tests, taking into account the influences, the coefficients of explicability and the vifs, we decided that the best we can get is a model where q.tlenkm does not apply any operation.

So let's analyze it:

```{r}
residualPlots(model_13)
```

In the residualPlots, what we find is, for each factor, a boxplot of its categories and, for each quantitative variable, a pearson graph.

Let's use another tool to fully understand our model:

```{r}
marginalModelPlots(model_13)
```

In relation to the variable q.tip_amount, we see that there is a bit of mismatch, but not much, since tips given in cash are always declared as 0. Therefore, the data are not entirely real.

As for the variable q.tlenkm, we see that some observations do not follow the required pattern, and we have to modify them in some way.

How do we do that?

```{r}
ll1<-Boxplot(rstudent(model_13));ll1
ll1<-c(4269, 80, 2621)
df[ll1,]
```

Let's see the strangest:

* 4269
  + target.total_amount: 5.0
  + q.tip_amount: 0
  + q.espeed: 11.06889
  + q.tlenkm: 16.769364
* 80
  + target.total_amount: 3.8
  + q.tip_amount: 0
  + q.espeed: 23.16672
  + q.tlenkm: 9.012326
* 2621
  + target.total_amount: 3.8
  + q.tip_amount: 0
  + q.espeed: 23.05353
  + q.tlenkm: 8.851392

Veiem que són observacionsa vastant normals. Tot i això, per exemple, podem destacar que la observació 4269, a la qual ja se li aplica una tarifa de 5$, per molts km és que hagi fet, el preu no ha pujat.

We do the same for the cook distance:

```{r}
ll4 <- Boxplot(cooks.distance(model_13));ll4
ll4<-c(4269, 2005, 2434)
df[ll4,]
```

* 4269
  + target.total_amount: 5.0
  + q.tip_amount: 0
  + q.espeed: 11.06889
  + q.tlenkm: 16.769364
* 2005
  + target.total_amount: 50.00
  + q.tip_amount: 0
  + q.espeed: 27.33968
  + q.tlenkm: 1.00000
* 2434
  + target.total_amount: 49.99
  + q.tip_amount: 0
  + q.espeed: 23.79045
  + q.tlenkm: 1.00000

We see that, apart from the first, explained above, the other two observations have a trip length of 1km, but instead has been paid about $ 50. We see that this is not possible.

It is necessary to eliminate these observations that do not have the same tendency as our model:

```{r}
dfred<-df[-ll4,]

model_14<-lm(log(target.total_amount)~ f.extra + q.tip_amount + f.paid_tolls + f.improvement_surcharge + q.espeed + log(q.tlenkm),data=dfred);summary(model_14)
Anova(model_14)
vif(model_14)
```

We see that the coefficient of determination has increased a bit and it seems that we have no collinearity problems.

### (3) Add to the best model of step 2, the main effects of the factors and retain the significant net effects.

```{r}
names(df)
model_15<-lm(log(target.total_amount) ~ q.tip_amount + log(q.tlenkm)+ f.paid_tolls+ f.improvement_surcharge + f.espeed + f.extra + f.code_rate_id + f.vendor_id + f.payment_type+f.period ,data=df); summary(model_15)
Anova(model_15)
```

We see that, of all the new explanatory variables introduced, the ones we can save are:

* f.espeed: 22.49
* f.code_rate_id: 11.06
* f.payment_type: 4.19

We create a new model with them:

```{r}
model_16<-lm(log(target.total_amount) ~ q.tip_amount + log(q.tlenkm)+ f.paid_tolls+ f.espeed + f.extra + f.code_rate_id + f.payment_type+f.period ,data=df)

anova(model_15, model_16)


influencePlot( model_16, id=c(list="noteworthy",n=5))
residualPlots(model_16)
marginalModelPlots(model_16)
```

We see that we haven't lost anything.

### (4) Interactions

```{r}
model_17<-lm( log(target.total_amount) ~ (q.tip_amount + log(q.tlenkm))*(f.paid_tolls + f.espeed + f.extra + f.code_rate_id + f.payment_type + f.period),data=df)
model_17<-step( model_17, k=log(nrow(df)))
```

This method tells us that:

* log(target.total_amount) depends on:
  + q.tip_amount
  + log(q.tlenkm)
  + f.paid_tolls
  + f.espeed
  + f.extra
  + f.code_rate_id
  + f.payment_type
* and there are interactionsa between:
  + q.tip_amount:f.espeed
  + q.tip_amount:f.code_rate_id
  + log(q.tlenkm):f.espeed
  + log(q.tlenkm):f.extra
  + log(q.tlenkm):f.code_rate_id
  + log(q.tlenkm):f.payment_type

```{r}
Anova(model_17)
summary(model_17)
```


#### Exhaustive
````{r}
ll1<-Boxplot(rstudent(model_17));ll1
sel2<-which(hatvalues(model_17)>5*length(model_17$coefficients)/nrow(df));sel2;length(sel2)
```


--------------------------------------------------------------------------------

## Binary Logistics Regression

```{r}
vars_cexp <- vars_cexp[c(1:4,6:10)]; vars_cexp

table(df$target.tip_is_given, df$f.payment_type)
```

We can see from the table that it is no credible the fact that any of the people that paid in cash did not leave any tip.

```{r}
res.cat <- catdes(df, num.var = which(names(df)=="target.tip_is_given"))
res.cat$quanti.var
res.cat$test.chi2
```

From the quanti.var we can see that tip_is_given depends on tip_amount which seems obvious, due to the fact that they are the same variable treated in different ways.

From the test.chi2 we can see that payment_type has something really clear with the tip_is_given, as we have p-value of 0. Which means that we cannot use payment_type as a predictor.

### Filter

```{r}
ll<-which(df$f.payment_type=="Cash"); length(ll)
dff<-df[-ll,]
set.seed(12345)
llwork<-sample(1:nrow(dff),0.70*nrow(dff),replace=FALSE)
llwork<-sort(llwork);length(llwork)
dffwork<-dff[llwork,]
dfftest<-dff[-llwork,]

table(df$target.tip_is_given, df$f.payment_type)
```

Steps to follow:

1. Enter all relevant numerical variables in the model
2. See if you need to replace a number with its equivalent factor
3. Add to the best model of step 2, the main effects of the factors and retain the significant net effects.
4. Add interactions: between factor-factor and between factor-numeric (doubles).
5. Diagnosis of waste and observations. Lack of adjustment and / or influential.

### (1) Numerical variables
#### Model 20

```{r}
model_20 <- glm(target.tip_is_given~.,family = "binomial",data=dffwork[,c("target.tip_is_given",vars_cexp)]);summary(model_20)
Anova(model_20, test="Wald") #binary target
```

Comments:

* We can see that the most influent variable, in our case, is the q.hour.
* We can see that the residual deviance is of 1386.6 on 1472  degrees of freedom.

```{r}
vif(model_20)
```

We can see that we have some variables with very high vifs:

* q.trip_distance (66.67)
* q.tlenkm (63.09) --> correlated with the previous
* q.fare_amount (9.48)
* q.traveltime (5.16)

#### Model 21

*NOTE: we are aware that we should not have factors in this section, but we have decided to include them due to the fact that we overwrote their numeric values and created their factors in the previous deliverables.*

We know there is not colinearity, so we create a new model:

```{r}
model_21 <- glm(target.tip_is_given~f.improvement_surcharge+f.mta_tax+q.passenger_count+q.extra+q.tolls_amount+q.hour+q.espeed+q.tlenkm+q.traveltime ,family = "binomial",data=dffwork);summary(model_21)
vif(model_21)

Anova(model_21, test="Wald") #binary target

anova(model_21, model_20, test="Chisq") # only for nested models
```

We can transform tlenkm and remove improvement_surcharge in order to have lower vifs:

#### Model 22

```{r}
model_22 <- glm(target.tip_is_given~f.mta_tax+q.passenger_count+q.extra+q.tolls_amount+q.hour+q.espeed+poly(q.tlenkm,2)+q.traveltime,family = "binomial",data=dffwork); summary(model_22)
vif(model_22)
anova(model_21, model_22, test="Chisq") # only for nested models
Anova(model_22, test="Wald") # binary target
```

Now, we can do a step:

#### Model 23
```{r}
model_23 <- step(model_22, k=log(nrow(dffwork)))
summary(model_23)
```

Due to the fact that this model is really ppor, we will take also the q.extra variable in order to be able to extract more information. For instance, we could do the marginal plots:

```{r}
model_23 <- glm(target.tip_is_given~f.mta_tax+q.extra,family = "binomial",data=dffwork); summary(model_23)
```

##### Understanding the model

```{r}
plot(allEffects(model_23))
```

* For the f.mta_tax: we that if the value of the variable is "Yes", it is more probable that the target.tip_is_given value will be "Yes" as well.
* For the q.extra: as we have said before, this variable does not really affect to the target, but we will include it in order to be able to do more plots. At most, we could say that it is inversely proportional to the target.

```{r}
marginalModelPlots(model_23)
```

We can observe that q.extra is a candidate to be a factor.

```{r}
residualPlots(model_23)
```

We see that the smoothers are relatively plain, so we could say that, for now, everything is ok.


We are going, though, to propose a model which brings us more chances:

#### Model 24
```{r}
model_24 <- glm(target.tip_is_given~poly(q.tlenkm, 2)+f.mta_tax+q.extra+q.espeed,family = "binomial",data=dffwork); summary(model_24)
residualPlots(model_24)
marginalModelPlots(model_24)
(model_16)

```

* q.tlenkm:
  + we see that the smoothe is plain, so it is ok.
  + the "weird" shapes that appear are because of the binery response model.
* q.extra:
  + we observe that the smoother is plain, so it is ok.
* q.espeed:
  + we see that the smoother is plain, so it is ok.
  + the "weird" shapes that appear are because of the binery response model.
* the whole model:
  + we see that the smoothe is not completely straight, but as it was said in class, we can work with unfitted values in the model, due to the fact that it is a really dense topic.


```{r}
Anova(model_24, test="Wald")
```

We have to ensure that we do not have any variable with a non significant net effect.

Thus, we are going to redp the model:

```{r}
model_24 <- glm(target.tip_is_given~poly(q.tlenkm, 2)+f.mta_tax,family = "binomial",data=dffwork); summary(model_24)
vif(model_24)
residualPlots(model_24)
marginalModelPlots(model_24)
Anova(model_24, test="Wald")
```

With Anova(model_24), we see that it is fulfilled.


### (2) Factors

We look if any of the numeric variables can be substituted by a factor.

The first thing we will do, it would be change the "q.mta_tax" (if it existed in our dataset) for a "f.mta_tax". Due to the fact that mta_tax is already a factor, we do not need to do this step.

Given that the other variable that could be a factor depends on a polynomial, we keep as it is. The code that should be done in case of a new model with an added factor, would be the following:


```{r}
model_25 <- glm(
  target.tip_is_given~
    poly(q.tlenkm, 2)+
    f.mta_tax
  ,family = "binomial"
  ,data=dffwork
); summary(model_25)

BIC(model_24, model_25) # same model --> same bic
```

Thanks to the BIC(model_24, model_25) we could see te changes generated by the new model. The less the BIC is, the better the model will be. We need to remember that, in case of have done an exchange of a numeric variable to a factor, we could not have done it with an anova test, due to the fact that there is an exchange, which means that any model is bigger than the other.

### (3) Add to the best model of step 2, the main effects of the factors and retain the significant net effects.

We decide to keep with the model_25.


### (4) Interactions

Now that we have a defined model, we are going to do some interactions with all of the factor variables we think are the relevant:

```{r}
model_26 <- glm(target.tip_is_given~(poly(q.tlenkm, 2))*(f.mta_tax+f.vendor_id+f.period+f.espeed+f.paid_tolls+f.tt+f.extra), family="binomial",data=dffwork); summary(model_26)
Anova(model_26, test="Wald")
```

We remove the non significant variables:
```{r}
model_27 <- step(model_26, k=log(nrow(dffwork)))
```

From what we can see, it only stays with the tax, but in order to have more freedom, we will keep what we had before.

Hence:
```{r}
model_27 <- glm(target.tip_is_given~(poly(q.tlenkm,2))*(f.mta_tax),family = "binomial",data=dffwork); summary(model_27)
Anova(model_27, test="Wald")
```

We do a comparison:
```{r}
BIC(model_27, model_25)
```

We keep with the 25.

We can see now the effects of it:
```{r}
plot(allEffects(model_25))
marginalModelPlots(model_25)
residualPlots(model_25)
```

* We can observe that only the tips is given in certain range of driven km, due to the fact that for few km, it makes no sense to give it, and for many km it is too much.
* As we have previously commented, it is more likely to give some tips if a tax is present.


Although, for this deliverable it is asked to do some interactions between factors, we will do it even though the results could not be realistic:

```{r}
# interaccions dobles entre factors:
model_factors_1 <- glm(target.tip_is_given~(poly(q.tlenkm,2)+q.extra)+(f.mta_tax+f.vendor_id+f.espeed)^2,family="binomial",data=dffwork); summary(model_factors_1)
model_factors_1_step <- step(model_factors_1, k=log(nrow(dffwork)))
```

We stick with what we had.


```{r}
# interaccions dobles entre factor-numèrica
model_factors_2 <- glm(target.tip_is_given~(poly(q.tlenkm,2)+q.extra)*(f.mta_tax+f.vendor_id+f.espeed),family="binomial",data=dffwork); summary(model_factors_2)
model_factors_2_step <- step(model_factors_2, k=log(nrow(dffwork)))
```

We stick with what we had.


```{r}
# interaccions dobles entre factor-numèrica + dobles entre factors
model_factors_3 <- glm(target.tip_is_given~(poly(q.tlenkm,2)+q.extra)*(f.mta_tax+f.vendor_id+f.espeed)^2,family="binomial",data=dffwork); summary(model_factors_3)
model_factors_3_step <- step(model_factors_3, k=log(nrow(dffwork)))
```

We stick with what we had.


Conclusion: we stick with the idea that the best model is model_25.

Now, we are going to do some diagnosis:

### (5) Diagnosis

```{r}
Boxplot(rstudent(model_25), id.n=15)
sout <- which(abs(rstudent(model_25))>2); length(sout) # posem 2 en comptes de 2.5 perquè no tenim observacions en aquell rang
llout <- which(row.names(dffwork) %in% names(rstudent(model_25)[sout])); llout
table(dffwork[llout,]$f.mta_tax, dffwork[llout,]$target.tip_is_given)
```

We see that they are samples that contain mta_tax, but in the other hand they do not have tip.


We are going to determine which are the potencially influent observations:

```{r}
quantile(hatvalues(model_25), seq(0,1,0.1))
mean(hatvalues(model_25))

hh <- 5*mean(hatvalues(model_25)); hh
shat <- which(hatvalues(model_25)>hh); length(shat); shat

summary(dffwork[shat,])
```

They tend to:

* have rate=Rate-Other
* be in the same location (they have very similar latitudes and longitudes)
* have extra=0
* don't have mta_tax
* be at night
* be one passenger
* be long (distance) but short (time)

Now, to decide the influences, we are going to take a look at the cook distances:

```{r}
Boxplot(cooks.distance(model_25))
scoo <- which(cooks.distance(model_25) > 0.02); length(scoo)
llcoo <- which(row.names(dffwork) %in% names(cooks.distance(model_25)[scoo])); llcoo
llista<-influencePlot(model_25, id=c(list="noteworthy", n=10))

summary(dffwork[llcoo,])
```

They tend to:

* have rate=Rate-1
* be in the same location (they have very similar latitudes and longitudes)
* be one passanger
* be between 20 and 60 min long
* have mta_tax
* be long (distance) but short (time)


We redo the model now:

```{r}
llout<-row.names(llista)
ll<-which(row.names(dffwork)%in%llout);
dffwork<-dffwork[-ll,]

model_25 <- glm(target.tip_is_given~poly(q.tlenkm, 2)+f.mta_tax,family = "binomial",data=dffwork); summary(model_25)
influencePlot(model_25, id=c(list="noteworthy", n=10))
```

```{r}
# interaccions dobles entre factors:
model_factors_5 <- glm(target.tip_is_given~(poly(q.tlenkm,2)+q.extra)+(f.mta_tax+f.vendor_id+f.espeed)^2,family="binomial",data=dffwork); summary(model_factors_5)
model_factors_5_step <- step(model_factors_5, k=log(nrow(dffwork)))
```

We stick with what we had.

```{r}
# interaccions dobles entre factor-numèrica
model_factors_6 <- glm(target.tip_is_given~(poly(q.tlenkm,2)+q.extra)*(f.mta_tax+f.vendor_id+f.espeed),family="binomial",data=dffwork); summary(model_factors_6)
model_factors_6_step <- step(model_factors_6, k=log(nrow(dffwork)))
```

We stick with what we had.

```{r}
# interaccions dobles entre factor-numèrica + dobles entre factors
model_factors_7 <- glm(target.tip_is_given~(poly(q.tlenkm,2)+q.extra)*(f.mta_tax+f.vendor_id+f.espeed)^2,family="binomial",data=dffwork); summary(model_factors_7)
model_factors_7_step <- step(model_factors_7, k=log(nrow(dffwork)))
```

We stick with what we had.

## Confusion Table
```{r}
fit.tip_is_given <- factor(ifelse(predict(model_25, type="response")<0.5,0,1), labels=c("fit.no", "fit.yes"))
tt <- table(fit.tip_is_given,dffwork$target.tip_is_given); tt
100*sum(diag(tt)/sum(tt)) #accuracy
100*(tt[2,2]/(tt[2,2] + tt[1,2])) # recall (sensitivity)
100*(tt[1,1]/(tt[1,1] + tt[2,1])) # specificity
100*(tt[2,2]/(tt[2,1]+ tt[2,2])) # precision
```

We have an accuracy of 83.05%. We have a recall of 99.42% which means that the positive results of this confusion table is very accurate. We can see that we have 1201 + 7 positive observations, from which 1201 of them have been correctly classified. Now, we are going to do the same, but for the negative results (specificity). We can see that only a 3.61% of specificity, which is a very bad result. Only 9 of the 240 + 9 negative observations have been classified as negative. To conclude, we see that the precision of this confusion table is 83.34%.


--------------------------------------------------------------------------------

## Finally, save the data
```{r}
save.image("Taxi5000_del4.RData")
```

